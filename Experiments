{"cells":[{"cell_type":"code","source":["!cat /proc/cpuinfo\n","!cat /proc/meminfo"],"metadata":{"id":"goA9hXSlzYxn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z8LsSb1z1WZj","executionInfo":{"status":"ok","timestamp":1692700864731,"user_tz":-60,"elapsed":10,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"6485b041-2ca4-4883-b25d-4ed8e73a0f79"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Aug 22 10:40:42 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26304,"status":"ok","timestamp":1693447780494,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"P9QeYhUdGi7q","outputId":"c9f26b9b-3932-42a0-91f6-7962b4ab3389"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/bias-bench-main\n"]}],"source":["from google.colab import drive\n","try:\n","  drive.mount('/content/drive')\n","except:\n","  pass\n","%cd /content/drive/My Drive/bias-bench-main"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":422,"status":"ok","timestamp":1693447891380,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"v_LdI4VLHcqd","outputId":"997ef8b2-5f74-41a7-9b46-2c34613b86f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/bias-bench-main\n"]}],"source":["! pwd"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41910,"status":"ok","timestamp":1693447936398,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"eqRCDQ7pGi-d","outputId":"3050e245-8afb-46dd-92eb-af0f705b3bbc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Obtaining file:///content/drive/MyDrive/bias-bench-main\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from bias-bench==0.1.0) (2.0.1+cu118)\n","Collecting transformers==4.16.2 (from bias-bench==0.1.0)\n","  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scipy==1.7.3 (from bias-bench==0.1.0)\n","  Downloading scipy-1.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scikit-learn==1.0.2 (from bias-bench==0.1.0)\n","  Downloading scikit_learn-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nltk==3.7.0 (from bias-bench==0.1.0)\n","  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets==1.18.3 (from bias-bench==0.1.0)\n","  Downloading datasets-1.18.3-py3-none-any.whl (311 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting accelerate==0.5.1 (from bias-bench==0.1.0)\n","  Downloading accelerate-0.5.1-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.5.1->bias-bench==0.1.0) (6.0.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.5.1->bias-bench==0.1.0) (1.23.5)\n","Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->bias-bench==0.1.0) (9.0.0)\n","Collecting dill (from datasets==1.18.3->bias-bench==0.1.0)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->bias-bench==0.1.0) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->bias-bench==0.1.0) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->bias-bench==0.1.0) (4.66.1)\n","Collecting xxhash (from datasets==1.18.3->bias-bench==0.1.0)\n","  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets==1.18.3->bias-bench==0.1.0)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->bias-bench==0.1.0) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->bias-bench==0.1.0) (3.8.5)\n","Collecting huggingface-hub<1.0.0,>=0.1.0 (from datasets==1.18.3->bias-bench==0.1.0)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==1.18.3->bias-bench==0.1.0) (23.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.7.0->bias-bench==0.1.0) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.7.0->bias-bench==0.1.0) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.7.0->bias-bench==0.1.0) (2023.6.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2->bias-bench==0.1.0) (3.2.0)\n","Collecting numpy>=1.17 (from accelerate==0.5.1->bias-bench==0.1.0)\n","  Downloading numpy-1.22.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->bias-bench==0.1.0) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->bias-bench==0.1.0) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->bias-bench==0.1.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->bias-bench==0.1.0) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->bias-bench==0.1.0) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->bias-bench==0.1.0) (2.0.0)\n","Collecting sacremoses (from transformers==4.16.2->bias-bench==0.1.0)\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting tokenizers!=0.11.3,>=0.10.1 (from transformers==4.16.2->bias-bench==0.1.0)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m117.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->bias-bench==0.1.0) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->bias-bench==0.1.0) (16.0.6)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3->bias-bench==0.1.0) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3->bias-bench==0.1.0) (3.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3->bias-bench==0.1.0) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3->bias-bench==0.1.0) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3->bias-bench==0.1.0) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3->bias-bench==0.1.0) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==1.18.3->bias-bench==0.1.0) (1.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.18.3->bias-bench==0.1.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.18.3->bias-bench==0.1.0) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==1.18.3->bias-bench==0.1.0) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->bias-bench==0.1.0) (2.1.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.18.3->bias-bench==0.1.0) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==1.18.3->bias-bench==0.1.0) (2023.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==4.16.2->bias-bench==0.1.0) (1.16.0)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->bias-bench==0.1.0) (1.3.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=b55d9183ac25ef6e62139932ec576991d55e0b2dcac5b06e3300a5157349fd29\n","  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, xxhash, sacremoses, numpy, nltk, dill, scipy, multiprocess, huggingface-hub, transformers, scikit-learn, datasets, accelerate, bias-bench\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.23.5\n","    Uninstalling numpy-1.23.5:\n","      Successfully uninstalled numpy-1.23.5\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.8.1\n","    Uninstalling nltk-3.8.1:\n","      Successfully uninstalled nltk-3.8.1\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.10.1\n","    Uninstalling scipy-1.10.1:\n","      Successfully uninstalled scipy-1.10.1\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.2.2\n","    Uninstalling scikit-learn-1.2.2:\n","      Successfully uninstalled scikit-learn-1.2.2\n","  Running setup.py develop for bias-bench\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","arviz 0.15.1 requires scipy>=1.8.0, but you have scipy 1.7.3 which is incompatible.\n","plotnine 0.12.2 requires numpy>=1.23.0, but you have numpy 1.22.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed accelerate-0.5.1 bias-bench-0.1.0 datasets-1.18.3 dill-0.3.7 huggingface-hub-0.16.4 multiprocess-0.70.15 nltk-3.7 numpy-1.22.4 sacremoses-0.0.53 scikit-learn-1.0.2 scipy-1.7.3 tokenizers-0.13.3 transformers-4.16.2 xxhash-3.3.0\n"]}],"source":["!python -m pip install -e ."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1480,"status":"ok","timestamp":1693010116346,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"EnIRYyS8LK6j","outputId":"d89dbb57-250f-4950-ad68-52a1108f584d"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}],"source":["import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VlXzZzp9LLDM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692702063572,"user_tz":-60,"elapsed":571,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"26bf7522-44dd-4768-92b3-f94acdb9401e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Operating System: Linux\n","No LSB modules are available.\n","Distributor ID:\tUbuntu\n","Description:\tUbuntu 22.04.2 LTS\n","Release:\t22.04\n","Codename:\tjammy\n"]}],"source":["import platform\n","\n","os_info = platform.system()\n","\n","print(\"Operating System:\", os_info)\n","\n","!lsb_release -a\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zr7ppzKGLNeH"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"_dZoegdrXeZk"},"source":["##5.1 INLP baseline\n","\n","1. crows.py without debiasing\n","2. crows_debias.py gender 10000 80\n","3. crows_debias.py race 10000 80\n","4. crows_debias.py religion 10000 80"]},{"cell_type":"markdown","source":["###5.1.1 crows.py"],"metadata":{"id":"6YB8AtsV7Zjp"}},{"cell_type":"code","source":["! python experiments/crows.py --model AlbertForMaskedLM --model_name_or_path albert-base-v2 --bias_type gender"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PoK8bA0f7Ys4","executionInfo":{"status":"ok","timestamp":1692536249412,"user_tz":-60,"elapsed":38339,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"41c55d45-015d-438c-bb7f-d72d93188c30"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/My Drive/bias-bench-main\n"," - model: AlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_type: gender\n","Evaluating gender examples.\n","100% 262/262 [02:58<00:00,  1.47it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 48.09\n","Stereotype score: 47.8\n","Anti-stereotype score: 48.54\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 48.09\n"]}]},{"cell_type":"code","source":["! python experiments/crows.py --model AlbertForMaskedLM --model_name_or_path albert-base-v2 --bias_type race"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rv4MTDMW7iWF","executionInfo":{"status":"ok","timestamp":1692536876560,"user_tz":-60,"elapsed":74087,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"193a790a-ab95-4a3c-bb91-809bbe4ad021"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/My Drive/bias-bench-main\n"," - model: AlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_type: race\n","Evaluating race-color examples.\n"," 70% 363/516 [03:16<00:45,  3.39it/s]Skipping example 363.\n","100% 515/516 [04:34<00:00,  1.88it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 62.52\n","Stereotype score: 64.62\n","Anti-stereotype score: 39.53\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 62.52\n"]}]},{"cell_type":"code","source":["! python experiments/crows.py --model AlbertForMaskedLM --model_name_or_path albert-base-v2 --bias_type religion"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WWUt27eG7Y8X","executionInfo":{"status":"ok","timestamp":1692536934198,"user_tz":-60,"elapsed":57639,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"58d4a95f-79e2-4e28-d99d-e63f0120fb73"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/My Drive/bias-bench-main\n"," - model: AlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_type: religion\n","Evaluating religion examples.\n","100% 105/105 [00:47<00:00,  2.19it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 60.0\n","Stereotype score: 60.61\n","Anti-stereotype score: 50.0\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 60.0\n"]}]},{"cell_type":"markdown","source":["### 5.1.2 crows_debias.py gender 10000 80"],"metadata":{"id":"aqHAdTYX7PoQ"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"18YFoxEaGcuXxiDmnULXezausTJimcb8_"},"executionInfo":{"elapsed":496222,"status":"ok","timestamp":1692485638660,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"Kf2_xiWDDWXz","outputId":"4342be73-faa9-4c42-a648-7bedeaa1c845"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# inlp for albert inlp_projection_matrix\n","# 10000 80 --bias_type gender only\n","\n","! python experiments/inlp_projection_matrix.py --model AlbertModel --model_name_or_path albert-base-v2 --bias_type gender"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30481,"status":"ok","timestamp":1692486051078,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"pO3UGQeZGYQL","outputId":"89861c0c-c8ab-4d2f-82a9-393c196d49e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-gender10000_s-0.pt\n"," - load_path: None\n"," - bias_type: gender\n","Evaluating gender examples.\n","100% 262/262 [00:17<00:00, 15.37it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 47.33\n","Stereotype score: 34.59\n","Anti-stereotype score: 66.99\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 47.33\n"]}],"source":["# matrix that has only removed bias related to gender\n","# crows_debias:gender\n","\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-gender10000_s-0.pt --bias_type gender"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33007,"status":"ok","timestamp":1692486090313,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"5T09AnwqXxeM","outputId":"5c30b743-696a-4369-f112-2d8d44f04bc3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-gender10000_s-0.pt\n"," - load_path: None\n"," - bias_type: race\n","Evaluating race-color examples.\n"," 70% 361/516 [00:16<00:08, 18.21it/s]Skipping example 363.\n","100% 515/516 [00:24<00:00, 21.46it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 50.49\n","Stereotype score: 49.79\n","Anti-stereotype score: 58.14\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 50.49\n"]}],"source":["# matrix that has only removed bias related to gender\n","# crows_debias:race\n","\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-gender10000_s-0.pt --bias_type race"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14628,"status":"ok","timestamp":1692486122729,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"KYZRs1f5Xxjl","outputId":"824c62a6-8b17-4c52-c7fb-a0d82c6b1571"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-gender10000_s-0.pt\n"," - load_path: None\n"," - bias_type: religion\n","Evaluating religion examples.\n","100% 105/105 [00:04<00:00, 21.81it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 52.38\n","Stereotype score: 52.53\n","Anti-stereotype score: 50.0\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 52.38\n"]}],"source":["# matrix that has only removed bias related to gender\n","# crows_debias:religion\n","\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-gender10000_s-0.pt --bias_type religion"]},{"cell_type":"markdown","metadata":{"id":"2G-7fj87XkO-"},"source":["###5.1.3 crows_debias.py race 10000 80"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":191978,"status":"ok","timestamp":1692487351522,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"k4dgZwBvDWaM","outputId":"9eaa6160-3f18-4956-e136-afb8ede66605"},"outputs":[{"output_type":"stream","name":"stdout","text":["Computing projection matrix:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: AlbertModel\n"," - model_name_or_path: albert-base-v2\n"," - bias_type: race\n"," - n_classifiers: 80\n"," - seed: 0\n","Loading INLP data:  20% 280629/1372632 [00:21<01:15, 14449.03it/s]INLP dataset collected:\n"," - Num. bias sentences: 10000\n"," - Num. neutral sentences: 10000\n","Loading INLP data:  20% 280927/1372632 [00:21<01:22, 13191.06it/s]\n","Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias']\n","- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Encoding bias sentences: 100% 10000/10000 [02:07<00:00, 78.28it/s]\n","Encoding neutral sentences: 100% 10000/10000 [02:04<00:00, 80.12it/s]\n","Dataset split sizes:\n","Train size: 9800; Dev size: 4200; Test size: 6000\n","Dataset split sizes:\n","Train size: 9800; Dev size: 4200; Test size: 6000\n","iteration: 0, accuracy: 0.8811904761904762:   0% 0/80 [00:05<?, ?it/s]iteration: 0, accuracy: 0.8811904761904762\n","iteration: 1, accuracy: 0.8797619047619047:   1% 1/80 [00:14<07:44,  5.89s/it]iteration: 1, accuracy: 0.8797619047619047\n","iteration: 2, accuracy: 0.8811904761904762:   2% 2/80 [00:24<10:59,  8.45s/it]iteration: 2, accuracy: 0.8811904761904762\n","iteration: 3, accuracy: 0.8745238095238095:   4% 3/80 [00:31<11:13,  8.75s/it]iteration: 3, accuracy: 0.8745238095238095\n","iteration: 4, accuracy: 0.8726190476190476:   5% 4/80 [00:38<10:27,  8.26s/it]iteration: 4, accuracy: 0.8726190476190476\n","iteration: 5, accuracy: 0.8638095238095238:   6% 5/80 [00:45<09:36,  7.69s/it]iteration: 5, accuracy: 0.8638095238095238\n","iteration: 6, accuracy: 0.8588095238095238:   8% 6/80 [00:53<08:58,  7.28s/it]iteration: 6, accuracy: 0.8588095238095238\n","iteration: 7, accuracy: 0.849047619047619:   9% 7/80 [01:02<09:25,  7.74s/it] iteration: 7, accuracy: 0.849047619047619\n","iteration: 8, accuracy: 0.8380952380952381:  10% 8/80 [01:11<09:44,  8.12s/it]iteration: 8, accuracy: 0.8380952380952381\n","iteration: 9, accuracy: 0.8302380952380952:  11% 9/80 [01:18<09:49,  8.30s/it]iteration: 9, accuracy: 0.8302380952380952\n","iteration: 10, accuracy: 0.8233333333333334:  12% 10/80 [01:26<09:25,  8.08s/it]iteration: 10, accuracy: 0.8233333333333334\n","iteration: 11, accuracy: 0.8061904761904762:  14% 11/80 [01:35<09:11,  7.99s/it]iteration: 11, accuracy: 0.8061904761904762\n","iteration: 12, accuracy: 0.8030952380952381:  15% 12/80 [01:43<09:24,  8.30s/it]iteration: 12, accuracy: 0.8030952380952381\n","iteration: 13, accuracy: 0.8011904761904762:  16% 13/80 [01:51<09:13,  8.27s/it]iteration: 13, accuracy: 0.8011904761904762\n","iteration: 14, accuracy: 0.7919047619047619:  18% 14/80 [01:59<08:54,  8.10s/it]iteration: 14, accuracy: 0.7919047619047619\n","iteration: 15, accuracy: 0.7835714285714286:  19% 15/80 [02:06<08:46,  8.10s/it]iteration: 15, accuracy: 0.7835714285714286\n","iteration: 16, accuracy: 0.7738095238095238:  20% 16/80 [02:14<08:29,  7.97s/it]iteration: 16, accuracy: 0.7738095238095238\n","iteration: 17, accuracy: 0.7695238095238095:  21% 17/80 [02:21<08:10,  7.78s/it]iteration: 17, accuracy: 0.7695238095238095\n","iteration: 18, accuracy: 0.7602380952380953:  22% 18/80 [02:29<07:55,  7.67s/it]iteration: 18, accuracy: 0.7602380952380953\n","iteration: 19, accuracy: 0.7564285714285715:  24% 19/80 [02:37<07:37,  7.50s/it]iteration: 19, accuracy: 0.7564285714285715\n","iteration: 20, accuracy: 0.746904761904762:  25% 20/80 [02:45<07:44,  7.74s/it] iteration: 20, accuracy: 0.746904761904762\n","iteration: 21, accuracy: 0.7411904761904762:  26% 21/80 [02:52<07:31,  7.65s/it]iteration: 21, accuracy: 0.7411904761904762\n","iteration: 22, accuracy: 0.7338095238095238:  28% 22/80 [02:59<07:22,  7.62s/it]iteration: 22, accuracy: 0.7338095238095238\n","iteration: 23, accuracy: 0.7257142857142858:  29% 23/80 [03:06<07:09,  7.53s/it]iteration: 23, accuracy: 0.7257142857142858\n","iteration: 24, accuracy: 0.7147619047619047:  30% 24/80 [03:15<06:49,  7.31s/it]iteration: 24, accuracy: 0.7147619047619047\n","iteration: 25, accuracy: 0.7092857142857143:  31% 25/80 [03:22<07:04,  7.71s/it]iteration: 25, accuracy: 0.7092857142857143\n","iteration: 26, accuracy: 0.7:  32% 26/80 [03:31<06:50,  7.61s/it]               iteration: 26, accuracy: 0.7\n","iteration: 27, accuracy: 0.6964285714285714:  34% 27/80 [03:38<06:57,  7.87s/it]iteration: 27, accuracy: 0.6964285714285714\n","iteration: 28, accuracy: 0.6933333333333334:  35% 28/80 [03:45<06:42,  7.73s/it]iteration: 28, accuracy: 0.6933333333333334\n","iteration: 29, accuracy: 0.6792857142857143:  36% 29/80 [03:51<06:26,  7.57s/it]iteration: 29, accuracy: 0.6792857142857143\n","iteration: 30, accuracy: 0.6823809523809524:  38% 30/80 [03:58<05:56,  7.13s/it]iteration: 30, accuracy: 0.6823809523809524\n","iteration: 31, accuracy: 0.6721428571428572:  39% 31/80 [04:05<05:52,  7.20s/it]iteration: 31, accuracy: 0.6721428571428572\n","iteration: 32, accuracy: 0.6697619047619048:  40% 32/80 [04:13<05:34,  6.97s/it]iteration: 32, accuracy: 0.6697619047619048\n","iteration: 33, accuracy: 0.6707142857142857:  41% 33/80 [04:22<05:49,  7.43s/it]iteration: 33, accuracy: 0.6707142857142857\n","iteration: 34, accuracy: 0.6654761904761904:  42% 34/80 [04:29<05:49,  7.59s/it]iteration: 34, accuracy: 0.6654761904761904\n","iteration: 35, accuracy: 0.6661904761904762:  44% 35/80 [04:37<05:43,  7.64s/it]iteration: 35, accuracy: 0.6661904761904762\n","iteration: 36, accuracy: 0.6504761904761904:  45% 36/80 [04:43<05:29,  7.50s/it]iteration: 36, accuracy: 0.6504761904761904\n","iteration: 37, accuracy: 0.6514285714285715:  46% 37/80 [04:53<05:23,  7.52s/it]iteration: 37, accuracy: 0.6514285714285715\n","iteration: 38, accuracy: 0.6473809523809524:  48% 38/80 [05:00<05:36,  8.01s/it]iteration: 38, accuracy: 0.6473809523809524\n","iteration: 39, accuracy: 0.645:  49% 39/80 [05:09<05:14,  7.66s/it]             iteration: 39, accuracy: 0.645\n","iteration: 40, accuracy: 0.6490476190476191:  50% 40/80 [05:16<05:21,  8.03s/it]iteration: 40, accuracy: 0.6490476190476191\n","iteration: 41, accuracy: 0.6488095238095238:  51% 41/80 [05:24<05:04,  7.81s/it]iteration: 41, accuracy: 0.6488095238095238\n","iteration: 42, accuracy: 0.6471428571428571:  52% 42/80 [05:31<05:01,  7.93s/it]iteration: 42, accuracy: 0.6471428571428571\n","iteration: 43, accuracy: 0.6445238095238095:  54% 43/80 [05:38<04:36,  7.49s/it]iteration: 43, accuracy: 0.6445238095238095\n","iteration: 44, accuracy: 0.6371428571428571:  55% 44/80 [05:46<04:24,  7.33s/it]iteration: 44, accuracy: 0.6371428571428571\n","iteration: 45, accuracy: 0.6378571428571429:  56% 45/80 [05:52<04:18,  7.39s/it]iteration: 45, accuracy: 0.6378571428571429\n","iteration: 46, accuracy: 0.6385714285714286:  57% 46/80 [06:00<04:04,  7.20s/it]iteration: 46, accuracy: 0.6385714285714286\n","iteration: 47, accuracy: 0.6371428571428571:  59% 47/80 [06:07<04:01,  7.32s/it]iteration: 47, accuracy: 0.6371428571428571\n","iteration: 48, accuracy: 0.6416666666666667:  60% 48/80 [06:15<03:57,  7.42s/it]iteration: 48, accuracy: 0.6416666666666667\n","iteration: 49, accuracy: 0.64:  61% 49/80 [06:22<03:50,  7.44s/it]              iteration: 49, accuracy: 0.64\n","iteration: 50, accuracy: 0.6273809523809524:  62% 50/80 [06:29<03:45,  7.53s/it]iteration: 50, accuracy: 0.6273809523809524\n","iteration: 51, accuracy: 0.63:  64% 51/80 [06:35<03:28,  7.18s/it]              iteration: 51, accuracy: 0.63\n","iteration: 52, accuracy: 0.6245238095238095:  65% 52/80 [06:42<03:09,  6.78s/it]iteration: 52, accuracy: 0.6245238095238095\n","iteration: 53, accuracy: 0.6195238095238095:  66% 53/80 [06:48<03:08,  6.99s/it]iteration: 53, accuracy: 0.6195238095238095\n","iteration: 54, accuracy: 0.6226190476190476:  68% 54/80 [06:55<02:54,  6.70s/it]iteration: 54, accuracy: 0.6226190476190476\n","iteration: 55, accuracy: 0.6173809523809524:  69% 55/80 [07:02<02:47,  6.69s/it]iteration: 55, accuracy: 0.6173809523809524\n","iteration: 56, accuracy: 0.616904761904762:  70% 56/80 [07:08<02:40,  6.69s/it] iteration: 56, accuracy: 0.616904761904762\n","iteration: 57, accuracy: 0.6121428571428571:  71% 57/80 [07:14<02:28,  6.46s/it]iteration: 57, accuracy: 0.6121428571428571\n","iteration: 58, accuracy: 0.6095238095238096:  72% 58/80 [07:21<02:26,  6.66s/it]iteration: 58, accuracy: 0.6095238095238096\n","iteration: 59, accuracy: 0.6085714285714285:  74% 59/80 [07:27<02:16,  6.52s/it]iteration: 59, accuracy: 0.6085714285714285\n","iteration: 60, accuracy: 0.6107142857142858:  75% 60/80 [07:36<02:09,  6.48s/it]iteration: 60, accuracy: 0.6107142857142858\n","iteration: 61, accuracy: 0.6009523809523809:  76% 61/80 [07:43<02:12,  6.96s/it]iteration: 61, accuracy: 0.6009523809523809\n","iteration: 62, accuracy: 0.606904761904762:  78% 62/80 [07:51<02:09,  7.17s/it] iteration: 62, accuracy: 0.606904761904762\n","iteration: 63, accuracy: 0.6026190476190476:  79% 63/80 [07:58<02:07,  7.51s/it]iteration: 63, accuracy: 0.6026190476190476\n","iteration: 64, accuracy: 0.5997619047619047:  80% 64/80 [08:06<01:59,  7.48s/it]iteration: 64, accuracy: 0.5997619047619047\n","iteration: 65, accuracy: 0.6011904761904762:  81% 65/80 [08:13<01:52,  7.49s/it]iteration: 65, accuracy: 0.6011904761904762\n","iteration: 66, accuracy: 0.5966666666666667:  82% 66/80 [08:21<01:43,  7.37s/it]iteration: 66, accuracy: 0.5966666666666667\n","iteration: 67, accuracy: 0.5935714285714285:  84% 67/80 [08:28<01:35,  7.34s/it]iteration: 67, accuracy: 0.5935714285714285\n","iteration: 68, accuracy: 0.5933333333333334:  85% 68/80 [08:34<01:26,  7.24s/it]iteration: 68, accuracy: 0.5933333333333334\n","iteration: 69, accuracy: 0.5957142857142858:  86% 69/80 [08:41<01:19,  7.22s/it]iteration: 69, accuracy: 0.5957142857142858\n","iteration: 70, accuracy: 0.5933333333333334:  88% 70/80 [08:49<01:09,  6.96s/it]iteration: 70, accuracy: 0.5933333333333334\n","iteration: 71, accuracy: 0.5852380952380952:  89% 71/80 [08:57<01:06,  7.37s/it]iteration: 71, accuracy: 0.5852380952380952\n","iteration: 72, accuracy: 0.585952380952381:  90% 72/80 [09:02<00:58,  7.26s/it] iteration: 72, accuracy: 0.585952380952381\n","iteration: 73, accuracy: 0.5835714285714285:  91% 73/80 [09:09<00:48,  6.93s/it]iteration: 73, accuracy: 0.5835714285714285\n","iteration: 74, accuracy: 0.5807142857142857:  92% 74/80 [09:15<00:41,  6.85s/it]iteration: 74, accuracy: 0.5807142857142857\n","iteration: 75, accuracy: 0.5852380952380952:  94% 75/80 [09:22<00:33,  6.62s/it]iteration: 75, accuracy: 0.5852380952380952\n","iteration: 76, accuracy: 0.5828571428571429:  95% 76/80 [09:30<00:27,  6.88s/it]iteration: 76, accuracy: 0.5828571428571429\n","iteration: 77, accuracy: 0.5828571428571429:  96% 77/80 [09:36<00:20,  6.82s/it]iteration: 77, accuracy: 0.5828571428571429\n","iteration: 78, accuracy: 0.574047619047619:  98% 78/80 [09:42<00:13,  6.58s/it] iteration: 78, accuracy: 0.574047619047619\n","iteration: 79, accuracy: 0.575:  99% 79/80 [09:48<00:06,  6.65s/it]            iteration: 79, accuracy: 0.575\n","iteration: 79, accuracy: 0.575: 100% 80/80 [09:49<00:00,  7.37s/it]\n","Saving computed projection matrix to: /content/drive/MyDrive/bias-bench-main/results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-race_s-0.pt\n"]}],"source":["# inlp for albert inlp_projection_matrix\n","# 10000 80 --bias_type race only\n","\n","! python experiments/inlp_projection_matrix.py --model AlbertModel --model_name_or_path albert-base-v2 --bias_type race"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25526,"status":"ok","timestamp":1692488446859,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"xPuJxGoEX4-b","outputId":"eb1a0147-2c68-4baf-e67d-d3bd0cea08c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-race10000_s-0.pt\n"," - load_path: None\n"," - bias_type: gender\n","Evaluating gender examples.\n","100% 262/262 [00:17<00:00, 15.26it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 47.71\n","Stereotype score: 44.03\n","Anti-stereotype score: 53.4\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 47.71\n"]}],"source":["# matrix that has only removed bias related to race\n","# crows_debias:gender\n","\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-race10000_s-0.pt --bias_type gender"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34382,"status":"ok","timestamp":1692488483860,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"X3Rklg1WX5JP","outputId":"038272a7-a700-4b30-e927-9e9af4baff91"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-race10000_s-0.pt\n"," - load_path: None\n"," - bias_type: race\n","Evaluating race-color examples.\n"," 70% 361/516 [00:17<00:05, 29.37it/s]Skipping example 363.\n","100% 515/516 [00:25<00:00, 19.83it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 55.34\n","Stereotype score: 56.99\n","Anti-stereotype score: 37.21\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 55.34\n"]}],"source":["# matrix that has only removed bias related to race\n","# crows_debias:race\n","\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-race10000_s-0.pt --bias_type race"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11840,"status":"ok","timestamp":1692488496539,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"KyOgcr6qX5Pq","outputId":"6574f90e-0984-4419-a19f-6d0a895d6327"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-race10000_s-0.pt\n"," - load_path: None\n"," - bias_type: religion\n","Evaluating religion examples.\n","100% 105/105 [00:04<00:00, 23.89it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 50.48\n","Stereotype score: 50.51\n","Anti-stereotype score: 50.0\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 50.48\n"]}],"source":["# matrix that has only removed bias related to race\n","# crows_debias:religion\n","\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-race10000_s-0.pt --bias_type religion"]},{"cell_type":"markdown","metadata":{"id":"BcsfXjjGll2y"},"source":["###5.1.4 crows_debias.py religion 10000 80"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4102257,"status":"ok","timestamp":1690821870316,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"vf2CUTzujBIE","outputId":"c08f2cba-9586-4021-d03e-a919518c9eb4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Computing projection matrix:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: AlbertModel\n"," - model_name_or_path: albert-base-v2\n"," - bias_type: religion\n"," - n_classifiers: 80\n"," - seed: 0\n","Loading INLP data:  27% 365188/1372632 [00:36<01:24, 11904.78it/s]INLP dataset collected:\n"," - Num. bias sentences: 10000\n"," - Num. neutral sentences: 10000\n","Loading INLP data:  27% 365565/1372632 [00:36<01:40, 10065.73it/s]\n","Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.dense.weight']\n","- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Encoding bias sentences: 100% 10000/10000 [31:04<00:00,  5.36it/s]\n","Encoding neutral sentences: 100% 10000/10000 [25:08<00:00,  6.63it/s]\n","Dataset split sizes:\n","Train size: 9800; Dev size: 4200; Test size: 6000\n","Dataset split sizes:\n","Train size: 9800; Dev size: 4200; Test size: 6000\n","iteration: 0, accuracy: 0.9214285714285714:   0% 0/80 [00:04<?, ?it/s]iteration: 0, accuracy: 0.9214285714285714\n","iteration: 1, accuracy: 0.920952380952381:   1% 1/80 [00:12<07:01,  5.34s/it] iteration: 1, accuracy: 0.920952380952381\n","iteration: 2, accuracy: 0.9245238095238095:   2% 2/80 [00:23<09:45,  7.51s/it]iteration: 2, accuracy: 0.9245238095238095\n","iteration: 3, accuracy: 0.9254761904761905:   4% 3/80 [00:31<10:53,  8.49s/it]iteration: 3, accuracy: 0.9254761904761905\n","iteration: 4, accuracy: 0.9219047619047619:   5% 4/80 [00:39<11:00,  8.69s/it]iteration: 4, accuracy: 0.9219047619047619\n","iteration: 5, accuracy: 0.9202380952380952:   6% 5/80 [00:46<10:24,  8.32s/it]iteration: 5, accuracy: 0.9202380952380952\n","iteration: 6, accuracy: 0.9166666666666666:   8% 6/80 [00:54<09:54,  8.03s/it]iteration: 6, accuracy: 0.9166666666666666\n","iteration: 7, accuracy: 0.9092857142857143:   9% 7/80 [01:01<09:20,  7.68s/it]iteration: 7, accuracy: 0.9092857142857143\n","iteration: 8, accuracy: 0.9121428571428571:  10% 8/80 [01:09<09:09,  7.63s/it]iteration: 8, accuracy: 0.9121428571428571\n","iteration: 9, accuracy: 0.9064285714285715:  11% 9/80 [01:16<09:06,  7.70s/it]iteration: 9, accuracy: 0.9064285714285715\n","iteration: 10, accuracy: 0.9007142857142857:  12% 10/80 [01:24<08:51,  7.59s/it]iteration: 10, accuracy: 0.9007142857142857\n","iteration: 11, accuracy: 0.8947619047619048:  14% 11/80 [01:32<09:02,  7.86s/it]iteration: 11, accuracy: 0.8947619047619048\n","iteration: 12, accuracy: 0.8876190476190476:  15% 12/80 [01:40<08:46,  7.74s/it]iteration: 12, accuracy: 0.8876190476190476\n","iteration: 13, accuracy: 0.8773809523809524:  16% 13/80 [01:49<08:44,  7.82s/it]iteration: 13, accuracy: 0.8773809523809524\n","iteration: 14, accuracy: 0.8714285714285714:  18% 14/80 [01:56<08:48,  8.01s/it]iteration: 14, accuracy: 0.8714285714285714\n","iteration: 15, accuracy: 0.8673809523809524:  19% 15/80 [02:05<08:42,  8.04s/it]iteration: 15, accuracy: 0.8673809523809524\n","iteration: 16, accuracy: 0.8609523809523809:  20% 16/80 [02:13<08:37,  8.09s/it]iteration: 16, accuracy: 0.8609523809523809\n","iteration: 17, accuracy: 0.855:  21% 17/80 [02:22<08:37,  8.21s/it]             iteration: 17, accuracy: 0.855\n","iteration: 18, accuracy: 0.8521428571428571:  22% 18/80 [02:30<08:37,  8.35s/it]iteration: 18, accuracy: 0.8521428571428571\n","iteration: 19, accuracy: 0.8471428571428572:  24% 19/80 [02:39<08:17,  8.16s/it]iteration: 19, accuracy: 0.8471428571428572\n","iteration: 20, accuracy: 0.8385714285714285:  25% 20/80 [02:47<08:31,  8.52s/it]iteration: 20, accuracy: 0.8385714285714285\n","iteration: 21, accuracy: 0.8319047619047619:  26% 21/80 [02:57<08:13,  8.36s/it]iteration: 21, accuracy: 0.8319047619047619\n","iteration: 22, accuracy: 0.829047619047619:  28% 22/80 [03:04<08:26,  8.74s/it] iteration: 22, accuracy: 0.829047619047619\n","iteration: 23, accuracy: 0.8192857142857143:  29% 23/80 [03:12<07:52,  8.29s/it]iteration: 23, accuracy: 0.8192857142857143\n","iteration: 24, accuracy: 0.8114285714285714:  30% 24/80 [03:20<07:52,  8.45s/it]iteration: 24, accuracy: 0.8114285714285714\n","iteration: 25, accuracy: 0.805:  31% 25/80 [03:29<07:27,  8.13s/it]             iteration: 25, accuracy: 0.805\n","iteration: 26, accuracy: 0.7921428571428571:  32% 26/80 [03:39<07:45,  8.61s/it]iteration: 26, accuracy: 0.7921428571428571\n","iteration: 27, accuracy: 0.7852380952380953:  34% 27/80 [03:49<07:45,  8.78s/it]iteration: 27, accuracy: 0.7852380952380953\n","iteration: 28, accuracy: 0.775952380952381:  35% 28/80 [03:59<07:59,  9.22s/it] iteration: 28, accuracy: 0.775952380952381\n","iteration: 29, accuracy: 0.7697619047619048:  36% 29/80 [04:09<07:53,  9.29s/it]iteration: 29, accuracy: 0.7697619047619048\n","iteration: 30, accuracy: 0.7633333333333333:  38% 30/80 [04:16<07:54,  9.49s/it]iteration: 30, accuracy: 0.7633333333333333\n","iteration: 31, accuracy: 0.7571428571428571:  39% 31/80 [04:24<07:12,  8.82s/it]iteration: 31, accuracy: 0.7571428571428571\n","iteration: 32, accuracy: 0.7502380952380953:  40% 32/80 [04:35<07:05,  8.86s/it]iteration: 32, accuracy: 0.7502380952380953\n","iteration: 33, accuracy: 0.7409523809523809:  41% 33/80 [04:43<07:04,  9.02s/it]iteration: 33, accuracy: 0.7409523809523809\n","iteration: 34, accuracy: 0.7285714285714285:  42% 34/80 [04:52<06:59,  9.12s/it]iteration: 34, accuracy: 0.7285714285714285\n","iteration: 35, accuracy: 0.7252380952380952:  44% 35/80 [05:01<06:36,  8.81s/it]iteration: 35, accuracy: 0.7252380952380952\n","iteration: 36, accuracy: 0.7195238095238096:  45% 36/80 [05:11<06:45,  9.22s/it]iteration: 36, accuracy: 0.7195238095238096\n","iteration: 37, accuracy: 0.7083333333333334:  46% 37/80 [05:20<06:25,  8.97s/it]iteration: 37, accuracy: 0.7083333333333334\n","iteration: 38, accuracy: 0.7011904761904761:  48% 38/80 [05:28<06:28,  9.24s/it]iteration: 38, accuracy: 0.7011904761904761\n","iteration: 39, accuracy: 0.7016666666666667:  49% 39/80 [05:37<06:03,  8.88s/it]iteration: 39, accuracy: 0.7016666666666667\n","iteration: 40, accuracy: 0.6895238095238095:  50% 40/80 [05:46<06:04,  9.10s/it]iteration: 40, accuracy: 0.6895238095238095\n","iteration: 41, accuracy: 0.6857142857142857:  51% 41/80 [05:56<05:37,  8.66s/it]iteration: 41, accuracy: 0.6857142857142857\n","iteration: 42, accuracy: 0.6723809523809524:  52% 42/80 [06:04<05:53,  9.30s/it]iteration: 42, accuracy: 0.6723809523809524\n","iteration: 43, accuracy: 0.6683333333333333:  54% 43/80 [06:13<05:24,  8.76s/it]iteration: 43, accuracy: 0.6683333333333333\n","iteration: 44, accuracy: 0.6695238095238095:  55% 44/80 [06:22<05:25,  9.05s/it]iteration: 44, accuracy: 0.6695238095238095\n","iteration: 45, accuracy: 0.6695238095238095:  56% 45/80 [06:30<05:08,  8.80s/it]iteration: 45, accuracy: 0.6695238095238095\n","iteration: 46, accuracy: 0.6619047619047619:  57% 46/80 [06:39<05:03,  8.91s/it]iteration: 46, accuracy: 0.6619047619047619\n","iteration: 47, accuracy: 0.6561904761904762:  59% 47/80 [06:48<04:42,  8.56s/it]iteration: 47, accuracy: 0.6561904761904762\n","iteration: 48, accuracy: 0.6471428571428571:  60% 48/80 [06:57<04:49,  9.05s/it]iteration: 48, accuracy: 0.6471428571428571\n","iteration: 49, accuracy: 0.6435714285714286:  61% 49/80 [07:06<04:28,  8.65s/it]iteration: 49, accuracy: 0.6435714285714286\n","iteration: 50, accuracy: 0.6352380952380953:  62% 50/80 [07:15<04:36,  9.20s/it]iteration: 50, accuracy: 0.6352380952380953\n","iteration: 51, accuracy: 0.6364285714285715:  64% 51/80 [07:22<04:11,  8.68s/it]iteration: 51, accuracy: 0.6364285714285715\n","iteration: 52, accuracy: 0.6390476190476191:  65% 52/80 [07:32<03:59,  8.55s/it]iteration: 52, accuracy: 0.6390476190476191\n","iteration: 53, accuracy: 0.6373809523809524:  66% 53/80 [07:40<03:58,  8.83s/it]iteration: 53, accuracy: 0.6373809523809524\n","iteration: 54, accuracy: 0.635:  68% 54/80 [07:51<03:47,  8.77s/it]             iteration: 54, accuracy: 0.635\n","iteration: 55, accuracy: 0.638095238095238:  69% 55/80 [08:00<03:48,  9.16s/it]iteration: 55, accuracy: 0.638095238095238\n","iteration: 56, accuracy: 0.6359523809523809:  70% 56/80 [08:10<03:40,  9.19s/it]iteration: 56, accuracy: 0.6359523809523809\n","iteration: 57, accuracy: 0.6340476190476191:  71% 57/80 [08:19<03:31,  9.21s/it]iteration: 57, accuracy: 0.6340476190476191\n","iteration: 58, accuracy: 0.6411904761904762:  72% 58/80 [08:28<03:31,  9.60s/it]iteration: 58, accuracy: 0.6411904761904762\n","iteration: 59, accuracy: 0.6288095238095238:  74% 59/80 [08:35<03:07,  8.94s/it]iteration: 59, accuracy: 0.6288095238095238\n","iteration: 60, accuracy: 0.6228571428571429:  75% 60/80 [08:45<02:56,  8.85s/it]iteration: 60, accuracy: 0.6228571428571429\n","iteration: 61, accuracy: 0.6269047619047619:  76% 61/80 [08:53<02:51,  9.00s/it]iteration: 61, accuracy: 0.6269047619047619\n","iteration: 62, accuracy: 0.6285714285714286:  78% 62/80 [09:02<02:39,  8.87s/it]iteration: 62, accuracy: 0.6285714285714286\n","iteration: 63, accuracy: 0.6247619047619047:  79% 63/80 [09:09<02:24,  8.49s/it]iteration: 63, accuracy: 0.6247619047619047\n","iteration: 64, accuracy: 0.621904761904762:  80% 64/80 [09:18<02:13,  8.33s/it] iteration: 64, accuracy: 0.621904761904762\n","iteration: 65, accuracy: 0.6164285714285714:  81% 65/80 [09:25<02:03,  8.21s/it]iteration: 65, accuracy: 0.6164285714285714\n","iteration: 66, accuracy: 0.606904761904762:  82% 66/80 [09:34<01:53,  8.11s/it] iteration: 66, accuracy: 0.606904761904762\n","iteration: 67, accuracy: 0.6073809523809524:  84% 67/80 [09:42<01:45,  8.15s/it]iteration: 67, accuracy: 0.6073809523809524\n","iteration: 68, accuracy: 0.6004761904761905:  85% 68/80 [09:51<01:39,  8.33s/it]iteration: 68, accuracy: 0.6004761904761905\n","iteration: 69, accuracy: 0.5971428571428572:  86% 69/80 [09:58<01:31,  8.33s/it]iteration: 69, accuracy: 0.5971428571428572\n","iteration: 70, accuracy: 0.5988095238095238:  88% 70/80 [10:06<01:19,  7.90s/it]iteration: 70, accuracy: 0.5988095238095238\n","iteration: 71, accuracy: 0.5926190476190476:  89% 71/80 [10:14<01:14,  8.23s/it]iteration: 71, accuracy: 0.5926190476190476\n","iteration: 72, accuracy: 0.5935714285714285:  90% 72/80 [10:22<01:04,  8.00s/it]iteration: 72, accuracy: 0.5935714285714285\n","iteration: 73, accuracy: 0.585:  91% 73/80 [10:30<00:56,  8.08s/it]             iteration: 73, accuracy: 0.585\n","iteration: 74, accuracy: 0.5821428571428572:  92% 74/80 [10:38<00:48,  8.04s/it]iteration: 74, accuracy: 0.5821428571428572\n","iteration: 75, accuracy: 0.5747619047619048:  94% 75/80 [10:47<00:40,  8.06s/it]iteration: 75, accuracy: 0.5747619047619048\n","iteration: 76, accuracy: 0.5719047619047619:  95% 76/80 [10:54<00:32,  8.08s/it]iteration: 76, accuracy: 0.5719047619047619\n","iteration: 77, accuracy: 0.5726190476190476:  96% 77/80 [11:02<00:23,  7.91s/it]iteration: 77, accuracy: 0.5726190476190476\n","iteration: 78, accuracy: 0.5738095238095238:  98% 78/80 [11:09<00:16,  8.05s/it]iteration: 78, accuracy: 0.5738095238095238\n","iteration: 79, accuracy: 0.5714285714285714:  99% 79/80 [11:17<00:07,  7.71s/it]iteration: 79, accuracy: 0.5714285714285714\n","iteration: 79, accuracy: 0.5714285714285714: 100% 80/80 [11:19<00:00,  8.49s/it]\n","Saving computed projection matrix to: /content/drive/MyDrive/bias-bench-main/results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religion_s-0.pt\n"]}],"source":["! python experiments/inlp_projection_matrix.py --model AlbertModel --model_name_or_path albert-base-v2 --bias_type religion"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":166502,"status":"ok","timestamp":1690822101174,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"PUU9C194jBam","outputId":"31f0d6cf-c84c-44d1-f836-2a54d62b2556"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religion10000_s-0.pt\n"," - load_path: None\n"," - bias_type: gender\n","Evaluating gender examples.\n","100% 262/262 [02:38<00:00,  1.65it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 43.89\n","Stereotype score: 39.62\n","Anti-stereotype score: 50.49\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 43.89\n"]}],"source":["# matrix that has only removed bias related to religion\n","# crows_debias:gender\n","\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religion10000_s-0.pt --bias_type gender"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":262464,"status":"ok","timestamp":1690822753868,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"dSiLjbQgjBlu","outputId":"9cf24166-c77d-4824-f818-a47430433549"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/My Drive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religion10000_s-0.pt\n"," - load_path: None\n"," - bias_type: race\n","Evaluating race-color examples.\n"," 70% 363/516 [02:58<00:45,  3.35it/s]Skipping example 363.\n","100% 515/516 [04:11<00:00,  2.05it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 54.56\n","Stereotype score: 55.72\n","Anti-stereotype score: 41.86\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 54.56\n"]}],"source":["# matrix that has only removed bias related to religion\n","# crows_debias:race\n","\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religion10000_s-0.pt --bias_type race"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51056,"status":"ok","timestamp":1690822804919,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"-3gZ9erejB9I","outputId":"e56917a4-3f4e-4526-931b-52a127c3a2ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/My Drive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religion10000_s-0.pt\n"," - load_path: None\n"," - bias_type: religion\n","Evaluating religion examples.\n","100% 105/105 [00:43<00:00,  2.39it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 57.14\n","Stereotype score: 57.58\n","Anti-stereotype score: 50.0\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 57.14\n"]}],"source":["# matrix that has only removed bias related to religion\n","# crows_debias:religion\n","\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religion10000_s-0.pt --bias_type religion"]},{"cell_type":"markdown","source":["## 5.2 INLP Determining the optimal parameters for multi-type debiasing\n","\n","optimal dataset size:\n","\n","1.religion race gender 1000 1000 1000 80\n","\n","2.religion race gender 10000 10000 10000 80\n","\n","3.religion race gender 3000 3000 3000 80\n","\n","optimal number of training epochs:\n","\n","4.gender race religion 3000 3000 3000 30\n","\n","5.gender race religion 3000 3000 3000 80\n","\n","6.gender race religion 3000 3000 3000 100\n","\n","\n","\n"],"metadata":{"id":"SToDYVGh9bHG"}},{"cell_type":"markdown","source":["### 5.2.1 optimal dataset size 1000\n","religion race gender 1000 1000 1000 80\n","\n","link the three datasets sequentially according to religion race gender and put them into matrix generation. Modify context_nullspace_projection.py and inlp_projection_matrix.py"],"metadata":{"id":"SAKSYU-Ltem-"}},{"cell_type":"code","source":["\n","! python experiments/inlp_projection_matrix.py --model AlbertModel --model_name_or_path albert-base-v2 --bias_type race --n_classifiers 80"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p_41UjV6tc2P","executionInfo":{"status":"ok","timestamp":1692565513590,"user_tz":-60,"elapsed":65611,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"1f0714f1-fa97-421c-bb08-6bd97af82bc2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Computing projection matrix:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: AlbertModel\n"," - model_name_or_path: albert-base-v2\n"," - bias_type: race\n"," - n_classifiers: 80\n"," - seed: 0\n","Loading INLP data:   1% 9194/1372632 [00:00<01:37, 13923.09it/s]INLP dataset collected:\n"," - Num. male sentences: 1000\n"," - Num. female sentences: 1000\n"," - Num. neutral sentences: 1000\n","Loading INLP data:   1% 9399/1372632 [00:00<01:44, 13083.24it/s]\n","Loading INLP data:   2% 26838/1372632 [00:02<02:27, 9106.35it/s]INLP dataset collected:\n"," - Num. bias sentences: 1000\n"," - Num. neutral sentences: 1000\n","Loading INLP data:   2% 27691/1372632 [00:02<02:11, 10201.53it/s]\n","Loading INLP data:   3% 36718/1372632 [00:02<01:28, 15061.70it/s]INLP dataset collected:\n"," - Num. bias sentences: 1000\n"," - Num. neutral sentences: 1000\n","Loading INLP data:   3% 37378/1372632 [00:02<01:31, 14585.33it/s]\n","Downloading: 100% 684/684 [00:00<00:00, 3.95MB/s]\n","Downloading: 100% 45.2M/45.2M [00:00<00:00, 48.4MB/s]\n","Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.decoder.weight']\n","- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Downloading: 100% 742k/742k [00:00<00:00, 8.19MB/s]\n","Downloading: 100% 1.25M/1.25M [00:00<00:00, 11.8MB/s]\n","Encoding male sentences: 100% 1000/1000 [00:16<00:00, 61.44it/s]\n","Encoding female sentences: 100% 1000/1000 [00:13<00:00, 76.17it/s]\n","Encoding neutral sentences: 100% 1000/1000 [00:14<00:00, 70.57it/s]\n","Dataset split sizes:\n","Train size: 1470; Dev size: 630; Test size: 900\n","Encoding bias sentences: 100% 1000/1000 [00:14<00:00, 70.00it/s]\n","Encoding neutral sentences: 100% 1000/1000 [00:13<00:00, 71.76it/s]\n","Dataset split sizes:\n","Train size: 980; Dev size: 420; Test size: 600\n","Encoding bias sentences: 100% 1000/1000 [00:12<00:00, 79.23it/s]\n","Encoding neutral sentences: 100% 1000/1000 [00:13<00:00, 75.48it/s]\n","Dataset split sizes:\n","Train size: 980; Dev size: 420; Test size: 600\n","Dataset split sizes:\n","Train size: 3430; Dev size: 1470; Test size: 2100\n","iteration: 0, accuracy: 0.6238095238095238:   0% 0/80 [00:08<?, ?it/s]iteration: 0, accuracy: 0.6238095238095238\n","iteration: 1, accuracy: 0.6646258503401361:   1% 1/80 [00:16<11:29,  8.73s/it]iteration: 1, accuracy: 0.6646258503401361\n","iteration: 2, accuracy: 0.6598639455782312:   2% 2/80 [00:26<11:20,  8.73s/it]iteration: 2, accuracy: 0.6598639455782312\n","iteration: 3, accuracy: 0.6482993197278911:   4% 3/80 [00:32<11:32,  8.99s/it]iteration: 3, accuracy: 0.6482993197278911\n","iteration: 4, accuracy: 0.6496598639455783:   5% 4/80 [00:40<10:08,  8.00s/it]iteration: 4, accuracy: 0.6496598639455783\n","iteration: 5, accuracy: 0.6476190476190476:   6% 5/80 [00:47<10:02,  8.03s/it]iteration: 5, accuracy: 0.6476190476190476\n","iteration: 6, accuracy: 0.6360544217687075:   8% 6/80 [00:53<09:13,  7.49s/it]iteration: 6, accuracy: 0.6360544217687075\n","iteration: 7, accuracy: 0.6428571428571429:   9% 7/80 [01:01<08:51,  7.28s/it]iteration: 7, accuracy: 0.6428571428571429\n","iteration: 8, accuracy: 0.6428571428571429:  10% 8/80 [01:07<08:44,  7.28s/it]iteration: 8, accuracy: 0.6428571428571429\n","iteration: 9, accuracy: 0.6156462585034014:  11% 9/80 [01:14<08:13,  6.95s/it]iteration: 9, accuracy: 0.6156462585034014\n","iteration: 10, accuracy: 0.6054421768707483:  12% 10/80 [01:21<08:20,  7.15s/it]iteration: 10, accuracy: 0.6054421768707483\n","iteration: 11, accuracy: 0.6054421768707483:  14% 11/80 [01:30<08:04,  7.03s/it]iteration: 11, accuracy: 0.6054421768707483\n","iteration: 12, accuracy: 0.591156462585034:  15% 12/80 [01:38<08:38,  7.63s/it] iteration: 12, accuracy: 0.591156462585034\n","iteration: 13, accuracy: 0.5959183673469388:  16% 13/80 [01:44<08:27,  7.58s/it]iteration: 13, accuracy: 0.5959183673469388\n","iteration: 14, accuracy: 0.573469387755102:  18% 14/80 [01:52<07:50,  7.13s/it] iteration: 14, accuracy: 0.573469387755102\n","iteration: 15, accuracy: 0.5761904761904761:  19% 15/80 [01:58<07:55,  7.31s/it]iteration: 15, accuracy: 0.5761904761904761\n","iteration: 16, accuracy: 0.5700680272108843:  20% 16/80 [02:05<07:26,  6.98s/it]iteration: 16, accuracy: 0.5700680272108843\n","iteration: 17, accuracy: 0.563265306122449:  21% 17/80 [02:12<07:34,  7.21s/it] iteration: 17, accuracy: 0.563265306122449\n","iteration: 18, accuracy: 0.5564625850340136:  22% 18/80 [02:19<07:15,  7.03s/it]iteration: 18, accuracy: 0.5564625850340136\n","iteration: 19, accuracy: 0.5537414965986395:  24% 19/80 [02:26<07:15,  7.15s/it]iteration: 19, accuracy: 0.5537414965986395\n","iteration: 20, accuracy: 0.5496598639455782:  25% 20/80 [02:33<07:00,  7.01s/it]iteration: 20, accuracy: 0.5496598639455782\n","iteration: 21, accuracy: 0.5326530612244897:  26% 21/80 [02:40<06:50,  6.95s/it]iteration: 21, accuracy: 0.5326530612244897\n","iteration: 22, accuracy: 0.5503401360544218:  28% 22/80 [02:46<06:47,  7.03s/it]iteration: 22, accuracy: 0.5503401360544218\n","iteration: 23, accuracy: 0.5333333333333333:  29% 23/80 [02:52<06:13,  6.56s/it]iteration: 23, accuracy: 0.5333333333333333\n","iteration: 24, accuracy: 0.5292517006802722:  30% 24/80 [02:58<06:05,  6.53s/it]iteration: 24, accuracy: 0.5292517006802722\n","iteration: 25, accuracy: 0.5394557823129251:  31% 25/80 [03:04<05:51,  6.38s/it]iteration: 25, accuracy: 0.5394557823129251\n","iteration: 26, accuracy: 0.5292517006802722:  32% 26/80 [03:10<05:28,  6.08s/it]iteration: 26, accuracy: 0.5292517006802722\n","iteration: 27, accuracy: 0.5306122448979592:  34% 27/80 [03:16<05:34,  6.32s/it]iteration: 27, accuracy: 0.5306122448979592\n","iteration: 28, accuracy: 0.5244897959183673:  35% 28/80 [03:22<05:18,  6.12s/it]iteration: 28, accuracy: 0.5244897959183673\n","iteration: 29, accuracy: 0.5163265306122449:  36% 29/80 [03:29<05:15,  6.18s/it]iteration: 29, accuracy: 0.5163265306122449\n","iteration: 30, accuracy: 0.5081632653061224:  38% 30/80 [03:35<05:07,  6.14s/it]iteration: 30, accuracy: 0.5081632653061224\n","iteration: 31, accuracy: 0.5156462585034014:  39% 31/80 [03:41<05:00,  6.14s/it]iteration: 31, accuracy: 0.5156462585034014\n","iteration: 32, accuracy: 0.49863945578231295:  40% 32/80 [03:48<05:09,  6.44s/it]iteration: 32, accuracy: 0.49863945578231295\n","iteration: 33, accuracy: 0.49387755102040815:  41% 33/80 [03:54<04:55,  6.29s/it]iteration: 33, accuracy: 0.49387755102040815\n","iteration: 34, accuracy: 0.5054421768707483:  42% 34/80 [04:00<04:46,  6.24s/it] iteration: 34, accuracy: 0.5054421768707483\n","iteration: 35, accuracy: 0.5061224489795918:  44% 35/80 [04:05<04:39,  6.20s/it]iteration: 35, accuracy: 0.5061224489795918\n","iteration: 36, accuracy: 0.501360544217687:  45% 36/80 [04:11<04:21,  5.95s/it] iteration: 36, accuracy: 0.501360544217687\n","iteration: 37, accuracy: 0.5115646258503401:  46% 37/80 [04:17<04:13,  5.90s/it]iteration: 37, accuracy: 0.5115646258503401\n","iteration: 38, accuracy: 0.5149659863945578:  48% 38/80 [04:23<04:09,  5.95s/it]iteration: 38, accuracy: 0.5149659863945578\n","iteration: 39, accuracy: 0.5095238095238095:  49% 39/80 [04:29<04:00,  5.86s/it]iteration: 39, accuracy: 0.5095238095238095\n","iteration: 40, accuracy: 0.5061224489795918:  50% 40/80 [04:35<04:00,  6.02s/it]iteration: 40, accuracy: 0.5061224489795918\n","iteration: 41, accuracy: 0.49183673469387756:  51% 41/80 [04:40<03:47,  5.83s/it]iteration: 41, accuracy: 0.49183673469387756\n","iteration: 42, accuracy: 0.48775510204081635:  52% 42/80 [04:46<03:32,  5.59s/it]iteration: 42, accuracy: 0.48775510204081635\n","iteration: 43, accuracy: 0.49727891156462584:  54% 43/80 [04:51<03:35,  5.83s/it]iteration: 43, accuracy: 0.49727891156462584\n","iteration: 44, accuracy: 0.4884353741496599:  55% 44/80 [04:57<03:22,  5.61s/it] iteration: 44, accuracy: 0.4884353741496599\n","iteration: 45, accuracy: 0.49183673469387756:  56% 45/80 [05:03<03:15,  5.57s/it]iteration: 45, accuracy: 0.49183673469387756\n","iteration: 46, accuracy: 0.48639455782312924:  57% 46/80 [05:09<03:20,  5.90s/it]iteration: 46, accuracy: 0.48639455782312924\n","iteration: 47, accuracy: 0.4850340136054422:  59% 47/80 [05:14<03:10,  5.77s/it] iteration: 47, accuracy: 0.4850340136054422\n","iteration: 48, accuracy: 0.47891156462585033:  60% 48/80 [05:20<02:57,  5.55s/it]iteration: 48, accuracy: 0.47891156462585033\n","iteration: 49, accuracy: 0.48095238095238096:  61% 49/80 [05:25<02:55,  5.66s/it]iteration: 49, accuracy: 0.48095238095238096\n","iteration: 50, accuracy: 0.48435374149659866:  62% 50/80 [05:30<02:45,  5.50s/it]iteration: 50, accuracy: 0.48435374149659866\n","iteration: 51, accuracy: 0.48639455782312924:  64% 51/80 [05:36<02:42,  5.61s/it]iteration: 51, accuracy: 0.48639455782312924\n","iteration: 52, accuracy: 0.49047619047619045:  65% 52/80 [05:41<02:36,  5.58s/it]iteration: 52, accuracy: 0.49047619047619045\n","iteration: 53, accuracy: 0.4857142857142857:  66% 53/80 [05:46<02:24,  5.35s/it] iteration: 53, accuracy: 0.4857142857142857\n","iteration: 54, accuracy: 0.49727891156462584:  68% 54/80 [05:52<02:19,  5.35s/it]iteration: 54, accuracy: 0.49727891156462584\n","iteration: 55, accuracy: 0.4931972789115646:  69% 55/80 [05:57<02:17,  5.49s/it] iteration: 55, accuracy: 0.4931972789115646\n","iteration: 56, accuracy: 0.4897959183673469:  70% 56/80 [06:02<02:07,  5.30s/it]iteration: 56, accuracy: 0.4897959183673469\n","iteration: 57, accuracy: 0.4897959183673469:  71% 57/80 [06:08<02:01,  5.27s/it]iteration: 57, accuracy: 0.4897959183673469\n","iteration: 58, accuracy: 0.48299319727891155:  72% 58/80 [06:13<01:59,  5.42s/it]iteration: 58, accuracy: 0.48299319727891155\n","iteration: 59, accuracy: 0.4857142857142857:  74% 59/80 [06:18<01:49,  5.22s/it] iteration: 59, accuracy: 0.4857142857142857\n","iteration: 60, accuracy: 0.48299319727891155:  75% 60/80 [06:24<01:44,  5.22s/it]iteration: 60, accuracy: 0.48299319727891155\n","iteration: 61, accuracy: 0.4891156462585034:  76% 61/80 [06:29<01:42,  5.41s/it] iteration: 61, accuracy: 0.4891156462585034\n","iteration: 62, accuracy: 0.4891156462585034:  78% 62/80 [06:33<01:33,  5.20s/it]iteration: 62, accuracy: 0.4891156462585034\n","iteration: 63, accuracy: 0.48367346938775513:  79% 63/80 [06:40<01:27,  5.16s/it]iteration: 63, accuracy: 0.48367346938775513\n","iteration: 64, accuracy: 0.4816326530612245:  80% 64/80 [06:44<01:26,  5.43s/it] iteration: 64, accuracy: 0.4816326530612245\n","iteration: 65, accuracy: 0.4775510204081633:  81% 65/80 [06:48<01:15,  5.04s/it]iteration: 65, accuracy: 0.4775510204081633\n","iteration: 66, accuracy: 0.4891156462585034:  82% 66/80 [06:53<01:07,  4.84s/it]iteration: 66, accuracy: 0.4891156462585034\n","iteration: 67, accuracy: 0.48775510204081635:  84% 67/80 [06:59<01:05,  5.07s/it]iteration: 67, accuracy: 0.48775510204081635\n","iteration: 68, accuracy: 0.4931972789115646:  85% 68/80 [07:04<01:01,  5.09s/it] iteration: 68, accuracy: 0.4931972789115646\n","iteration: 69, accuracy: 0.48435374149659866:  86% 69/80 [07:09<00:54,  4.98s/it]iteration: 69, accuracy: 0.48435374149659866\n","iteration: 70, accuracy: 0.48775510204081635:  88% 70/80 [07:14<00:51,  5.15s/it]iteration: 70, accuracy: 0.48775510204081635\n","iteration: 71, accuracy: 0.4884353741496599:  89% 71/80 [07:19<00:46,  5.16s/it] iteration: 71, accuracy: 0.4884353741496599\n","iteration: 72, accuracy: 0.48095238095238096:  90% 72/80 [07:24<00:39,  4.95s/it]iteration: 72, accuracy: 0.48095238095238096\n","iteration: 73, accuracy: 0.46802721088435373:  91% 73/80 [07:29<00:35,  5.07s/it]iteration: 73, accuracy: 0.46802721088435373\n","iteration: 74, accuracy: 0.4673469387755102:  92% 74/80 [07:33<00:30,  5.05s/it] iteration: 74, accuracy: 0.4673469387755102\n","iteration: 75, accuracy: 0.47551020408163264:  94% 75/80 [07:38<00:24,  4.81s/it]iteration: 75, accuracy: 0.47551020408163264\n","iteration: 76, accuracy: 0.46802721088435373:  95% 76/80 [07:43<00:18,  4.65s/it]iteration: 76, accuracy: 0.46802721088435373\n","iteration: 77, accuracy: 0.4714285714285714:  96% 77/80 [07:48<00:14,  4.98s/it] iteration: 77, accuracy: 0.4714285714285714\n","iteration: 78, accuracy: 0.46870748299319726:  98% 78/80 [07:52<00:09,  4.84s/it]iteration: 78, accuracy: 0.46870748299319726\n","iteration: 79, accuracy: 0.4564625850340136:  99% 79/80 [07:57<00:04,  4.68s/it] iteration: 79, accuracy: 0.4564625850340136\n","iteration: 79, accuracy: 0.4564625850340136: 100% 80/80 [07:58<00:00,  5.98s/it]\n","Saving computed projection matrix to: /content/drive/MyDrive/bias-bench-main/results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-race_s-0.pt\n"]}]},{"cell_type":"code","source":["\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religionracegender1000_s-0.pt --bias_type gender"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x3lI9Mc9uk6U","executionInfo":{"status":"ok","timestamp":1692566540274,"user_tz":-60,"elapsed":24734,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"b93ef61f-d958-40aa-8e83-a7cee860804e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religionracegender1000_s-0.pt\n"," - load_path: None\n"," - bias_type: gender\n","Evaluating gender examples.\n","100% 262/262 [00:16<00:00, 15.86it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 46.95\n","Stereotype score: 57.86\n","Anti-stereotype score: 30.1\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 46.95\n"]}]},{"cell_type":"code","source":["! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religionracegender1000_s-0.pt --bias_type race"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QUDWkusHulGj","executionInfo":{"status":"ok","timestamp":1692566652033,"user_tz":-60,"elapsed":34175,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"4da14345-f1c9-4d20-b029-6a3d0702c5a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religionracegender1000_s-0.pt\n"," - load_path: None\n"," - bias_type: race\n","Evaluating race-color examples.\n"," 70% 361/516 [00:17<00:05, 29.27it/s]Skipping example 363.\n","100% 515/516 [00:25<00:00, 20.09it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 40.78\n","Stereotype score: 39.83\n","Anti-stereotype score: 51.16\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 40.78\n"]}]},{"cell_type":"code","source":["! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religionracegender1000_s-0.pt --bias_type religion"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k9N9tQu0ulTV","executionInfo":{"status":"ok","timestamp":1692566688768,"user_tz":-60,"elapsed":13665,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"0767ba6c-9ccf-45d6-9da9-abd6aab4b118"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religionracegender1000_s-0.pt\n"," - load_path: None\n"," - bias_type: religion\n","Evaluating religion examples.\n","100% 105/105 [00:04<00:00, 24.50it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 65.71\n","Stereotype score: 67.68\n","Anti-stereotype score: 33.33\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 65.71\n"]}]},{"cell_type":"markdown","source":["###5.2.2 optimal dataset size 10000\n","\n","religion race gender 10000 10000 10000 80"],"metadata":{"id":"MbUtPQOY-Atu"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17743488,"status":"ok","timestamp":1690854492878,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"ONDiT3ftQSw2","outputId":"2ac2b05b-c324-44ff-c088-0f377009074c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Computing projection matrix:\n"," - persistent_dir: /content/drive/My Drive/bias-bench-main\n"," - model: AlbertModel\n"," - model_name_or_path: albert-base-v2\n"," - bias_type: race\n"," - n_classifiers: 80\n"," - seed: 0\n","Loading INLP data:   6% 88174/1372632 [00:12<02:29, 8616.77it/s]INLP dataset collected:\n"," - Num. male sentences: 10000\n"," - Num. female sentences: 10000\n"," - Num. neutral sentences: 10000\n","Loading INLP data:   6% 88640/1372632 [00:13<03:08, 6810.74it/s]\n","Loading INLP data:  20% 280887/1372632 [00:30<01:41, 10708.36it/s]INLP dataset collected:\n"," - Num. bias sentences: 10000\n"," - Num. neutral sentences: 10000\n","Loading INLP data:  20% 280927/1372632 [00:30<01:58, 9219.42it/s] \n","Loading INLP data:  27% 364907/1372632 [00:49<01:37, 10289.72it/s]INLP dataset collected:\n"," - Num. bias sentences: 10000\n"," - Num. neutral sentences: 10000\n","Loading INLP data:  27% 365565/1372632 [00:49<02:17, 7340.80it/s] \n","Downloading: 100% 684/684 [00:00<00:00, 3.50MB/s]\n","Downloading: 100% 45.2M/45.2M [00:00<00:00, 92.7MB/s]\n","Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.bias']\n","- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Downloading: 100% 742k/742k [00:00<00:00, 3.83MB/s]\n","Downloading: 100% 1.25M/1.25M [00:00<00:00, 10.6MB/s]\n","Encoding male sentences: 100% 10000/10000 [28:30<00:00,  5.84it/s]\n","Encoding female sentences: 100% 10000/10000 [28:26<00:00,  5.86it/s]\n","Encoding neutral sentences: 100% 10000/10000 [24:15<00:00,  6.87it/s]\n","Dataset split sizes:\n","Train size: 14700; Dev size: 6300; Test size: 9000\n","Encoding bias sentences: 100% 10000/10000 [32:07<00:00,  5.19it/s]\n","Encoding neutral sentences: 100% 10000/10000 [24:43<00:00,  6.74it/s]\n","Dataset split sizes:\n","Train size: 9800; Dev size: 4200; Test size: 6000\n","Encoding bias sentences: 100% 10000/10000 [30:55<00:00,  5.39it/s]\n","Encoding neutral sentences: 100% 10000/10000 [24:27<00:00,  6.81it/s]\n","Dataset split sizes:\n","Train size: 9800; Dev size: 4200; Test size: 6000\n","Dataset split sizes:\n","Train size: 34300; Dev size: 14700; Test size: 21000\n","iteration: 0, accuracy: 0.7015646258503402:   0% 0/80 [03:18<?, ?it/s]iteration: 0, accuracy: 0.7015646258503402\n","iteration: 1, accuracy: 0.6970748299319728:   1% 1/80 [05:18<4:26:11, 202.17s/it]iteration: 1, accuracy: 0.6970748299319728\n","iteration: 2, accuracy: 0.6970068027210884:   2% 2/80 [07:02<3:20:43, 154.41s/it]iteration: 2, accuracy: 0.6970068027210884\n","iteration: 3, accuracy: 0.6844897959183673:   4% 3/80 [08:41<2:47:17, 130.36s/it]iteration: 3, accuracy: 0.6844897959183673\n","iteration: 4, accuracy: 0.6744217687074829:   5% 4/80 [10:27<2:30:49, 119.07s/it]iteration: 4, accuracy: 0.6744217687074829\n","iteration: 5, accuracy: 0.6641496598639456:   6% 5/80 [12:14<2:22:21, 113.89s/it]iteration: 5, accuracy: 0.6641496598639456\n","iteration: 6, accuracy: 0.6493877551020408:   8% 6/80 [13:51<2:17:29, 111.48s/it]iteration: 6, accuracy: 0.6493877551020408\n","iteration: 7, accuracy: 0.6361224489795918:   9% 7/80 [15:16<2:09:38, 106.55s/it]iteration: 7, accuracy: 0.6361224489795918\n","iteration: 8, accuracy: 0.6248299319727891:  10% 8/80 [16:55<1:59:58, 99.97s/it]iteration: 8, accuracy: 0.6248299319727891\n","iteration: 9, accuracy: 0.6098639455782313:  11% 9/80 [18:37<1:57:19, 99.15s/it]iteration: 9, accuracy: 0.6098639455782313\n","iteration: 10, accuracy: 0.6003401360544217:  12% 10/80 [20:09<1:56:53, 100.19s/it]iteration: 10, accuracy: 0.6003401360544217\n","iteration: 11, accuracy: 0.5892517006802721:  14% 11/80 [21:40<1:52:18, 97.66s/it]iteration: 11, accuracy: 0.5892517006802721\n","iteration: 12, accuracy: 0.5834013605442177:  15% 12/80 [23:09<1:48:22, 95.62s/it]iteration: 12, accuracy: 0.5834013605442177\n","iteration: 13, accuracy: 0.5774149659863945:  16% 13/80 [24:38<1:44:22, 93.48s/it]iteration: 13, accuracy: 0.5774149659863945\n","iteration: 14, accuracy: 0.573469387755102:  18% 14/80 [25:59<1:41:26, 92.22s/it] iteration: 14, accuracy: 0.573469387755102\n","iteration: 15, accuracy: 0.57:  19% 15/80 [27:26<1:36:50, 89.39s/it]             iteration: 15, accuracy: 0.57\n","iteration: 16, accuracy: 0.5630612244897959:  20% 16/80 [28:53<1:34:23, 88.50s/it]iteration: 16, accuracy: 0.5630612244897959\n","iteration: 17, accuracy: 0.5587074829931973:  21% 17/80 [30:17<1:32:11, 87.80s/it]iteration: 17, accuracy: 0.5587074829931973\n","iteration: 18, accuracy: 0.5529251700680272:  22% 18/80 [31:42<1:29:31, 86.64s/it]iteration: 18, accuracy: 0.5529251700680272\n","iteration: 19, accuracy: 0.5514965986394558:  24% 19/80 [33:05<1:27:51, 86.42s/it]iteration: 19, accuracy: 0.5514965986394558\n","iteration: 20, accuracy: 0.5480952380952381:  25% 20/80 [34:25<1:24:53, 84.89s/it]iteration: 20, accuracy: 0.5480952380952381\n","iteration: 21, accuracy: 0.5448299319727891:  26% 21/80 [35:46<1:22:25, 83.82s/it]iteration: 21, accuracy: 0.5448299319727891\n","iteration: 22, accuracy: 0.5463265306122449:  28% 22/80 [37:00<1:19:51, 82.62s/it]iteration: 22, accuracy: 0.5463265306122449\n","iteration: 23, accuracy: 0.5431972789115647:  29% 23/80 [38:11<1:16:11, 80.20s/it]iteration: 23, accuracy: 0.5431972789115647\n","iteration: 24, accuracy: 0.5372108843537415:  30% 24/80 [39:26<1:12:02, 77.18s/it]iteration: 24, accuracy: 0.5372108843537415\n","iteration: 25, accuracy: 0.5365986394557823:  31% 25/80 [40:42<1:10:53, 77.33s/it]iteration: 25, accuracy: 0.5365986394557823\n","iteration: 26, accuracy: 0.5360544217687074:  32% 26/80 [41:54<1:08:51, 76.51s/it]iteration: 26, accuracy: 0.5360544217687074\n","iteration: 27, accuracy: 0.5351020408163265:  34% 27/80 [42:59<1:06:20, 75.11s/it]iteration: 27, accuracy: 0.5351020408163265\n","iteration: 28, accuracy: 0.5329931972789116:  35% 28/80 [44:10<1:02:46, 72.44s/it]iteration: 28, accuracy: 0.5329931972789116\n","iteration: 29, accuracy: 0.5295238095238095:  36% 29/80 [45:22<1:01:16, 72.09s/it]iteration: 29, accuracy: 0.5295238095238095\n","iteration: 30, accuracy: 0.5289115646258503:  38% 30/80 [46:35<59:50, 71.82s/it]iteration: 30, accuracy: 0.5289115646258503\n","iteration: 31, accuracy: 0.5257142857142857:  39% 31/80 [47:58<59:06, 72.37s/it]iteration: 31, accuracy: 0.5257142857142857\n","iteration: 32, accuracy: 0.523469387755102:  40% 32/80 [49:09<59:59, 75.00s/it] iteration: 32, accuracy: 0.523469387755102\n","iteration: 33, accuracy: 0.5221768707482993:  41% 33/80 [50:30<57:45, 73.73s/it]iteration: 33, accuracy: 0.5221768707482993\n","iteration: 34, accuracy: 0.5223809523809524:  42% 34/80 [51:49<58:18, 76.07s/it]iteration: 34, accuracy: 0.5223809523809524\n","iteration: 35, accuracy: 0.5181632653061224:  44% 35/80 [53:11<57:53, 77.18s/it]iteration: 35, accuracy: 0.5181632653061224\n","iteration: 36, accuracy: 0.5162585034013606:  45% 36/80 [54:32<57:31, 78.44s/it]iteration: 36, accuracy: 0.5162585034013606\n","iteration: 37, accuracy: 0.5176870748299319:  46% 37/80 [55:58<57:07, 79.71s/it]iteration: 37, accuracy: 0.5176870748299319\n","iteration: 38, accuracy: 0.5141496598639456:  48% 38/80 [57:20<56:43, 81.05s/it]iteration: 38, accuracy: 0.5141496598639456\n","iteration: 39, accuracy: 0.5136734693877552:  49% 39/80 [58:34<55:54, 81.81s/it]iteration: 39, accuracy: 0.5136734693877552\n","iteration: 40, accuracy: 0.5120408163265306:  50% 40/80 [59:50<52:54, 79.35s/it]iteration: 40, accuracy: 0.5120408163265306\n","iteration: 41, accuracy: 0.508843537414966:  51% 41/80 [1:01:03<50:41, 77.98s/it]iteration: 41, accuracy: 0.508843537414966\n","iteration: 42, accuracy: 0.508843537414966:  52% 42/80 [1:02:14<48:31, 76.62s/it]iteration: 42, accuracy: 0.508843537414966\n","iteration: 43, accuracy: 0.5037414965986394:  54% 43/80 [1:03:27<46:24, 75.26s/it]iteration: 43, accuracy: 0.5037414965986394\n","iteration: 44, accuracy: 0.5037414965986394:  55% 44/80 [1:04:32<44:43, 74.53s/it]iteration: 44, accuracy: 0.5037414965986394\n","iteration: 45, accuracy: 0.501360544217687:  56% 45/80 [1:05:40<41:33, 71.23s/it] iteration: 45, accuracy: 0.501360544217687\n","iteration: 46, accuracy: 0.49993197278911566:  57% 46/80 [1:06:53<39:42, 70.07s/it]iteration: 46, accuracy: 0.49993197278911566\n","iteration: 47, accuracy: 0.4948299319727891:  59% 47/80 [1:07:59<39:11, 71.25s/it] iteration: 47, accuracy: 0.4948299319727891\n","iteration: 48, accuracy: 0.49455782312925173:  60% 48/80 [1:09:03<37:20, 70.01s/it]iteration: 48, accuracy: 0.49455782312925173\n","iteration: 49, accuracy: 0.49333333333333335:  61% 49/80 [1:10:11<34:55, 67.60s/it]iteration: 49, accuracy: 0.49333333333333335\n","iteration: 50, accuracy: 0.49394557823129254:  62% 50/80 [1:11:16<33:56, 67.90s/it]iteration: 50, accuracy: 0.49394557823129254\n","iteration: 51, accuracy: 0.49204081632653063:  64% 51/80 [1:12:22<32:31, 67.29s/it]iteration: 51, accuracy: 0.49204081632653063\n","iteration: 52, accuracy: 0.48836734693877554:  65% 52/80 [1:13:21<30:59, 66.43s/it]iteration: 52, accuracy: 0.48836734693877554\n","iteration: 53, accuracy: 0.4817687074829932:  66% 53/80 [1:14:20<29:14, 64.97s/it] iteration: 53, accuracy: 0.4817687074829932\n","iteration: 54, accuracy: 0.4869387755102041:  68% 54/80 [1:15:15<27:04, 62.49s/it]iteration: 54, accuracy: 0.4869387755102041\n","iteration: 55, accuracy: 0.4844897959183673:  69% 55/80 [1:16:10<25:09, 60.37s/it]iteration: 55, accuracy: 0.4844897959183673\n","iteration: 56, accuracy: 0.4825170068027211:  70% 56/80 [1:17:13<23:28, 58.69s/it]iteration: 56, accuracy: 0.4825170068027211\n","iteration: 57, accuracy: 0.48142857142857143:  71% 57/80 [1:18:16<23:06, 60.30s/it]iteration: 57, accuracy: 0.48142857142857143\n","iteration: 58, accuracy: 0.47401360544217686:  72% 58/80 [1:19:15<22:19, 60.90s/it]iteration: 58, accuracy: 0.47401360544217686\n","iteration: 59, accuracy: 0.47312925170068026:  74% 59/80 [1:20:16<21:14, 60.68s/it]iteration: 59, accuracy: 0.47312925170068026\n","iteration: 60, accuracy: 0.47034013605442176:  75% 60/80 [1:21:21<20:08, 60.43s/it]iteration: 60, accuracy: 0.47034013605442176\n","iteration: 61, accuracy: 0.4657823129251701:  76% 61/80 [1:22:17<19:44, 62.33s/it] iteration: 61, accuracy: 0.4657823129251701\n","iteration: 62, accuracy: 0.4654421768707483:  78% 62/80 [1:23:13<18:05, 60.29s/it]iteration: 62, accuracy: 0.4654421768707483\n","iteration: 63, accuracy: 0.45938775510204083:  79% 63/80 [1:24:10<16:42, 58.95s/it]iteration: 63, accuracy: 0.45938775510204083\n","iteration: 64, accuracy: 0.4586394557823129:  80% 64/80 [1:25:11<15:26, 57.92s/it] iteration: 64, accuracy: 0.4586394557823129\n","iteration: 65, accuracy: 0.4526530612244898:  81% 65/80 [1:26:10<14:45, 59.01s/it]iteration: 65, accuracy: 0.4526530612244898\n","iteration: 66, accuracy: 0.44945578231292516:  82% 66/80 [1:27:09<13:51, 59.37s/it]iteration: 66, accuracy: 0.44945578231292516\n","iteration: 67, accuracy: 0.4440816326530612:  84% 67/80 [1:28:05<12:42, 58.65s/it] iteration: 67, accuracy: 0.4440816326530612\n","iteration: 68, accuracy: 0.44197278911564625:  85% 68/80 [1:29:00<11:33, 57.79s/it]iteration: 68, accuracy: 0.44197278911564625\n","iteration: 69, accuracy: 0.4378231292517007:  86% 69/80 [1:29:57<10:28, 57.11s/it] iteration: 69, accuracy: 0.4378231292517007\n","iteration: 70, accuracy: 0.44:  88% 70/80 [1:30:53<09:34, 57.44s/it]              iteration: 70, accuracy: 0.44\n","iteration: 71, accuracy: 0.43802721088435376:  89% 71/80 [1:31:53<08:35, 57.30s/it]iteration: 71, accuracy: 0.43802721088435376\n","iteration: 72, accuracy: 0.43333333333333335:  90% 72/80 [1:32:55<07:42, 57.79s/it]iteration: 72, accuracy: 0.43333333333333335\n","iteration: 73, accuracy: 0.42448979591836733:  91% 73/80 [1:33:58<06:51, 58.83s/it]iteration: 73, accuracy: 0.42448979591836733\n","iteration: 74, accuracy: 0.42360544217687074:  92% 74/80 [1:34:58<06:02, 60.47s/it]iteration: 74, accuracy: 0.42360544217687074\n","iteration: 75, accuracy: 0.41625850340136056:  94% 75/80 [1:35:59<04:59, 59.88s/it]iteration: 75, accuracy: 0.41625850340136056\n","iteration: 76, accuracy: 0.41401360544217686:  95% 76/80 [1:36:59<04:02, 60.61s/it]iteration: 76, accuracy: 0.41401360544217686\n","iteration: 77, accuracy: 0.4104761904761905:  96% 77/80 [1:37:52<03:01, 60.45s/it] iteration: 77, accuracy: 0.4104761904761905\n","iteration: 78, accuracy: 0.40843537414965986:  98% 78/80 [1:38:55<01:56, 58.43s/it]iteration: 78, accuracy: 0.40843537414965986\n","iteration: 79, accuracy: 0.40714285714285714:  99% 79/80 [1:39:55<00:59, 59.23s/it]iteration: 79, accuracy: 0.40714285714285714\n","iteration: 79, accuracy: 0.40714285714285714: 100% 80/80 [1:39:58<00:00, 74.98s/it]\n","Saving computed projection matrix to: /content/drive/My Drive/bias-bench-main/results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-race_s-0.pt\n"]}],"source":["\n","! python experiments/inlp_projection_matrix.py --model AlbertModel --model_name_or_path albert-base-v2 --bias_type race --n_classifiers 80"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":177469,"status":"ok","timestamp":1690892943833,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"gQ9FseEE7DsX","outputId":"1eed4c68-0c27-4ffb-f266-1280b71b1c19"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/My Drive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religionracegender10000_s-0.pt\n"," - load_path: None\n"," - bias_type: gender\n","Downloading: 100% 684/684 [00:00<00:00, 2.82MB/s]\n","Downloading: 100% 45.2M/45.2M [00:00<00:00, 54.1MB/s]\n","Downloading: 100% 742k/742k [00:00<00:00, 4.01MB/s]\n","Downloading: 100% 1.25M/1.25M [00:00<00:00, 5.32MB/s]\n","Evaluating gender examples.\n","100% 262/262 [02:33<00:00,  1.71it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 45.8\n","Stereotype score: 52.83\n","Anti-stereotype score: 34.95\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 45.8\n"]}],"source":["\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religionracegender10000_s-0.pt --bias_type gender"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":163242,"status":"ok","timestamp":1690893200949,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"17HAH-fZ7Du0","outputId":"99171d9d-5be0-43f5-88a2-2985594de259"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religionracegender10000_s-0.pt\n"," - load_path: None\n"," - bias_type: race\n","Evaluating race-color examples.\n"," 70% 363/516 [02:51<00:49,  3.07it/s]Skipping example 363.\n","100% 515/516 [04:03<00:00,  2.12it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 38.45\n","Stereotype score: 36.23\n","Anti-stereotype score: 62.79\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 38.45\n"]}],"source":["\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religionracegender10000_s-0.pt --bias_type race"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60394,"status":"ok","timestamp":1690893270164,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"0TVHFGM07DxV","outputId":"8b523664-b365-4f10-fc21-ab66d2e0fed6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/My Drive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religionracegender10000_s-0.pt\n"," - load_path: None\n"," - bias_type: religion\n","Evaluating religion examples.\n","100% 105/105 [00:46<00:00,  2.28it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 64.76\n","Stereotype score: 65.66\n","Anti-stereotype score: 50.0\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 64.76\n"]}],"source":["\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religionracegender10000_s-0.pt --bias_type religion"]},{"cell_type":"markdown","source":["###5.2.3 optimal dataset size 3000\n","\n","religion race gender 3000 3000 3000 80"],"metadata":{"id":"7SRbO0Gr-EJl"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":89489,"status":"ok","timestamp":1690899690675,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"wqAO1BCe7Dzu","outputId":"9fcab19a-b678-4f96-b77c-137d576c09c5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Computing projection matrix:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: AlbertModel\n"," - model_name_or_path: albert-base-v2\n"," - bias_type: race\n"," - n_classifiers: 80\n"," - seed: 0\n","Loading INLP data:   2% 26533/1372632 [00:03<01:54, 11764.34it/s]INLP dataset collected:\n"," - Num. male sentences: 3000\n"," - Num. female sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   2% 27135/1372632 [00:04<03:18, 6780.12it/s] \n","Loading INLP data:   6% 85725/1372632 [00:10<01:59, 10741.01it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   6% 86562/1372632 [00:10<02:29, 8576.52it/s] \n","Loading INLP data:   8% 109724/1372632 [00:10<02:36, 8059.95it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   8% 110011/1372632 [00:10<01:57, 10708.74it/s]\n","Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias']\n","- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Encoding male sentences: 100% 3000/3000 [08:32<00:00,  5.86it/s]\n","Encoding female sentences: 100% 3000/3000 [08:39<00:00,  5.77it/s]\n","Encoding neutral sentences: 100% 3000/3000 [07:25<00:00,  6.73it/s]\n","Dataset split sizes:\n","Train size: 4410; Dev size: 1890; Test size: 2700\n","Encoding bias sentences: 100% 3000/3000 [09:45<00:00,  5.12it/s]\n","Encoding neutral sentences: 100% 3000/3000 [07:26<00:00,  6.72it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Encoding bias sentences: 100% 3000/3000 [09:29<00:00,  5.27it/s]\n","Encoding neutral sentences: 100% 3000/3000 [07:30<00:00,  6.65it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Dataset split sizes:\n","Train size: 10290; Dev size: 4410; Test size: 6300\n","iteration: 0, accuracy: 0.6802721088435374:   0% 0/80 [00:47<?, ?it/s]iteration: 0, accuracy: 0.6802721088435374\n","iteration: 1, accuracy: 0.6727891156462585:   1% 1/80 [01:24<1:04:33, 49.03s/it]iteration: 1, accuracy: 0.6727891156462585\n","iteration: 2, accuracy: 0.6723356009070295:   2% 2/80 [02:01<53:54, 41.47s/it]iteration: 2, accuracy: 0.6723356009070295\n","iteration: 3, accuracy: 0.6730158730158731:   4% 3/80 [02:39<50:46, 39.57s/it]iteration: 3, accuracy: 0.6730158730158731\n","iteration: 4, accuracy: 0.6687074829931973:   5% 4/80 [03:15<49:41, 39.23s/it]iteration: 4, accuracy: 0.6687074829931973\n","iteration: 5, accuracy: 0.6564625850340136:   6% 5/80 [03:47<47:46, 38.22s/it]iteration: 5, accuracy: 0.6564625850340136\n","iteration: 6, accuracy: 0.6462585034013606:   8% 6/80 [04:24<44:26, 36.04s/it]iteration: 6, accuracy: 0.6462585034013606\n","iteration: 7, accuracy: 0.6410430839002268:   9% 7/80 [05:00<43:58, 36.14s/it]iteration: 7, accuracy: 0.6410430839002268\n","iteration: 8, accuracy: 0.628344671201814:  10% 8/80 [05:32<43:17, 36.07s/it] iteration: 8, accuracy: 0.628344671201814\n","iteration: 9, accuracy: 0.6199546485260771:  11% 9/80 [06:02<40:56, 34.60s/it]iteration: 9, accuracy: 0.6199546485260771\n","iteration: 10, accuracy: 0.6158730158730159:  12% 10/80 [06:33<39:02, 33.46s/it]iteration: 10, accuracy: 0.6158730158730159\n","iteration: 11, accuracy: 0.6061224489795919:  14% 11/80 [07:02<37:33, 32.66s/it]iteration: 11, accuracy: 0.6061224489795919\n","iteration: 12, accuracy: 0.5943310657596372:  15% 12/80 [07:32<35:42, 31.50s/it]iteration: 12, accuracy: 0.5943310657596372\n","iteration: 13, accuracy: 0.5879818594104308:  16% 13/80 [07:58<34:30, 30.90s/it]iteration: 13, accuracy: 0.5879818594104308\n","iteration: 14, accuracy: 0.5829931972789115:  18% 14/80 [08:29<32:44, 29.77s/it]iteration: 14, accuracy: 0.5829931972789115\n","iteration: 15, accuracy: 0.5705215419501134:  19% 15/80 [08:58<32:26, 29.95s/it]iteration: 15, accuracy: 0.5705215419501134\n","iteration: 16, accuracy: 0.5653061224489796:  20% 16/80 [09:27<31:35, 29.62s/it]iteration: 16, accuracy: 0.5653061224489796\n","iteration: 17, accuracy: 0.5621315192743764:  21% 17/80 [09:56<30:47, 29.32s/it]iteration: 17, accuracy: 0.5621315192743764\n","iteration: 18, accuracy: 0.5501133786848073:  22% 18/80 [10:26<30:25, 29.45s/it]iteration: 18, accuracy: 0.5501133786848073\n","iteration: 19, accuracy: 0.5448979591836735:  24% 19/80 [10:53<30:03, 29.56s/it]iteration: 19, accuracy: 0.5448979591836735\n","iteration: 20, accuracy: 0.5501133786848073:  25% 20/80 [11:20<28:43, 28.72s/it]iteration: 20, accuracy: 0.5501133786848073\n","iteration: 21, accuracy: 0.5505668934240363:  26% 21/80 [11:47<27:53, 28.36s/it]iteration: 21, accuracy: 0.5505668934240363\n","iteration: 22, accuracy: 0.5421768707482993:  28% 22/80 [12:14<27:11, 28.14s/it]iteration: 22, accuracy: 0.5421768707482993\n","iteration: 23, accuracy: 0.5419501133786848:  29% 23/80 [12:41<26:09, 27.53s/it]iteration: 23, accuracy: 0.5419501133786848\n","iteration: 24, accuracy: 0.5417233560090703:  30% 24/80 [13:07<25:45, 27.59s/it]iteration: 24, accuracy: 0.5417233560090703\n","iteration: 25, accuracy: 0.5405895691609978:  31% 25/80 [13:34<24:36, 26.85s/it]iteration: 25, accuracy: 0.5405895691609978\n","iteration: 26, accuracy: 0.5435374149659864:  32% 26/80 [14:00<24:20, 27.05s/it]iteration: 26, accuracy: 0.5435374149659864\n","iteration: 27, accuracy: 0.5356009070294785:  34% 27/80 [14:24<23:24, 26.50s/it]iteration: 27, accuracy: 0.5356009070294785\n","iteration: 28, accuracy: 0.5297052154195011:  35% 28/80 [14:52<22:26, 25.89s/it]iteration: 28, accuracy: 0.5297052154195011\n","iteration: 29, accuracy: 0.5392290249433107:  36% 29/80 [15:16<22:33, 26.55s/it]iteration: 29, accuracy: 0.5392290249433107\n","iteration: 30, accuracy: 0.5326530612244897:  38% 30/80 [15:41<21:17, 25.55s/it]iteration: 30, accuracy: 0.5326530612244897\n","iteration: 31, accuracy: 0.5324263038548753:  39% 31/80 [16:07<21:04, 25.80s/it]iteration: 31, accuracy: 0.5324263038548753\n","iteration: 32, accuracy: 0.528344671201814:  40% 32/80 [16:35<20:30, 25.64s/it] iteration: 32, accuracy: 0.528344671201814\n","iteration: 33, accuracy: 0.5267573696145125:  41% 33/80 [17:02<20:48, 26.56s/it]iteration: 33, accuracy: 0.5267573696145125\n","iteration: 34, accuracy: 0.5231292517006803:  42% 34/80 [17:26<20:16, 26.46s/it]iteration: 34, accuracy: 0.5231292517006803\n","iteration: 35, accuracy: 0.51859410430839:  44% 35/80 [17:49<19:25, 25.90s/it]  iteration: 35, accuracy: 0.51859410430839\n","iteration: 36, accuracy: 0.5219954648526077:  45% 36/80 [18:11<18:10, 24.79s/it]iteration: 36, accuracy: 0.5219954648526077\n","iteration: 37, accuracy: 0.5238095238095238:  46% 37/80 [18:36<17:16, 24.10s/it]iteration: 37, accuracy: 0.5238095238095238\n","iteration: 38, accuracy: 0.517233560090703:  48% 38/80 [18:59<17:02, 24.34s/it] iteration: 38, accuracy: 0.517233560090703\n","iteration: 39, accuracy: 0.5147392290249433:  49% 39/80 [19:22<16:20, 23.92s/it]iteration: 39, accuracy: 0.5147392290249433\n","iteration: 40, accuracy: 0.5133786848072562:  50% 40/80 [19:47<15:55, 23.89s/it]iteration: 40, accuracy: 0.5133786848072562\n","iteration: 41, accuracy: 0.5070294784580499:  51% 41/80 [20:07<15:27, 23.79s/it]iteration: 41, accuracy: 0.5070294784580499\n","iteration: 42, accuracy: 0.5081632653061224:  52% 42/80 [20:28<14:20, 22.66s/it]iteration: 42, accuracy: 0.5081632653061224\n","iteration: 43, accuracy: 0.5002267573696145:  54% 43/80 [20:52<13:56, 22.60s/it]iteration: 43, accuracy: 0.5002267573696145\n","iteration: 44, accuracy: 0.5049886621315193:  55% 44/80 [21:16<13:36, 22.67s/it]iteration: 44, accuracy: 0.5049886621315193\n","iteration: 45, accuracy: 0.4977324263038549:  56% 45/80 [21:41<13:34, 23.28s/it]iteration: 45, accuracy: 0.4977324263038549\n","iteration: 46, accuracy: 0.500453514739229:  57% 46/80 [22:02<13:25, 23.70s/it] iteration: 46, accuracy: 0.500453514739229\n","iteration: 47, accuracy: 0.5006802721088436:  59% 47/80 [22:23<12:32, 22.81s/it]iteration: 47, accuracy: 0.5006802721088436\n","iteration: 48, accuracy: 0.5036281179138322:  60% 48/80 [22:46<12:01, 22.54s/it]iteration: 48, accuracy: 0.5036281179138322\n","iteration: 49, accuracy: 0.5006802721088436:  61% 49/80 [23:08<11:36, 22.48s/it]iteration: 49, accuracy: 0.5006802721088436\n","iteration: 50, accuracy: 0.5018140589569161:  62% 50/80 [23:30<11:04, 22.15s/it]iteration: 50, accuracy: 0.5018140589569161\n","iteration: 51, accuracy: 0.49705215419501136:  64% 51/80 [23:53<10:51, 22.46s/it]iteration: 51, accuracy: 0.49705215419501136\n","iteration: 52, accuracy: 0.49002267573696145:  65% 52/80 [24:14<10:23, 22.28s/it]iteration: 52, accuracy: 0.49002267573696145\n","iteration: 53, accuracy: 0.4931972789115646:  66% 53/80 [24:35<09:52, 21.96s/it] iteration: 53, accuracy: 0.4931972789115646\n","iteration: 54, accuracy: 0.48299319727891155:  68% 54/80 [24:57<09:31, 21.96s/it]iteration: 54, accuracy: 0.48299319727891155\n","iteration: 55, accuracy: 0.48299319727891155:  69% 55/80 [25:18<09:07, 21.90s/it]iteration: 55, accuracy: 0.48299319727891155\n","iteration: 56, accuracy: 0.4786848072562358:  70% 56/80 [25:40<08:35, 21.50s/it] iteration: 56, accuracy: 0.4786848072562358\n","iteration: 57, accuracy: 0.47891156462585033:  71% 57/80 [26:02<08:19, 21.71s/it]iteration: 57, accuracy: 0.47891156462585033\n","iteration: 58, accuracy: 0.47278911564625853:  72% 58/80 [26:22<08:03, 22.00s/it]iteration: 58, accuracy: 0.47278911564625853\n","iteration: 59, accuracy: 0.47392290249433106:  74% 59/80 [26:41<07:20, 20.99s/it]iteration: 59, accuracy: 0.47392290249433106\n","iteration: 60, accuracy: 0.47278911564625853:  75% 60/80 [27:00<06:48, 20.44s/it]iteration: 60, accuracy: 0.47278911564625853\n","iteration: 61, accuracy: 0.47052154195011336:  76% 61/80 [27:21<06:20, 20.03s/it]iteration: 61, accuracy: 0.47052154195011336\n","iteration: 62, accuracy: 0.46598639455782315:  78% 62/80 [27:41<06:08, 20.48s/it]iteration: 62, accuracy: 0.46598639455782315\n","iteration: 63, accuracy: 0.463265306122449:  79% 63/80 [28:03<05:46, 20.38s/it]  iteration: 63, accuracy: 0.463265306122449\n","iteration: 64, accuracy: 0.4605442176870748:  80% 64/80 [28:23<05:28, 20.56s/it]iteration: 64, accuracy: 0.4605442176870748\n","iteration: 65, accuracy: 0.4605442176870748:  81% 65/80 [28:42<05:08, 20.58s/it]iteration: 65, accuracy: 0.4605442176870748\n","iteration: 66, accuracy: 0.4598639455782313:  82% 66/80 [29:01<04:39, 19.95s/it]iteration: 66, accuracy: 0.4598639455782313\n","iteration: 67, accuracy: 0.45782312925170066:  84% 67/80 [29:23<04:21, 20.09s/it]iteration: 67, accuracy: 0.45782312925170066\n","iteration: 68, accuracy: 0.4560090702947846:  85% 68/80 [29:44<04:06, 20.53s/it] iteration: 68, accuracy: 0.4560090702947846\n","iteration: 69, accuracy: 0.4580498866213152:  86% 69/80 [30:04<03:44, 20.44s/it]iteration: 69, accuracy: 0.4580498866213152\n","iteration: 70, accuracy: 0.4594104308390023:  88% 70/80 [30:23<03:22, 20.25s/it]iteration: 70, accuracy: 0.4594104308390023\n","iteration: 71, accuracy: 0.45374149659863944:  89% 71/80 [30:46<02:59, 19.99s/it]iteration: 71, accuracy: 0.45374149659863944\n","iteration: 72, accuracy: 0.45374149659863944:  90% 72/80 [31:06<02:48, 21.04s/it]iteration: 72, accuracy: 0.45374149659863944\n","iteration: 73, accuracy: 0.4505668934240363:  91% 73/80 [31:24<02:23, 20.53s/it] iteration: 73, accuracy: 0.4505668934240363\n","iteration: 74, accuracy: 0.4530612244897959:  92% 74/80 [31:46<01:58, 19.83s/it]iteration: 74, accuracy: 0.4530612244897959\n","iteration: 75, accuracy: 0.44285714285714284:  94% 75/80 [32:07<01:41, 20.39s/it]iteration: 75, accuracy: 0.44285714285714284\n","iteration: 76, accuracy: 0.44126984126984126:  95% 76/80 [32:27<01:23, 20.82s/it]iteration: 76, accuracy: 0.44126984126984126\n","iteration: 77, accuracy: 0.43945578231292515:  96% 77/80 [32:47<01:01, 20.45s/it]iteration: 77, accuracy: 0.43945578231292515\n","iteration: 78, accuracy: 0.4324263038548753:  98% 78/80 [33:06<00:40, 20.39s/it] iteration: 78, accuracy: 0.4324263038548753\n","iteration: 79, accuracy: 0.4328798185941043:  99% 79/80 [33:24<00:19, 19.72s/it]iteration: 79, accuracy: 0.4328798185941043\n","iteration: 79, accuracy: 0.4328798185941043: 100% 80/80 [33:25<00:00, 25.07s/it]\n","Saving computed projection matrix to: /content/drive/MyDrive/bias-bench-main/results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-race_s-0.pt\n"]}],"source":["\n","! python experiments/inlp_projection_matrix.py --model AlbertModel --model_name_or_path albert-base-v2 --bias_type race --n_classifiers 80"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":221193,"status":"ok","timestamp":1690900100370,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"edmqdkvSQSzM","outputId":"de43d770-a9bc-4fc4-89e0-8eaa866e245c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religionracegender3000_s-0.pt\n"," - load_path: None\n"," - bias_type: gender\n","Evaluating gender examples.\n","100% 262/262 [03:29<00:00,  1.25it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 44.66\n","Stereotype score: 57.23\n","Anti-stereotype score: 25.24\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 44.66\n"]}],"source":["\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religionracegender3000_s-0.pt --bias_type gender"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":155363,"status":"ok","timestamp":1690900366421,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"YFjhY8GsIXAf","outputId":"1a0c2bd1-3f64-4908-d982-21cbd9b28944"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religionracegender3000_s-0.pt\n"," - load_path: None\n"," - bias_type: race\n","Evaluating race-color examples.\n"," 70% 363/516 [02:59<01:07,  2.26it/s]Skipping example 363.\n","100% 515/516 [04:19<00:00,  1.99it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 47.57\n","Stereotype score: 46.4\n","Anti-stereotype score: 60.47\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 47.57\n"]}],"source":["\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religionracegender3000_s-0.pt --bias_type race"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55183,"status":"ok","timestamp":1690900421601,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"hDS45M4kIXD4","outputId":"a8a7743b-2476-4c38-b548-82d613de8b44"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religionracegender3000_s-0.pt\n"," - load_path: None\n"," - bias_type: religion\n","Evaluating religion examples.\n","100% 105/105 [00:46<00:00,  2.27it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 56.19\n","Stereotype score: 56.57\n","Anti-stereotype score: 50.0\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 56.19\n"]}],"source":["\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religionracegender3000_s-0.pt --bias_type religion"]},{"cell_type":"markdown","source":["### 5.2.4 optimal number of training epochs 30\n","gender race religion 3000 30"],"metadata":{"id":"yWBCUp-T3adi"}},{"cell_type":"code","source":["\n","! python experiments/inlp_projection_matrix.py --model AlbertModel --model_name_or_path albert-base-v2 --bias_type race --n_classifiers 30"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tr3AD7-S3awE","executionInfo":{"status":"ok","timestamp":1692568208094,"user_tz":-60,"elapsed":933777,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"55ed950d-8f22-4b1a-9828-4efea784a930"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Computing projection matrix:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: AlbertModel\n"," - model_name_or_path: albert-base-v2\n"," - bias_type: race\n"," - n_classifiers: 30\n"," - seed: 0\n","Loading INLP data:   2% 26477/1372632 [00:02<01:34, 14235.84it/s]INLP dataset collected:\n"," - Num. male sentences: 3000\n"," - Num. female sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   2% 27135/1372632 [00:02<02:18, 9698.32it/s] \n","Loading INLP data:   6% 85258/1372632 [00:05<01:23, 15403.71it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   6% 86562/1372632 [00:05<01:23, 15362.67it/s]\n","Loading INLP data:   8% 109986/1372632 [00:08<01:22, 15360.49it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   8% 110011/1372632 [00:08<01:39, 12628.51it/s]\n","Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.decoder.bias']\n","- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Encoding male sentences: 100% 3000/3000 [00:45<00:00, 65.22it/s]\n","Encoding female sentences: 100% 3000/3000 [00:43<00:00, 69.57it/s]\n","Encoding neutral sentences: 100% 3000/3000 [00:42<00:00, 69.80it/s]\n","Dataset split sizes:\n","Train size: 4410; Dev size: 1890; Test size: 2700\n","Encoding bias sentences: 100% 3000/3000 [00:41<00:00, 72.93it/s]\n","Encoding neutral sentences: 100% 3000/3000 [00:42<00:00, 70.38it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Encoding bias sentences: 100% 3000/3000 [00:40<00:00, 73.47it/s]\n","Encoding neutral sentences: 100% 3000/3000 [00:40<00:00, 74.16it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Dataset split sizes:\n","Train size: 10290; Dev size: 4410; Test size: 6300\n","iteration: 0, accuracy: 0.6804988662131519:   0% 0/30 [00:29<?, ?it/s]iteration: 0, accuracy: 0.6804988662131519\n","iteration: 1, accuracy: 0.6727891156462585:   3% 1/30 [00:54<14:27, 29.90s/it]iteration: 1, accuracy: 0.6727891156462585\n","iteration: 2, accuracy: 0.6723356009070295:   7% 2/30 [01:16<12:55, 27.70s/it]iteration: 2, accuracy: 0.6723356009070295\n","iteration: 3, accuracy: 0.672562358276644:  10% 3/30 [01:40<11:14, 24.98s/it] iteration: 3, accuracy: 0.672562358276644\n","iteration: 4, accuracy: 0.6689342403628118:  13% 4/30 [02:03<10:42, 24.70s/it]iteration: 4, accuracy: 0.6689342403628118\n","iteration: 5, accuracy: 0.6569160997732426:  17% 5/30 [02:23<09:52, 23.72s/it]iteration: 5, accuracy: 0.6569160997732426\n","iteration: 6, accuracy: 0.645578231292517:  20% 6/30 [02:47<08:58, 22.43s/it] iteration: 6, accuracy: 0.645578231292517\n","iteration: 7, accuracy: 0.6419501133786848:  23% 7/30 [03:09<08:51, 23.09s/it]iteration: 7, accuracy: 0.6419501133786848\n","iteration: 8, accuracy: 0.626077097505669:  27% 8/30 [03:30<08:25, 22.96s/it] iteration: 8, accuracy: 0.626077097505669\n","iteration: 9, accuracy: 0.6199546485260771:  30% 9/30 [03:49<07:44, 22.12s/it]iteration: 9, accuracy: 0.6199546485260771\n","iteration: 10, accuracy: 0.6111111111111112:  33% 10/30 [04:09<07:02, 21.13s/it]iteration: 10, accuracy: 0.6111111111111112\n","iteration: 11, accuracy: 0.6058956916099774:  37% 11/30 [04:27<06:34, 20.79s/it]iteration: 11, accuracy: 0.6058956916099774\n","iteration: 12, accuracy: 0.590702947845805:  40% 12/30 [04:47<05:59, 19.96s/it] iteration: 12, accuracy: 0.590702947845805\n","iteration: 13, accuracy: 0.5888888888888889:  43% 13/30 [05:06<05:41, 20.12s/it]iteration: 13, accuracy: 0.5888888888888889\n","iteration: 14, accuracy: 0.5841269841269842:  47% 14/30 [05:26<05:16, 19.77s/it]iteration: 14, accuracy: 0.5841269841269842\n","iteration: 15, accuracy: 0.5700680272108843:  50% 15/30 [05:46<04:54, 19.66s/it]iteration: 15, accuracy: 0.5700680272108843\n","iteration: 16, accuracy: 0.5664399092970521:  53% 16/30 [06:05<04:38, 19.88s/it]iteration: 16, accuracy: 0.5664399092970521\n","iteration: 17, accuracy: 0.5598639455782313:  57% 17/30 [06:23<04:15, 19.69s/it]iteration: 17, accuracy: 0.5598639455782313\n","iteration: 18, accuracy: 0.5487528344671202:  60% 18/30 [06:42<03:49, 19.11s/it]iteration: 18, accuracy: 0.5487528344671202\n","iteration: 19, accuracy: 0.5462585034013605:  63% 19/30 [07:03<03:29, 19.03s/it]iteration: 19, accuracy: 0.5462585034013605\n","iteration: 20, accuracy: 0.5530612244897959:  67% 20/30 [07:21<03:16, 19.65s/it]iteration: 20, accuracy: 0.5530612244897959\n","iteration: 21, accuracy: 0.5521541950113379:  70% 21/30 [07:38<02:52, 19.18s/it]iteration: 21, accuracy: 0.5521541950113379\n","iteration: 22, accuracy: 0.5437641723356009:  73% 22/30 [07:55<02:29, 18.65s/it]iteration: 22, accuracy: 0.5437641723356009\n","iteration: 23, accuracy: 0.5426303854875284:  77% 23/30 [08:14<02:07, 18.23s/it]iteration: 23, accuracy: 0.5426303854875284\n","iteration: 24, accuracy: 0.5399092970521542:  80% 24/30 [08:31<01:49, 18.31s/it]iteration: 24, accuracy: 0.5399092970521542\n","iteration: 25, accuracy: 0.5471655328798186:  83% 25/30 [08:49<01:29, 17.87s/it]iteration: 25, accuracy: 0.5471655328798186\n","iteration: 26, accuracy: 0.5446712018140589:  87% 26/30 [09:06<01:11, 17.87s/it]iteration: 26, accuracy: 0.5446712018140589\n","iteration: 27, accuracy: 0.536734693877551:  90% 27/30 [09:22<00:53, 17.71s/it] iteration: 27, accuracy: 0.536734693877551\n","iteration: 28, accuracy: 0.5312925170068027:  93% 28/30 [09:41<00:34, 17.35s/it]iteration: 28, accuracy: 0.5312925170068027\n","iteration: 29, accuracy: 0.5328798185941043:  97% 29/30 [09:58<00:17, 17.63s/it]iteration: 29, accuracy: 0.5328798185941043\n","iteration: 29, accuracy: 0.5328798185941043: 100% 30/30 [09:59<00:00, 19.98s/it]\n","Saving computed projection matrix to: /content/drive/MyDrive/bias-bench-main/results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-race_s-0.pt\n"]}]},{"cell_type":"code","source":["! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-genderracereligion3000_30_s-0.pt --bias_type gender"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WLPIqf4c3a2F","executionInfo":{"status":"ok","timestamp":1692568458242,"user_tz":-60,"elapsed":26337,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"c87a04ce-8499-44f1-a4f1-726d6170e2b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-genderracereligion3000_30_s-0.pt\n"," - load_path: None\n"," - bias_type: gender\n","Evaluating gender examples.\n","100% 262/262 [00:17<00:00, 15.39it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 52.67\n","Stereotype score: 58.49\n","Anti-stereotype score: 43.69\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 52.67\n"]}]},{"cell_type":"code","source":["! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-genderracereligion3000_30_s-0.pt --bias_type race"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mr9e1YRg3bIb","executionInfo":{"status":"ok","timestamp":1692568494398,"user_tz":-60,"elapsed":33717,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"610f6ccd-8dba-4b18-ed0a-cf9b7122d6e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-genderracereligion3000_30_s-0.pt\n"," - load_path: None\n"," - bias_type: race\n","Evaluating race-color examples.\n"," 70% 362/516 [00:18<00:08, 17.73it/s]Skipping example 363.\n","100% 515/516 [00:24<00:00, 20.82it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 38.06\n","Stereotype score: 37.15\n","Anti-stereotype score: 48.84\n","Num. neutral: 0.19\n","====================================================================================================\n","\n","Metric: 38.06\n"]}]},{"cell_type":"code","source":["! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-genderracereligion3000_30_s-0.pt --bias_type religion"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gN-9Mfiz3bVM","executionInfo":{"status":"ok","timestamp":1692568534834,"user_tz":-60,"elapsed":13968,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"fa81fd02-fd40-4df7-ca57-414e9eb972bd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-genderracereligion3000_30_s-0.pt\n"," - load_path: None\n"," - bias_type: religion\n","Evaluating religion examples.\n","100% 105/105 [00:06<00:00, 16.36it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 79.05\n","Stereotype score: 80.81\n","Anti-stereotype score: 50.0\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 79.05\n"]}]},{"cell_type":"markdown","source":["### 5.2.5 optimal number of training epochs 80\n","gender race religion 3000 80"],"metadata":{"id":"l5yGR14q8wLz"}},{"cell_type":"code","source":["! python experiments/inlp_projection_matrix.py --model AlbertModel --model_name_or_path albert-base-v2 --bias_type race --n_classifiers 80"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PVk79YBV80sk","executionInfo":{"status":"ok","timestamp":1692570510868,"user_tz":-60,"elapsed":1743226,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"c94c6b0b-0a8f-44c7-8cda-997fef04350e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Computing projection matrix:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: AlbertModel\n"," - model_name_or_path: albert-base-v2\n"," - bias_type: race\n"," - n_classifiers: 80\n"," - seed: 0\n","Loading INLP data:   2% 26411/1372632 [00:02<01:42, 13170.47it/s]INLP dataset collected:\n"," - Num. male sentences: 3000\n"," - Num. female sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   2% 27135/1372632 [00:02<01:44, 12871.72it/s]\n","Loading INLP data:   6% 86248/1372632 [00:06<01:29, 14358.22it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   6% 86562/1372632 [00:06<01:43, 12406.92it/s]\n","Loading INLP data:   8% 109509/1372632 [00:10<01:55, 10897.42it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   8% 110011/1372632 [00:10<02:02, 10277.21it/s]\n","Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.weight', 'predictions.bias']\n","- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Encoding male sentences: 100% 3000/3000 [00:44<00:00, 66.75it/s]\n","Encoding female sentences: 100% 3000/3000 [00:43<00:00, 69.34it/s]\n","Encoding neutral sentences: 100% 3000/3000 [00:46<00:00, 64.66it/s]\n","Dataset split sizes:\n","Train size: 4410; Dev size: 1890; Test size: 2700\n","Encoding bias sentences: 100% 3000/3000 [00:43<00:00, 69.17it/s]\n","Encoding neutral sentences: 100% 3000/3000 [00:45<00:00, 66.43it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Encoding bias sentences: 100% 3000/3000 [00:43<00:00, 69.42it/s]\n","Encoding neutral sentences: 100% 3000/3000 [00:44<00:00, 67.19it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Dataset split sizes:\n","Train size: 10290; Dev size: 4410; Test size: 6300\n","iteration: 0, accuracy: 0.6804988662131519:   0% 0/80 [00:29<?, ?it/s]iteration: 0, accuracy: 0.6804988662131519\n","iteration: 1, accuracy: 0.6727891156462585:   1% 1/80 [00:56<40:58, 31.12s/it]iteration: 1, accuracy: 0.6727891156462585\n","iteration: 2, accuracy: 0.6723356009070295:   2% 2/80 [01:23<36:36, 28.16s/it]iteration: 2, accuracy: 0.6723356009070295\n","iteration: 3, accuracy: 0.672562358276644:   4% 3/80 [01:49<35:49, 27.91s/it] iteration: 3, accuracy: 0.672562358276644\n","iteration: 4, accuracy: 0.6689342403628118:   5% 4/80 [02:11<34:16, 27.06s/it]iteration: 4, accuracy: 0.6689342403628118\n","iteration: 5, accuracy: 0.6569160997732426:   6% 5/80 [02:31<31:25, 25.14s/it]iteration: 5, accuracy: 0.6569160997732426\n","iteration: 6, accuracy: 0.645578231292517:   8% 6/80 [02:57<29:09, 23.64s/it] iteration: 6, accuracy: 0.645578231292517\n","iteration: 7, accuracy: 0.6419501133786848:   9% 7/80 [03:20<29:29, 24.24s/it]iteration: 7, accuracy: 0.6419501133786848\n","iteration: 8, accuracy: 0.626077097505669:  10% 8/80 [03:41<28:22, 23.65s/it] iteration: 8, accuracy: 0.626077097505669\n","iteration: 9, accuracy: 0.6199546485260771:  11% 9/80 [04:01<27:20, 23.11s/it]iteration: 9, accuracy: 0.6199546485260771\n","iteration: 10, accuracy: 0.6111111111111112:  12% 10/80 [04:23<26:03, 22.34s/it]iteration: 10, accuracy: 0.6111111111111112\n","iteration: 11, accuracy: 0.6058956916099774:  14% 11/80 [04:42<25:20, 22.04s/it]iteration: 11, accuracy: 0.6058956916099774\n","iteration: 12, accuracy: 0.590702947845805:  15% 12/80 [05:02<23:44, 20.95s/it] iteration: 12, accuracy: 0.590702947845805\n","iteration: 13, accuracy: 0.5888888888888889:  16% 13/80 [05:24<23:02, 20.64s/it]iteration: 13, accuracy: 0.5888888888888889\n","iteration: 14, accuracy: 0.5841269841269842:  18% 14/80 [05:43<23:07, 21.03s/it]iteration: 14, accuracy: 0.5841269841269842\n","iteration: 15, accuracy: 0.5700680272108843:  19% 15/80 [06:02<22:18, 20.59s/it]iteration: 15, accuracy: 0.5700680272108843\n","iteration: 16, accuracy: 0.5664399092970521:  20% 16/80 [06:23<21:38, 20.28s/it]iteration: 16, accuracy: 0.5664399092970521\n","iteration: 17, accuracy: 0.5598639455782313:  21% 17/80 [06:42<21:20, 20.33s/it]iteration: 17, accuracy: 0.5598639455782313\n","iteration: 18, accuracy: 0.5487528344671202:  22% 18/80 [07:01<20:37, 19.96s/it]iteration: 18, accuracy: 0.5487528344671202\n","iteration: 19, accuracy: 0.5462585034013605:  24% 19/80 [07:21<19:50, 19.52s/it]iteration: 19, accuracy: 0.5462585034013605\n","iteration: 20, accuracy: 0.5530612244897959:  25% 20/80 [07:39<19:48, 19.80s/it]iteration: 20, accuracy: 0.5530612244897959\n","iteration: 21, accuracy: 0.5521541950113379:  26% 21/80 [07:57<18:56, 19.26s/it]iteration: 21, accuracy: 0.5521541950113379\n","iteration: 22, accuracy: 0.5437641723356009:  28% 22/80 [08:14<18:02, 18.66s/it]iteration: 22, accuracy: 0.5437641723356009\n","iteration: 23, accuracy: 0.5426303854875284:  29% 23/80 [08:32<17:27, 18.38s/it]iteration: 23, accuracy: 0.5426303854875284\n","iteration: 24, accuracy: 0.5399092970521542:  30% 24/80 [08:49<16:53, 18.11s/it]iteration: 24, accuracy: 0.5399092970521542\n","iteration: 25, accuracy: 0.5471655328798186:  31% 25/80 [09:06<16:14, 17.72s/it]iteration: 25, accuracy: 0.5471655328798186\n","iteration: 26, accuracy: 0.5446712018140589:  32% 26/80 [09:24<15:56, 17.71s/it]iteration: 26, accuracy: 0.5446712018140589\n","iteration: 27, accuracy: 0.536734693877551:  34% 27/80 [09:41<15:35, 17.65s/it] iteration: 27, accuracy: 0.536734693877551\n","iteration: 28, accuracy: 0.5312925170068027:  35% 28/80 [10:01<15:17, 17.65s/it]iteration: 28, accuracy: 0.5312925170068027\n","iteration: 29, accuracy: 0.5328798185941043:  36% 29/80 [10:19<15:31, 18.26s/it]iteration: 29, accuracy: 0.5328798185941043\n","iteration: 30, accuracy: 0.5351473922902494:  38% 30/80 [10:39<15:16, 18.33s/it]iteration: 30, accuracy: 0.5351473922902494\n","iteration: 31, accuracy: 0.5356009070294785:  39% 31/80 [10:58<15:24, 18.86s/it]iteration: 31, accuracy: 0.5356009070294785\n","iteration: 32, accuracy: 0.5242630385487528:  40% 32/80 [11:16<15:06, 18.88s/it]iteration: 32, accuracy: 0.5242630385487528\n","iteration: 33, accuracy: 0.5272108843537415:  41% 33/80 [11:33<14:32, 18.56s/it]iteration: 33, accuracy: 0.5272108843537415\n","iteration: 34, accuracy: 0.5208616780045352:  42% 34/80 [11:51<13:51, 18.08s/it]iteration: 34, accuracy: 0.5208616780045352\n","iteration: 35, accuracy: 0.5204081632653061:  44% 35/80 [12:08<13:21, 17.81s/it]iteration: 35, accuracy: 0.5204081632653061\n","iteration: 36, accuracy: 0.5219954648526077:  45% 36/80 [12:23<12:55, 17.63s/it]iteration: 36, accuracy: 0.5219954648526077\n","iteration: 37, accuracy: 0.5204081632653061:  46% 37/80 [12:41<12:19, 17.19s/it]iteration: 37, accuracy: 0.5204081632653061\n","iteration: 38, accuracy: 0.5160997732426303:  48% 38/80 [12:59<12:03, 17.23s/it]iteration: 38, accuracy: 0.5160997732426303\n","iteration: 39, accuracy: 0.5167800453514739:  49% 39/80 [13:15<11:55, 17.46s/it]iteration: 39, accuracy: 0.5167800453514739\n","iteration: 40, accuracy: 0.5138321995464853:  50% 40/80 [13:33<11:27, 17.18s/it]iteration: 40, accuracy: 0.5138321995464853\n","iteration: 41, accuracy: 0.5090702947845805:  51% 41/80 [13:49<11:13, 17.27s/it]iteration: 41, accuracy: 0.5090702947845805\n","iteration: 42, accuracy: 0.5065759637188209:  52% 42/80 [14:05<10:50, 17.12s/it]iteration: 42, accuracy: 0.5065759637188209\n","iteration: 43, accuracy: 0.5029478458049886:  54% 43/80 [14:20<10:13, 16.59s/it]iteration: 43, accuracy: 0.5029478458049886\n","iteration: 44, accuracy: 0.5006802721088436:  55% 44/80 [14:38<09:42, 16.19s/it]iteration: 44, accuracy: 0.5006802721088436\n","iteration: 45, accuracy: 0.501360544217687:  56% 45/80 [14:53<09:46, 16.77s/it] iteration: 45, accuracy: 0.501360544217687\n","iteration: 46, accuracy: 0.499546485260771:  57% 46/80 [15:10<09:13, 16.27s/it]iteration: 46, accuracy: 0.499546485260771\n","iteration: 47, accuracy: 0.5022675736961452:  59% 47/80 [15:27<09:07, 16.59s/it]iteration: 47, accuracy: 0.5022675736961452\n","iteration: 48, accuracy: 0.5034013605442177:  60% 48/80 [15:44<08:52, 16.65s/it]iteration: 48, accuracy: 0.5034013605442177\n","iteration: 49, accuracy: 0.49795918367346936:  61% 49/80 [15:58<08:26, 16.35s/it]iteration: 49, accuracy: 0.49795918367346936\n","iteration: 50, accuracy: 0.5011337868480725:  62% 50/80 [16:15<07:56, 15.88s/it] iteration: 50, accuracy: 0.5011337868480725\n","iteration: 51, accuracy: 0.4931972789115646:  64% 51/80 [16:29<07:44, 16.02s/it]iteration: 51, accuracy: 0.4931972789115646\n","iteration: 52, accuracy: 0.4931972789115646:  65% 52/80 [16:44<07:13, 15.50s/it]iteration: 52, accuracy: 0.4931972789115646\n","iteration: 53, accuracy: 0.49047619047619045:  66% 53/80 [16:59<06:54, 15.35s/it]iteration: 53, accuracy: 0.49047619047619045\n","iteration: 54, accuracy: 0.4850340136054422:  68% 54/80 [17:14<06:42, 15.49s/it] iteration: 54, accuracy: 0.4850340136054422\n","iteration: 55, accuracy: 0.48027210884353744:  69% 55/80 [17:30<06:25, 15.42s/it]iteration: 55, accuracy: 0.48027210884353744\n","iteration: 56, accuracy: 0.48072562358276644:  70% 56/80 [17:47<06:13, 15.58s/it]iteration: 56, accuracy: 0.48072562358276644\n","iteration: 57, accuracy: 0.473015873015873:  71% 57/80 [18:01<06:02, 15.76s/it]  iteration: 57, accuracy: 0.473015873015873\n","iteration: 58, accuracy: 0.47619047619047616:  72% 58/80 [18:14<05:30, 15.03s/it]iteration: 58, accuracy: 0.47619047619047616\n","iteration: 59, accuracy: 0.47278911564625853:  74% 59/80 [18:28<05:07, 14.62s/it]iteration: 59, accuracy: 0.47278911564625853\n","iteration: 60, accuracy: 0.47210884353741495:  75% 60/80 [18:41<04:45, 14.27s/it]iteration: 60, accuracy: 0.47210884353741495\n","iteration: 61, accuracy: 0.46757369614512473:  76% 61/80 [18:56<04:31, 14.29s/it]iteration: 61, accuracy: 0.46757369614512473\n","iteration: 62, accuracy: 0.47165532879818595:  78% 62/80 [19:10<04:20, 14.49s/it]iteration: 62, accuracy: 0.47165532879818595\n","iteration: 63, accuracy: 0.46122448979591835:  79% 63/80 [19:25<04:04, 14.40s/it]iteration: 63, accuracy: 0.46122448979591835\n","iteration: 64, accuracy: 0.4609977324263039:  80% 64/80 [19:40<03:47, 14.21s/it] iteration: 64, accuracy: 0.4609977324263039\n","iteration: 65, accuracy: 0.4598639455782313:  81% 65/80 [19:54<03:37, 14.48s/it]iteration: 65, accuracy: 0.4598639455782313\n","iteration: 66, accuracy: 0.4594104308390023:  82% 66/80 [20:07<03:20, 14.30s/it]iteration: 66, accuracy: 0.4594104308390023\n","iteration: 67, accuracy: 0.45691609977324266:  84% 67/80 [20:23<03:05, 14.23s/it]iteration: 67, accuracy: 0.45691609977324266\n","iteration: 68, accuracy: 0.4605442176870748:  85% 68/80 [20:37<02:54, 14.54s/it] iteration: 68, accuracy: 0.4605442176870748\n","iteration: 69, accuracy: 0.45396825396825397:  86% 69/80 [20:52<02:40, 14.58s/it]iteration: 69, accuracy: 0.45396825396825397\n","iteration: 70, accuracy: 0.45555555555555555:  88% 70/80 [21:07<02:24, 14.49s/it]iteration: 70, accuracy: 0.45555555555555555\n","iteration: 71, accuracy: 0.45396825396825397:  89% 71/80 [21:22<02:10, 14.55s/it]iteration: 71, accuracy: 0.45396825396825397\n","iteration: 72, accuracy: 0.45328798185941044:  90% 72/80 [21:35<01:58, 14.82s/it]iteration: 72, accuracy: 0.45328798185941044\n","iteration: 73, accuracy: 0.4523809523809524:  91% 73/80 [21:48<01:40, 14.34s/it] iteration: 73, accuracy: 0.4523809523809524\n","iteration: 74, accuracy: 0.4512471655328798:  92% 74/80 [22:03<01:23, 13.86s/it]iteration: 74, accuracy: 0.4512471655328798\n","iteration: 75, accuracy: 0.4435374149659864:  94% 75/80 [22:17<01:10, 14.01s/it]iteration: 75, accuracy: 0.4435374149659864\n","iteration: 76, accuracy: 0.4417233560090703:  95% 76/80 [22:31<00:56, 14.01s/it]iteration: 76, accuracy: 0.4417233560090703\n","iteration: 77, accuracy: 0.43718820861678004:  96% 77/80 [22:44<00:41, 13.97s/it]iteration: 77, accuracy: 0.43718820861678004\n","iteration: 78, accuracy: 0.43673469387755104:  98% 78/80 [22:58<00:28, 14.12s/it]iteration: 78, accuracy: 0.43673469387755104\n","iteration: 79, accuracy: 0.43605442176870746:  99% 79/80 [23:12<00:14, 14.10s/it]iteration: 79, accuracy: 0.43605442176870746\n","iteration: 79, accuracy: 0.43605442176870746: 100% 80/80 [23:13<00:00, 17.42s/it]\n","Saving computed projection matrix to: /content/drive/MyDrive/bias-bench-main/results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-race_s-0.pt\n"]}]},{"cell_type":"code","source":["! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-genderracereligion3000_80_s-0.pt --bias_type gender"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YkBxtmGE80wt","executionInfo":{"status":"ok","timestamp":1692570602929,"user_tz":-60,"elapsed":25011,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"fcc3b346-78cb-403a-e54b-ec2f3f19c3d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-genderracereligion3000_80_s-0.pt\n"," - load_path: None\n"," - bias_type: gender\n","Evaluating gender examples.\n","100% 262/262 [00:16<00:00, 15.99it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 45.8\n","Stereotype score: 60.38\n","Anti-stereotype score: 23.3\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 45.8\n"]}]},{"cell_type":"code","source":["! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-genderracereligion3000_80_s-0.pt --bias_type race"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vNAh_Mu6804K","executionInfo":{"status":"ok","timestamp":1692570636828,"user_tz":-60,"elapsed":33913,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"e9d7ea39-6c29-43ff-d8fb-a7a0cc32aa0a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-genderracereligion3000_80_s-0.pt\n"," - load_path: None\n"," - bias_type: race\n","Evaluating race-color examples.\n"," 70% 361/516 [00:17<00:06, 23.43it/s]Skipping example 363.\n","100% 515/516 [00:24<00:00, 20.79it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 45.44\n","Stereotype score: 43.95\n","Anti-stereotype score: 62.79\n","Num. neutral: 0.19\n","====================================================================================================\n","\n","Metric: 45.44\n"]}]},{"cell_type":"code","source":["! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-genderracereligion3000_80_s-0.pt --bias_type religion"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QoQ3WaAA81ND","executionInfo":{"status":"ok","timestamp":1692570649506,"user_tz":-60,"elapsed":12693,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"ac328cea-d87a-4598-ebc5-401bc84a99df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-genderracereligion3000_80_s-0.pt\n"," - load_path: None\n"," - bias_type: religion\n","Evaluating religion examples.\n","100% 105/105 [00:04<00:00, 23.97it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 57.14\n","Stereotype score: 57.58\n","Anti-stereotype score: 50.0\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 57.14\n"]}]},{"cell_type":"markdown","source":["### 5.2.6 optimal number of training epochs 100\n","gender race religion 3000 100"],"metadata":{"id":"zhVjGZN582IV"}},{"cell_type":"code","source":["! python experiments/inlp_projection_matrix.py --model AlbertModel --model_name_or_path albert-base-v2 --bias_type race --n_classifiers 100"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6w6-SsTD84n1","executionInfo":{"status":"ok","timestamp":1692572774975,"user_tz":-60,"elapsed":134918,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"75415e3a-82f5-4f36-916e-b0c1ca656f79"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Computing projection matrix:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: AlbertModel\n"," - model_name_or_path: albert-base-v2\n"," - bias_type: race\n"," - n_classifiers: 100\n"," - seed: 0\n","Loading INLP data:   2% 27104/1372632 [00:03<02:36, 8618.23it/s]INLP dataset collected:\n"," - Num. male sentences: 3000\n"," - Num. female sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   2% 27135/1372632 [00:03<02:43, 8254.41it/s]\n","Loading INLP data:   6% 86353/1372632 [00:05<01:23, 15317.25it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   6% 86562/1372632 [00:05<01:25, 15002.77it/s]\n","Loading INLP data:   8% 108861/1372632 [00:09<01:22, 15387.30it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   8% 110011/1372632 [00:09<01:54, 11035.39it/s]\n","Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.decoder.bias']\n","- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Encoding male sentences: 100% 3000/3000 [00:41<00:00, 72.14it/s]\n","Encoding female sentences: 100% 3000/3000 [00:42<00:00, 70.31it/s]\n","Encoding neutral sentences: 100% 3000/3000 [00:43<00:00, 68.29it/s]\n","Dataset split sizes:\n","Train size: 4410; Dev size: 1890; Test size: 2700\n","Encoding bias sentences: 100% 3000/3000 [00:43<00:00, 69.33it/s]\n","Encoding neutral sentences: 100% 3000/3000 [00:40<00:00, 74.33it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Encoding bias sentences: 100% 3000/3000 [00:41<00:00, 72.11it/s]\n","Encoding neutral sentences: 100% 3000/3000 [00:41<00:00, 72.80it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Dataset split sizes:\n","Train size: 10290; Dev size: 4410; Test size: 6300\n","iteration: 0, accuracy: 0.6804988662131519:   0% 0/100 [00:30<?, ?it/s]iteration: 0, accuracy: 0.6804988662131519\n","iteration: 1, accuracy: 0.6727891156462585:   1% 1/100 [00:56<52:06, 31.58s/it]iteration: 1, accuracy: 0.6727891156462585\n","iteration: 2, accuracy: 0.6723356009070295:   2% 2/100 [01:21<46:53, 28.71s/it]iteration: 2, accuracy: 0.6723356009070295\n","iteration: 3, accuracy: 0.672562358276644:   3% 3/100 [01:46<43:01, 26.61s/it] iteration: 3, accuracy: 0.672562358276644\n","iteration: 4, accuracy: 0.6689342403628118:   4% 4/100 [02:10<41:45, 26.10s/it]iteration: 4, accuracy: 0.6689342403628118\n","iteration: 5, accuracy: 0.6569160997732426:   5% 5/100 [02:34<40:17, 25.45s/it]iteration: 5, accuracy: 0.6569160997732426\n","iteration: 6, accuracy: 0.645578231292517:   6% 6/100 [03:04<38:56, 24.86s/it] iteration: 6, accuracy: 0.645578231292517\n","iteration: 7, accuracy: 0.6419501133786848:   7% 7/100 [03:30<40:47, 26.31s/it]iteration: 7, accuracy: 0.6419501133786848\n","iteration: 8, accuracy: 0.626077097505669:   8% 8/100 [03:55<40:34, 26.46s/it] iteration: 8, accuracy: 0.626077097505669\n","iteration: 9, accuracy: 0.6199546485260771:   9% 9/100 [04:18<39:13, 25.86s/it]iteration: 9, accuracy: 0.6199546485260771\n","iteration: 10, accuracy: 0.6111111111111112:  10% 10/100 [04:43<37:42, 25.14s/it]iteration: 10, accuracy: 0.6111111111111112\n","iteration: 11, accuracy: 0.6058956916099774:  11% 11/100 [05:05<37:00, 24.95s/it]iteration: 11, accuracy: 0.6058956916099774\n","iteration: 12, accuracy: 0.590702947845805:  12% 12/100 [05:30<35:33, 24.24s/it] iteration: 12, accuracy: 0.590702947845805\n","iteration: 13, accuracy: 0.5888888888888889:  13% 13/100 [05:51<35:07, 24.22s/it]iteration: 13, accuracy: 0.5888888888888889\n","iteration: 14, accuracy: 0.5841269841269842:  14% 14/100 [06:15<33:36, 23.45s/it]iteration: 14, accuracy: 0.5841269841269842\n","iteration: 15, accuracy: 0.5700680272108843:  15% 15/100 [06:38<33:32, 23.67s/it]iteration: 15, accuracy: 0.5700680272108843\n","iteration: 16, accuracy: 0.5664399092970521:  16% 16/100 [07:00<32:23, 23.14s/it]iteration: 16, accuracy: 0.5664399092970521\n","iteration: 17, accuracy: 0.5598639455782313:  17% 17/100 [07:22<32:04, 23.19s/it]iteration: 17, accuracy: 0.5598639455782313\n","iteration: 18, accuracy: 0.5487528344671202:  18% 18/100 [07:44<31:00, 22.69s/it]iteration: 18, accuracy: 0.5487528344671202\n","iteration: 19, accuracy: 0.5462585034013605:  19% 19/100 [08:07<30:05, 22.29s/it]iteration: 19, accuracy: 0.5462585034013605\n","iteration: 20, accuracy: 0.5530612244897959:  20% 20/100 [08:28<30:10, 22.64s/it]iteration: 20, accuracy: 0.5530612244897959\n","iteration: 21, accuracy: 0.5521541950113379:  21% 21/100 [08:49<29:16, 22.24s/it]iteration: 21, accuracy: 0.5521541950113379\n","iteration: 22, accuracy: 0.5437641723356009:  22% 22/100 [09:08<28:03, 21.58s/it]iteration: 22, accuracy: 0.5437641723356009\n","iteration: 23, accuracy: 0.5426303854875284:  23% 23/100 [09:29<27:03, 21.08s/it]iteration: 23, accuracy: 0.5426303854875284\n","iteration: 24, accuracy: 0.5399092970521542:  24% 24/100 [09:50<26:49, 21.17s/it]iteration: 24, accuracy: 0.5399092970521542\n","iteration: 25, accuracy: 0.5471655328798186:  25% 25/100 [10:10<25:55, 20.74s/it]iteration: 25, accuracy: 0.5471655328798186\n","iteration: 26, accuracy: 0.5446712018140589:  26% 26/100 [10:29<25:23, 20.59s/it]iteration: 26, accuracy: 0.5446712018140589\n","iteration: 27, accuracy: 0.536734693877551:  27% 27/100 [10:48<24:41, 20.29s/it] iteration: 27, accuracy: 0.536734693877551\n","iteration: 28, accuracy: 0.5312925170068027:  28% 28/100 [11:11<23:59, 20.00s/it]iteration: 28, accuracy: 0.5312925170068027\n","iteration: 29, accuracy: 0.5328798185941043:  29% 29/100 [11:30<24:29, 20.69s/it]iteration: 29, accuracy: 0.5328798185941043\n","iteration: 30, accuracy: 0.5351473922902494:  30% 30/100 [11:49<23:24, 20.07s/it]iteration: 30, accuracy: 0.5351473922902494\n","iteration: 31, accuracy: 0.5356009070294785:  31% 31/100 [12:08<22:56, 19.96s/it]iteration: 31, accuracy: 0.5356009070294785\n","iteration: 32, accuracy: 0.5242630385487528:  32% 32/100 [12:27<22:21, 19.73s/it]iteration: 32, accuracy: 0.5242630385487528\n","iteration: 33, accuracy: 0.5272108843537415:  33% 33/100 [12:45<21:48, 19.53s/it]iteration: 33, accuracy: 0.5272108843537415\n","iteration: 34, accuracy: 0.5208616780045352:  34% 34/100 [13:05<21:07, 19.20s/it]iteration: 34, accuracy: 0.5208616780045352\n","iteration: 35, accuracy: 0.5204081632653061:  35% 35/100 [13:24<20:49, 19.23s/it]iteration: 35, accuracy: 0.5204081632653061\n","iteration: 36, accuracy: 0.5219954648526077:  36% 36/100 [13:41<20:19, 19.05s/it]iteration: 36, accuracy: 0.5219954648526077\n","iteration: 37, accuracy: 0.5204081632653061:  37% 37/100 [14:00<19:28, 18.55s/it]iteration: 37, accuracy: 0.5204081632653061\n","iteration: 38, accuracy: 0.5160997732426303:  38% 38/100 [14:19<19:15, 18.63s/it]iteration: 38, accuracy: 0.5160997732426303\n","iteration: 39, accuracy: 0.5167800453514739:  39% 39/100 [14:35<18:54, 18.59s/it]iteration: 39, accuracy: 0.5167800453514739\n","iteration: 40, accuracy: 0.5138321995464853:  40% 40/100 [14:54<18:15, 18.25s/it]iteration: 40, accuracy: 0.5138321995464853\n","iteration: 41, accuracy: 0.5090702947845805:  41% 41/100 [15:13<18:10, 18.48s/it]iteration: 41, accuracy: 0.5090702947845805\n","iteration: 42, accuracy: 0.5065759637188209:  42% 42/100 [15:29<17:43, 18.34s/it]iteration: 42, accuracy: 0.5065759637188209\n","iteration: 43, accuracy: 0.5029478458049886:  43% 43/100 [15:45<16:54, 17.80s/it]iteration: 43, accuracy: 0.5029478458049886\n","iteration: 44, accuracy: 0.5006802721088436:  44% 44/100 [16:04<16:11, 17.35s/it]iteration: 44, accuracy: 0.5006802721088436\n","iteration: 45, accuracy: 0.501360544217687:  45% 45/100 [16:19<16:05, 17.56s/it] iteration: 45, accuracy: 0.501360544217687\n","iteration: 46, accuracy: 0.499546485260771:  46% 46/100 [16:36<15:15, 16.95s/it]iteration: 46, accuracy: 0.499546485260771\n","iteration: 47, accuracy: 0.5022675736961452:  47% 47/100 [16:53<14:52, 16.84s/it]iteration: 47, accuracy: 0.5022675736961452\n","iteration: 48, accuracy: 0.5034013605442177:  48% 48/100 [17:09<14:34, 16.81s/it]iteration: 48, accuracy: 0.5034013605442177\n","iteration: 49, accuracy: 0.49795918367346936:  49% 49/100 [17:24<14:05, 16.58s/it]iteration: 49, accuracy: 0.49795918367346936\n","iteration: 50, accuracy: 0.5011337868480725:  50% 50/100 [17:41<13:32, 16.25s/it] iteration: 50, accuracy: 0.5011337868480725\n","iteration: 51, accuracy: 0.4931972789115646:  51% 51/100 [17:56<13:25, 16.43s/it]iteration: 51, accuracy: 0.4931972789115646\n","iteration: 52, accuracy: 0.4931972789115646:  52% 52/100 [18:12<12:49, 16.03s/it]iteration: 52, accuracy: 0.4931972789115646\n","iteration: 53, accuracy: 0.49047619047619045:  53% 53/100 [18:28<12:31, 15.99s/it]iteration: 53, accuracy: 0.49047619047619045\n","iteration: 54, accuracy: 0.4850340136054422:  54% 54/100 [18:44<12:18, 16.06s/it] iteration: 54, accuracy: 0.4850340136054422\n","iteration: 55, accuracy: 0.48027210884353744:  55% 55/100 [19:00<11:59, 15.98s/it]iteration: 55, accuracy: 0.48027210884353744\n","iteration: 56, accuracy: 0.48072562358276644:  56% 56/100 [19:17<11:55, 16.27s/it]iteration: 56, accuracy: 0.48072562358276644\n","iteration: 57, accuracy: 0.473015873015873:  57% 57/100 [19:31<11:47, 16.46s/it]  iteration: 57, accuracy: 0.473015873015873\n","iteration: 58, accuracy: 0.47619047619047616:  58% 58/100 [19:46<11:00, 15.74s/it]iteration: 58, accuracy: 0.47619047619047616\n","iteration: 59, accuracy: 0.47278911564625853:  59% 59/100 [20:00<10:26, 15.27s/it]iteration: 59, accuracy: 0.47278911564625853\n","iteration: 60, accuracy: 0.47210884353741495:  60% 60/100 [20:14<09:50, 14.76s/it]iteration: 60, accuracy: 0.47210884353741495\n","iteration: 61, accuracy: 0.46757369614512473:  61% 61/100 [20:31<09:30, 14.64s/it]iteration: 61, accuracy: 0.46757369614512473\n","iteration: 62, accuracy: 0.47165532879818595:  62% 62/100 [20:45<09:39, 15.26s/it]iteration: 62, accuracy: 0.47165532879818595\n","iteration: 63, accuracy: 0.46122448979591835:  63% 63/100 [20:59<09:15, 15.00s/it]iteration: 63, accuracy: 0.46122448979591835\n","iteration: 64, accuracy: 0.4609977324263039:  64% 64/100 [21:15<08:51, 14.75s/it] iteration: 64, accuracy: 0.4609977324263039\n","iteration: 65, accuracy: 0.4598639455782313:  65% 65/100 [21:29<08:45, 15.03s/it]iteration: 65, accuracy: 0.4598639455782313\n","iteration: 66, accuracy: 0.4594104308390023:  66% 66/100 [21:43<08:22, 14.78s/it]iteration: 66, accuracy: 0.4594104308390023\n","iteration: 67, accuracy: 0.45691609977324266:  67% 67/100 [21:58<07:58, 14.51s/it]iteration: 67, accuracy: 0.45691609977324266\n","iteration: 68, accuracy: 0.4605442176870748:  68% 68/100 [22:14<07:50, 14.71s/it] iteration: 68, accuracy: 0.4605442176870748\n","iteration: 69, accuracy: 0.45396825396825397:  69% 69/100 [22:29<07:45, 15.03s/it]iteration: 69, accuracy: 0.45396825396825397\n","iteration: 70, accuracy: 0.45555555555555555:  70% 70/100 [22:44<07:32, 15.10s/it]iteration: 70, accuracy: 0.45555555555555555\n","iteration: 71, accuracy: 0.45396825396825397:  71% 71/100 [22:59<07:17, 15.09s/it]iteration: 71, accuracy: 0.45396825396825397\n","iteration: 72, accuracy: 0.45328798185941044:  72% 72/100 [23:13<06:55, 14.85s/it]iteration: 72, accuracy: 0.45328798185941044\n","iteration: 73, accuracy: 0.4523809523809524:  73% 73/100 [23:27<06:33, 14.59s/it] iteration: 73, accuracy: 0.4523809523809524\n","iteration: 74, accuracy: 0.4512471655328798:  74% 74/100 [23:42<06:17, 14.53s/it]iteration: 74, accuracy: 0.4512471655328798\n","iteration: 75, accuracy: 0.4435374149659864:  75% 75/100 [23:57<06:11, 14.87s/it]iteration: 75, accuracy: 0.4435374149659864\n","iteration: 76, accuracy: 0.4417233560090703:  76% 76/100 [24:11<05:55, 14.83s/it]iteration: 76, accuracy: 0.4417233560090703\n","iteration: 77, accuracy: 0.43718820861678004:  77% 77/100 [24:27<05:39, 14.77s/it]iteration: 77, accuracy: 0.43718820861678004\n","iteration: 78, accuracy: 0.43673469387755104:  78% 78/100 [24:42<05:27, 14.91s/it]iteration: 78, accuracy: 0.43673469387755104\n","iteration: 79, accuracy: 0.43605442176870746:  79% 79/100 [24:55<05:09, 14.73s/it]iteration: 79, accuracy: 0.43605442176870746\n","iteration: 80, accuracy: 0.43537414965986393:  80% 80/100 [25:10<04:48, 14.44s/it]iteration: 80, accuracy: 0.43537414965986393\n","iteration: 81, accuracy: 0.42630385487528344:  81% 81/100 [25:24<04:32, 14.33s/it]iteration: 81, accuracy: 0.42630385487528344\n","iteration: 82, accuracy: 0.4253968253968254:  82% 82/100 [25:37<04:15, 14.22s/it] iteration: 82, accuracy: 0.4253968253968254\n","iteration: 83, accuracy: 0.41950113378684806:  83% 83/100 [25:50<04:00, 14.13s/it]iteration: 83, accuracy: 0.41950113378684806\n","iteration: 84, accuracy: 0.41337868480725626:  84% 84/100 [26:03<03:42, 13.88s/it]iteration: 84, accuracy: 0.41337868480725626\n","iteration: 85, accuracy: 0.41156462585034015:  85% 85/100 [26:16<03:21, 13.43s/it]iteration: 85, accuracy: 0.41156462585034015\n","iteration: 86, accuracy: 0.41723356009070295:  86% 86/100 [26:28<03:03, 13.11s/it]iteration: 86, accuracy: 0.41723356009070295\n","iteration: 87, accuracy: 0.4140589569160998:  87% 87/100 [26:41<02:49, 13.02s/it] iteration: 87, accuracy: 0.4140589569160998\n","iteration: 88, accuracy: 0.408390022675737:  88% 88/100 [26:53<02:36, 13.05s/it] iteration: 88, accuracy: 0.408390022675737\n","iteration: 89, accuracy: 0.408843537414966:  89% 89/100 [27:07<02:21, 12.84s/it]iteration: 89, accuracy: 0.408843537414966\n","iteration: 90, accuracy: 0.40793650793650793:  90% 90/100 [27:19<02:10, 13.03s/it]iteration: 90, accuracy: 0.40793650793650793\n","iteration: 91, accuracy: 0.40294784580498866:  91% 91/100 [27:32<01:54, 12.72s/it]iteration: 91, accuracy: 0.40294784580498866\n","iteration: 92, accuracy: 0.3941043083900227:  92% 92/100 [27:46<01:43, 12.89s/it] iteration: 92, accuracy: 0.3941043083900227\n","iteration: 93, accuracy: 0.3961451247165533:  93% 93/100 [27:58<01:31, 13.01s/it]iteration: 93, accuracy: 0.3961451247165533\n","iteration: 94, accuracy: 0.38616780045351473:  94% 94/100 [28:10<01:18, 13.02s/it]iteration: 94, accuracy: 0.38616780045351473\n","iteration: 95, accuracy: 0.38412698412698415:  95% 95/100 [28:23<01:03, 12.61s/it]iteration: 95, accuracy: 0.38412698412698415\n","iteration: 96, accuracy: 0.38140589569161:  96% 96/100 [28:37<00:51, 12.81s/it]   iteration: 96, accuracy: 0.38140589569161\n","iteration: 97, accuracy: 0.3739229024943311:  97% 97/100 [28:50<00:38, 12.93s/it]iteration: 97, accuracy: 0.3739229024943311\n","iteration: 98, accuracy: 0.36167800453514737:  98% 98/100 [29:02<00:25, 12.89s/it]iteration: 98, accuracy: 0.36167800453514737\n","iteration: 99, accuracy: 0.3578231292517007:  99% 99/100 [29:14<00:12, 12.95s/it] iteration: 99, accuracy: 0.3578231292517007\n","iteration: 99, accuracy: 0.3578231292517007: 100% 100/100 [29:15<00:00, 17.56s/it]\n","Saving computed projection matrix to: /content/drive/MyDrive/bias-bench-main/results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-race_s-0.pt\n"]}]},{"cell_type":"code","source":["! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-genderracereligion3000_100_s-0.pt --bias_type gender"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w9PQRR5B84sZ","executionInfo":{"status":"ok","timestamp":1692572976813,"user_tz":-60,"elapsed":26401,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"9d0ceba2-ed3d-4aa2-826c-257bff2e32ac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-genderracereligion3000_100_s-0.pt\n"," - load_path: None\n"," - bias_type: gender\n","Evaluating gender examples.\n","100% 262/262 [00:16<00:00, 15.73it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 43.89\n","Stereotype score: 55.35\n","Anti-stereotype score: 26.21\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 43.89\n"]}]},{"cell_type":"code","source":["! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-genderracereligion3000_100_s-0.pt --bias_type race"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HVolenuS840b","executionInfo":{"status":"ok","timestamp":1692573011031,"user_tz":-60,"elapsed":34232,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"c0d750ff-990f-4be9-e61a-c81789d7802c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-genderracereligion3000_100_s-0.pt\n"," - load_path: None\n"," - bias_type: race\n","Evaluating race-color examples.\n"," 70% 361/516 [00:17<00:05, 28.43it/s]Skipping example 363.\n","100% 515/516 [00:26<00:00, 19.32it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 54.95\n","Stereotype score: 54.87\n","Anti-stereotype score: 55.81\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 54.95\n"]}]},{"cell_type":"code","source":["! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-genderracereligion3000_100_s-0.pt --bias_type religion"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KyEEkg4-85TE","executionInfo":{"status":"ok","timestamp":1692573023963,"user_tz":-60,"elapsed":12945,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"ae339ff5-c0e5-4f9a-c6a4-dd53f6622edd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-genderracereligion3000_100_s-0.pt\n"," - load_path: None\n"," - bias_type: religion\n","Evaluating religion examples.\n","100% 105/105 [00:05<00:00, 19.82it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 48.57\n","Stereotype score: 49.49\n","Anti-stereotype score: 33.33\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 48.57\n"]}]},{"cell_type":"markdown","source":["### accuary plot"],"metadata":{"id":"gJM42WYIU5vG"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","baseline_data = [\n","    \"iteration: 0, accuracy: 0.8811904761904762\",\n","    \"iteration: 1, accuracy: 0.8797619047619047\",\n","    \"iteration: 2, accuracy: 0.8811904761904762\",\n","    \"iteration: 3, accuracy: 0.8745238095238095\",\n","    \"iteration: 4, accuracy: 0.8726190476190476\",\n","    \"iteration: 5, accuracy: 0.8638095238095238\",\n","    \"iteration: 6, accuracy: 0.8588095238095238\",\n","    \"iteration: 7, accuracy: 0.849047619047619\",\n","    \"iteration: 8, accuracy: 0.8380952380952381\",\n","    \"iteration: 9, accuracy: 0.8302380952380952\",\n","    \"iteration: 10, accuracy: 0.8233333333333334\",\n","    \"iteration: 11, accuracy: 0.8061904761904762\",\n","    \"iteration: 12, accuracy: 0.8030952380952381\",\n","    \"iteration: 13, accuracy: 0.8011904761904762\",\n","    \"iteration: 14, accuracy: 0.7919047619047619\",\n","    \"iteration: 15, accuracy: 0.7835714285714286\",\n","    \"iteration: 16, accuracy: 0.7738095238095238\",\n","    \"iteration: 17, accuracy: 0.7695238095238095\",\n","    \"iteration: 18, accuracy: 0.7602380952380953\",\n","    \"iteration: 19, accuracy: 0.7564285714285715\",\n","    \"iteration: 20, accuracy: 0.746904761904762\",\n","    \"iteration: 21, accuracy: 0.7411904761904762\",\n","    \"iteration: 22, accuracy: 0.7338095238095238\",\n","    \"iteration: 23, accuracy: 0.7257142857142858\",\n","    \"iteration: 24, accuracy: 0.7147619047619047\",\n","    \"iteration: 25, accuracy: 0.7092857142857143\",\n","    \"iteration: 26, accuracy: 0.7\",\n","    \"iteration: 27, accuracy: 0.6964285714285714\",\n","    \"iteration: 28, accuracy: 0.6933333333333334\",\n","    \"iteration: 29, accuracy: 0.6792857142857143\",\n","    \"iteration: 30, accuracy: 0.6823809523809524\",\n","    \"iteration: 31, accuracy: 0.6721428571428572\",\n","    \"iteration: 32, accuracy: 0.6697619047619048\",\n","    \"iteration: 33, accuracy: 0.6707142857142857\",\n","    \"iteration: 34, accuracy: 0.6654761904761904\",\n","    \"iteration: 35, accuracy: 0.6661904761904762\",\n","    \"iteration: 36, accuracy: 0.6504761904761904\",\n","    \"iteration: 37, accuracy: 0.6514285714285715\",\n","    \"iteration: 38, accuracy: 0.6473809523809524\",\n","    \"iteration: 39, accuracy: 0.645\",\n","    \"iteration: 40, accuracy: 0.6490476190476191\",\n","    \"iteration: 41, accuracy: 0.6488095238095238\",\n","    \"iteration: 42, accuracy: 0.6471428571428571\",\n","    \"iteration: 43, accuracy: 0.6445238095238095\",\n","    \"iteration: 44, accuracy: 0.6371428571428571\",\n","    \"iteration: 45, accuracy: 0.6378571428571429\",\n","    \"iteration: 46, accuracy: 0.6385714285714286\",\n","    \"iteration: 47, accuracy: 0.6371428571428571\",\n","    \"iteration: 48, accuracy: 0.6416666666666667\",\n","    \"iteration: 49, accuracy: 0.64\",\n","    \"iteration: 50, accuracy: 0.6273809523809524\",\n","    \"iteration: 51, accuracy: 0.63\",\n","    \"iteration: 52, accuracy: 0.6245238095238095\",\n","    \"iteration: 53, accuracy: 0.6195238095238095\",\n","    \"iteration: 54, accuracy: 0.6226190476190476\",\n","    \"iteration: 55, accuracy: 0.6173809523809524\",\n","    \"iteration: 56, accuracy: 0.616904761904762\",\n","    \"iteration: 57, accuracy: 0.6121428571428571\",\n","    \"iteration: 58, accuracy: 0.6095238095238096\",\n","    \"iteration: 59, accuracy: 0.6085714285714285\",\n","    \"iteration: 60, accuracy: 0.6107142857142858\",\n","    \"iteration: 61, accuracy: 0.6009523809523809\",\n","    \"iteration: 62, accuracy: 0.606904761904762\",\n","    \"iteration: 63, accuracy: 0.6026190476190476\",\n","    \"iteration: 64, accuracy: 0.5997619047619047\",\n","    \"iteration: 65, accuracy: 0.6011904761904762\",\n","    \"iteration: 66, accuracy: 0.5966666666666667\",\n","    \"iteration: 67, accuracy: 0.5935714285714285\",\n","    \"iteration: 68, accuracy: 0.5933333333333334\",\n","    \"iteration: 69, accuracy: 0.5957142857142858\",\n","    \"iteration: 70, accuracy: 0.5933333333333334\",\n","    \"iteration: 71, accuracy: 0.5852380952380952\",\n","    \"iteration: 72, accuracy: 0.585952380952381\",\n","    \"iteration: 73, accuracy: 0.5835714285714285\",\n","    \"iteration: 74, accuracy: 0.5807142857142857\",\n","    \"iteration: 75, accuracy: 0.5852380952380952\",\n","    \"iteration: 76, accuracy: 0.5828571428571429\",\n","    \"iteration: 77, accuracy: 0.5828571428571429\",\n","    \"iteration: 78, accuracy: 0.574047619047619\",\n","    \"iteration: 79, accuracy: 0.575\"\n","]\n","\n","log_data = [\n","\t\"iteration: 0, accuracy: 0.6804988662131519\",\n","\"\titeration: 1, accuracy: 0.6727891156462585\t\",\n","\"\titeration: 2, accuracy: 0.6723356009070295\t\",\n","\"\titeration: 3, accuracy: 0.672562358276644\t\",\n","\"\titeration: 4, accuracy: 0.6689342403628118\t\",\n","\"\titeration: 5, accuracy: 0.6569160997732426\t\",\n","\"\titeration: 6, accuracy: 0.645578231292517\t\",\n","\"\titeration: 7, accuracy: 0.641950113378684\t\",\n","\"\titeration: 8, accuracy: 0.626077097505669\t\",\n","\"\titeration: 9, accuracy: 0.619954648526077\t\",\n","\"\titeration: 10, accuracy: 0.61111111111111\t\",\n","\"\titeration: 11, accuracy: 0.60589569160997\t\",\n","\"\titeration: 12, accuracy: 0.59070294784580\t\",\n","\"\titeration: 13, accuracy: 0.58888888888888\t\",\n","\"\titeration: 14, accuracy: 0.58412698412698\t\",\n","\"\titeration: 15, accuracy: 0.57006802721088\t\",\n","\"\titeration: 16, accuracy: 0.56643990929705\t\",\n","\"\titeration: 17, accuracy: 0.55986394557823\t\",\n","\"\titeration: 18, accuracy: 0.54875283446712\t\",\n","\"\titeration: 19, accuracy: 0.54625850340136\t\",\n","\"\titeration: 20, accuracy: 0.55306122448979\t\",\n","\"\titeration: 21, accuracy: 0.55215419501133\t\",\n","\"\titeration: 22, accuracy: 0.54376417233560\t\",\n","\"\titeration: 23, accuracy: 0.54263038548752\t\",\n","\"\titeration: 24, accuracy: 0.53990929705215\t\",\n","\"\titeration: 25, accuracy: 0.54716553287981\t\",\n","\"\titeration: 26, accuracy: 0.54467120181405\t\",\n","\"\titeration: 27, accuracy: 0.53673469387755\t\",\n","\"\titeration: 28, accuracy: 0.53129251700680\t\",\n","\"\titeration: 29, accuracy: 0.53287981859410\t\",\n","\"\titeration: 30, accuracy: 0.53514739229024\t\",\n","\"\titeration: 31, accuracy: 0.53560090702947\t\",\n","\"\titeration: 32, accuracy: 0.52426303854875\t\",\n","\"\titeration: 33, accuracy: 0.52721088435374\t\",\n","\"\titeration: 34, accuracy: 0.52086167800453\t\",\n","\"\titeration: 35, accuracy: 0.52040816326530\t\",\n","\"\titeration: 36, accuracy: 0.52199546485260\t\",\n","\"\titeration: 37, accuracy: 0.52040816326530\t\",\n","\"\titeration: 38, accuracy: 0.51609977324263\t\",\n","\"\titeration: 39, accuracy: 0.51678004535147\t\",\n","\"\titeration: 40, accuracy: 0.51383219954648\t\",\n","\"\titeration: 41, accuracy: 0.50907029478458\t\",\n","\"\titeration: 42, accuracy: 0.50657596371882\t\",\n","\"\titeration: 43, accuracy: 0.50294784580498\t\",\n","\"\titeration: 44, accuracy: 0.50068027210884\t\",\n","\"\titeration: 45, accuracy: 0.50136054421768\t\",\n","\"\titeration: 46, accuracy: 0.49954648526077\t\",\n","\"\titeration: 47, accuracy: 0.50226757369614\t\",\n","\"\titeration: 48, accuracy: 0.50340136054421\t\",\n","\"\titeration: 49, accuracy: 0.49795918367346\t\",\n","\"\titeration: 50, accuracy: 0.50113378684807\t\",\n","\"\titeration: 51, accuracy: 0.49319727891156\t\",\n","\"\titeration: 52, accuracy: 0.49319727891156\t\",\n","\"\titeration: 53, accuracy: 0.49047619047619\t\",\n","\"\titeration: 54, accuracy: 0.48503401360544\t\",\n","\"\titeration: 55, accuracy: 0.48027210884353\t\",\n","\"\titeration: 56, accuracy: 0.48072562358276\t\",\n","\"\titeration: 57, accuracy: 0.47301587301587\t\",\n","\"\titeration: 58, accuracy: 0.47619047619047\t\",\n","\"\titeration: 59, accuracy: 0.47278911564625\t\",\n","\"\titeration: 60, accuracy: 0.47210884353741\t\",\n","\"\titeration: 61, accuracy: 0.46757369614512\t\",\n","\"\titeration: 62, accuracy: 0.47165532879818\t\",\n","\"\titeration: 63, accuracy: 0.46122448979591\t\",\n","\"\titeration: 64, accuracy: 0.46099773242630\t\",\n","\"\titeration: 65, accuracy: 0.45986394557823\t\",\n","\"\titeration: 66, accuracy: 0.45941043083900\t\",\n","\"\titeration: 67, accuracy: 0.45691609977324\t\",\n","\"\titeration: 68, accuracy: 0.46054421768707\t\",\n","\"\titeration: 69, accuracy: 0.45396825396825\t\",\n","\"\titeration: 70, accuracy: 0.45555555555555\t\",\n","\"\titeration: 71, accuracy: 0.45396825396825\t\",\n","\"\titeration: 72, accuracy: 0.45328798185941\t\",\n","\"\titeration: 73, accuracy: 0.45238095238095\t\",\n","\"\titeration: 74, accuracy: 0.45124716553287\t\",\n","\"\titeration: 75, accuracy: 0.44353741496598\t\",\n","\"\titeration: 76, accuracy: 0.44172335600907\t\",\n","\"\titeration: 77, accuracy: 0.43718820861678\t\",\n","\"\titeration: 78, accuracy: 0.43673469387755\t\",\n","\"\titeration: 79, accuracy: 0.43605442176870\t\",\n","\"\titeration: 80, accuracy: 0.43537414965986\t\",\n","\"\titeration: 81, accuracy: 0.42630385487528\t\",\n","\"\titeration: 82, accuracy: 0.42539682539682\t\",\n","\"\titeration: 83, accuracy: 0.41950113378684\t\",\n","\"\titeration: 84, accuracy: 0.41337868480725\t\",\n","\"\titeration: 85, accuracy: 0.41156462585034\t\",\n","\"\titeration: 86, accuracy: 0.41723356009070\t\",\n","\"\titeration: 87, accuracy: 0.41405895691609\t\",\n","\"\titeration: 88, accuracy: 0.40839002267573\t\",\n","\"\titeration: 89, accuracy: 0.40884353741496\t\",\n","\"\titeration: 90, accuracy: 0.40793650793650\t\",\n","\"\titeration: 91, accuracy: 0.40294784580498\t\",\n","\"\titeration: 92, accuracy: 0.39410430839002\t\",\n","\"\titeration: 93, accuracy: 0.39614512471655\t\",\n","\"\titeration: 94, accuracy: 0.38616780045351\t\",\n","\"\titeration: 95, accuracy: 0.38412698412698\t\",\n","\"\titeration: 96, accuracy: 0.38140589569161\t\",\n","\"\titeration: 97, accuracy: 0.37392290249433\t\",\n","\"\titeration: 98, accuracy: 0.36167800453514\t\",\n","\t\"iteration: 99, accuracy: 0.35782312925170\"\n","]\n","\n","plt.rcParams.update({'font.size': 14})\n","# Extract iteration and accuracy values from the log data\n","iterations = []\n","accuracies = []\n","\n","for entry in log_data:\n","    parts = entry.split(\", \")\n","    iteration = int(parts[0].split(\": \")[1]) +1\n","    accuracy = float(parts[1].split(\": \")[1])\n","    iterations.append(iteration)\n","    accuracies.append(accuracy)\n","\n","# Extract iteration and accuracy values from the baseline data\n","baseline_iterations = []\n","baseline_accuracies = []\n","\n","for entry in baseline_data:\n","    parts = entry.split(\", \")\n","    iteration = int(parts[0].split(\": \")[1]) +1\n","    accuracy = float(parts[1].split(\": \")[1])\n","    baseline_iterations.append(iteration)\n","    baseline_accuracies.append(accuracy)\n","\n","# Plot the accuracy data without markers\n","plt.figure(figsize=(12, 6))\n","plt.plot(iterations, accuracies, linestyle='-', color='b', label='Multi-type debiasing')\n","plt.plot(baseline_iterations, baseline_accuracies, linestyle='--', color='g', label='Baseline: Race debiasing')\n","# plt.title('Accuracy of Removing Singular Race Bias using INLP')\n","plt.xlabel('Iteration')\n","plt.ylabel('Accuracy')\n","plt.grid(True)\n","plt.legend()\n","\n","# Label specific data points and connect them to the axes with dashed lines\n","label_points = [(1, accuracies[iterations.index(1)]),\n","          (30, accuracies[iterations.index(30)]),\n","                (80, accuracies[iterations.index(80)]),\n","                (100, accuracies[iterations.index(100)])\n","                ]\n","\n","baseline_label_points = [(1, baseline_accuracies[baseline_iterations.index(1)]),\n","              (80, baseline_accuracies[baseline_iterations.index(80)]),\n","                        ]\n","\n","for point in label_points:\n","    plt.annotate(f'Accuracy: {point[1]:.4f}', point, textcoords=\"offset points\", xytext=(0, 10), ha='center')\n","    plt.plot([point[0], point[0]], [0, point[1]], 'r--', lw=1)  # Dashed lines to x-axis\n","    plt.plot([0, point[0]], [point[1], point[1]], 'b--', lw=1)  # Dashed lines to y-axis\n","\n","# Label baseline data points\n","for point in baseline_label_points:\n","    plt.annotate(f'Accuracy: {point[1]:.4f}', point, textcoords=\"offset points\", xytext=(0, 10), ha='center')\n","    plt.plot([point[0], point[0]], [0, point[1]], 'r--', lw=1)  # Dashed lines to x-axis\n","    plt.plot([0, point[0]], [point[1], point[1]], 'g--', lw=1)  # Dashed lines to y-axis\n","\n","plt.show()\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":340},"id":"tYI5HswcU5Yu","executionInfo":{"status":"ok","timestamp":1692918350611,"user_tz":-60,"elapsed":848,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"cbba4ca1-5d6a-49e5-e2ff-1ac72023e682"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1200x600 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"YTlNOm76Xr7U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5.3 Verifying the effect of multi-type debiasing\n","\n","1.mixed up the three bias type data\n","\n","2.change data wikipedia-8 wikipedia2.5:80%-90%\n","\n","3.change data wikipedia-9 wikipedia2.5:90%-100%\n","\n","\n"],"metadata":{"id":"3CrE_v2tUHwW"}},{"cell_type":"markdown","source":["### 5.3.1 3000*3 80"],"metadata":{"id":"6ykJpNo5BAN1"}},{"cell_type":"markdown","metadata":{"id":"LmK3D2kfrEYN"},"source":["mix up data: modify context_nullspace_projection.py and inlp_projection_matrix.py 3000 80"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":823745,"status":"ok","timestamp":1690910567343,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"rPUDEI4BXL6w","outputId":"bc0185e2-638d-45d3-8543-f9c1429ad2af"},"outputs":[{"name":"stdout","output_type":"stream","text":["Computing projection matrix:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: AlbertModel\n"," - model_name_or_path: albert-base-v2\n"," - bias_type: race\n"," - n_classifiers: 80\n"," - seed: 0\n","Loading INLP data:   2% 25997/1372632 [00:02<01:55, 11620.85it/s]INLP dataset collected:\n"," - Num. male sentences: 3000\n"," - Num. female sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   2% 27135/1372632 [00:02<02:09, 10416.46it/s]\n","Loading INLP data:   6% 86090/1372632 [00:09<03:10, 6747.84it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   6% 86562/1372632 [00:09<02:20, 9170.89it/s]\n","Loading INLP data:   8% 109746/1372632 [00:09<02:41, 7820.94it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   8% 110011/1372632 [00:09<01:47, 11785.82it/s]\n","Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']\n","- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Encoding male sentences: 100% 3000/3000 [09:07<00:00,  5.48it/s]\n","Encoding female sentences: 100% 3000/3000 [08:32<00:00,  5.85it/s]\n","Encoding neutral sentences: 100% 3000/3000 [07:32<00:00,  6.64it/s]\n","Dataset split sizes:\n","Train size: 4410; Dev size: 1890; Test size: 2700\n","Encoding bias sentences: 100% 3000/3000 [09:55<00:00,  5.04it/s]\n","Encoding neutral sentences: 100% 3000/3000 [07:45<00:00,  6.44it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Encoding bias sentences: 100% 3000/3000 [10:25<00:00,  4.79it/s]\n","Encoding neutral sentences: 100% 3000/3000 [08:17<00:00,  6.03it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Dataset split sizes:\n","Train size: 10290; Dev size: 4410; Test size: 6300\n","iteration: 0, accuracy: 0.6804988662131519:   0% 0/80 [00:51<?, ?it/s]iteration: 0, accuracy: 0.6804988662131519\n","iteration: 1, accuracy: 0.672562358276644:   1% 1/80 [01:37<1:08:58, 52.38s/it] iteration: 1, accuracy: 0.672562358276644\n","iteration: 2, accuracy: 0.6723356009070295:   2% 2/80 [02:19<1:03:06, 48.54s/it]iteration: 2, accuracy: 0.6723356009070295\n","iteration: 3, accuracy: 0.672562358276644:   4% 3/80 [03:05<58:30, 45.59s/it] iteration: 3, accuracy: 0.672562358276644\n","iteration: 4, accuracy: 0.6689342403628118:   5% 4/80 [03:47<58:01, 45.81s/it]iteration: 4, accuracy: 0.6689342403628118\n","iteration: 5, accuracy: 0.6569160997732426:   6% 5/80 [04:21<55:50, 44.67s/it]iteration: 5, accuracy: 0.6569160997732426\n","iteration: 6, accuracy: 0.646485260770975:   8% 6/80 [05:00<50:53, 41.27s/it] iteration: 6, accuracy: 0.646485260770975\n","iteration: 7, accuracy: 0.6414965986394557:   9% 7/80 [05:35<49:06, 40.36s/it]iteration: 7, accuracy: 0.6414965986394557\n","iteration: 8, accuracy: 0.6256235827664399:  10% 8/80 [06:11<46:21, 38.64s/it]iteration: 8, accuracy: 0.6256235827664399\n","iteration: 9, accuracy: 0.6192743764172336:  11% 9/80 [06:46<44:36, 37.70s/it]iteration: 9, accuracy: 0.6192743764172336\n","iteration: 10, accuracy: 0.6115646258503401:  12% 10/80 [07:20<43:02, 36.90s/it]iteration: 10, accuracy: 0.6115646258503401\n","iteration: 11, accuracy: 0.6056689342403628:  14% 11/80 [07:48<41:07, 35.76s/it]iteration: 11, accuracy: 0.6056689342403628\n","iteration: 12, accuracy: 0.590702947845805:  15% 12/80 [08:23<38:21, 33.84s/it] iteration: 12, accuracy: 0.590702947845805\n","iteration: 13, accuracy: 0.5886621315192744:  16% 13/80 [09:00<38:14, 34.24s/it]iteration: 13, accuracy: 0.5886621315192744\n","iteration: 14, accuracy: 0.5827664399092971:  18% 14/80 [09:32<38:35, 35.08s/it]iteration: 14, accuracy: 0.5827664399092971\n","iteration: 15, accuracy: 0.5712018140589569:  19% 15/80 [10:00<36:44, 33.92s/it]iteration: 15, accuracy: 0.5712018140589569\n","iteration: 16, accuracy: 0.5668934240362812:  20% 16/80 [10:29<34:15, 32.12s/it]iteration: 16, accuracy: 0.5668934240362812\n","iteration: 17, accuracy: 0.5591836734693878:  21% 17/80 [11:00<32:45, 31.20s/it]iteration: 17, accuracy: 0.5591836734693878\n","iteration: 18, accuracy: 0.5476190476190477:  22% 18/80 [11:32<32:08, 31.10s/it]iteration: 18, accuracy: 0.5476190476190477\n","iteration: 19, accuracy: 0.5453514739229025:  24% 19/80 [12:07<32:09, 31.62s/it]iteration: 19, accuracy: 0.5453514739229025\n","iteration: 20, accuracy: 0.5530612244897959:  25% 20/80 [12:41<32:19, 32.33s/it]iteration: 20, accuracy: 0.5530612244897959\n","iteration: 21, accuracy: 0.5517006802721088:  26% 21/80 [13:12<32:10, 32.71s/it]iteration: 21, accuracy: 0.5517006802721088\n","iteration: 22, accuracy: 0.5435374149659864:  28% 22/80 [13:44<31:17, 32.37s/it]iteration: 22, accuracy: 0.5435374149659864\n","iteration: 23, accuracy: 0.5439909297052155:  29% 23/80 [14:14<30:38, 32.25s/it]iteration: 23, accuracy: 0.5439909297052155\n","iteration: 24, accuracy: 0.5394557823129251:  30% 24/80 [14:44<29:37, 31.73s/it]iteration: 24, accuracy: 0.5394557823129251\n","iteration: 25, accuracy: 0.546485260770975:  31% 25/80 [15:10<28:18, 30.87s/it] iteration: 25, accuracy: 0.546485260770975\n","iteration: 26, accuracy: 0.545578231292517:  32% 26/80 [15:40<26:36, 29.57s/it]iteration: 26, accuracy: 0.545578231292517\n","iteration: 27, accuracy: 0.536734693877551:  34% 27/80 [16:10<26:30, 30.01s/it]iteration: 27, accuracy: 0.536734693877551\n","iteration: 28, accuracy: 0.5331065759637188:  35% 28/80 [16:38<25:37, 29.56s/it]iteration: 28, accuracy: 0.5331065759637188\n","iteration: 29, accuracy: 0.5324263038548753:  36% 29/80 [17:09<24:55, 29.33s/it]iteration: 29, accuracy: 0.5324263038548753\n","iteration: 30, accuracy: 0.5342403628117914:  38% 30/80 [17:40<24:55, 29.90s/it]iteration: 30, accuracy: 0.5342403628117914\n","iteration: 31, accuracy: 0.5349206349206349:  39% 31/80 [18:10<24:38, 30.17s/it]iteration: 31, accuracy: 0.5349206349206349\n","iteration: 32, accuracy: 0.526984126984127:  40% 32/80 [18:39<23:58, 29.98s/it] iteration: 32, accuracy: 0.526984126984127\n","iteration: 33, accuracy: 0.527891156462585:  41% 33/80 [19:03<23:17, 29.74s/it]iteration: 33, accuracy: 0.527891156462585\n","iteration: 34, accuracy: 0.5215419501133787:  42% 34/80 [19:33<21:36, 28.18s/it]iteration: 34, accuracy: 0.5215419501133787\n","iteration: 35, accuracy: 0.5217687074829932:  44% 35/80 [20:02<21:17, 28.40s/it]iteration: 35, accuracy: 0.5217687074829932\n","iteration: 36, accuracy: 0.5219954648526077:  45% 36/80 [20:28<21:09, 28.84s/it]iteration: 36, accuracy: 0.5219954648526077\n","iteration: 37, accuracy: 0.5215419501133787:  46% 37/80 [20:55<19:57, 27.86s/it]iteration: 37, accuracy: 0.5215419501133787\n","iteration: 38, accuracy: 0.5165532879818594:  48% 38/80 [21:23<19:22, 27.69s/it]iteration: 38, accuracy: 0.5165532879818594\n","iteration: 39, accuracy: 0.5176870748299319:  49% 39/80 [21:50<19:05, 27.95s/it]iteration: 39, accuracy: 0.5176870748299319\n","iteration: 40, accuracy: 0.5138321995464853:  50% 40/80 [22:17<18:13, 27.35s/it]iteration: 40, accuracy: 0.5138321995464853\n","iteration: 41, accuracy: 0.5102040816326531:  51% 41/80 [22:45<17:56, 27.61s/it]iteration: 41, accuracy: 0.5102040816326531\n","iteration: 42, accuracy: 0.5070294784580499:  52% 42/80 [23:07<17:21, 27.41s/it]iteration: 42, accuracy: 0.5070294784580499\n","iteration: 43, accuracy: 0.5036281179138322:  54% 43/80 [23:35<16:02, 26.01s/it]iteration: 43, accuracy: 0.5036281179138322\n","iteration: 44, accuracy: 0.500907029478458:  55% 44/80 [24:02<15:48, 26.34s/it] iteration: 44, accuracy: 0.500907029478458\n","iteration: 45, accuracy: 0.5006802721088436:  56% 45/80 [24:31<15:41, 26.90s/it]iteration: 45, accuracy: 0.5006802721088436\n","iteration: 46, accuracy: 0.49727891156462584:  57% 46/80 [24:59<15:30, 27.36s/it]iteration: 46, accuracy: 0.49727891156462584\n","iteration: 47, accuracy: 0.5034013605442177:  59% 47/80 [25:23<15:02, 27.34s/it] iteration: 47, accuracy: 0.5034013605442177\n","iteration: 48, accuracy: 0.5029478458049886:  60% 48/80 [25:51<14:19, 26.84s/it]iteration: 48, accuracy: 0.5029478458049886\n","iteration: 49, accuracy: 0.4959183673469388:  61% 49/80 [26:15<13:50, 26.79s/it]iteration: 49, accuracy: 0.4959183673469388\n","iteration: 50, accuracy: 0.49705215419501136:  62% 50/80 [26:36<12:53, 25.78s/it]iteration: 50, accuracy: 0.49705215419501136\n","iteration: 51, accuracy: 0.4897959183673469:  64% 51/80 [26:59<11:51, 24.54s/it] iteration: 51, accuracy: 0.4897959183673469\n","iteration: 52, accuracy: 0.4941043083900227:  65% 52/80 [27:20<11:11, 23.97s/it]iteration: 52, accuracy: 0.4941043083900227\n","iteration: 53, accuracy: 0.49229024943310656:  66% 53/80 [27:42<10:27, 23.23s/it]iteration: 53, accuracy: 0.49229024943310656\n","iteration: 54, accuracy: 0.4827664399092971:  68% 54/80 [28:04<09:58, 23.01s/it] iteration: 54, accuracy: 0.4827664399092971\n","iteration: 55, accuracy: 0.4798185941043084:  69% 55/80 [28:26<09:23, 22.55s/it]iteration: 55, accuracy: 0.4798185941043084\n","iteration: 56, accuracy: 0.4782312925170068:  70% 56/80 [28:46<08:57, 22.38s/it]iteration: 56, accuracy: 0.4782312925170068\n","iteration: 57, accuracy: 0.473015873015873:  71% 57/80 [29:06<08:14, 21.52s/it] iteration: 57, accuracy: 0.473015873015873\n","iteration: 58, accuracy: 0.47528344671201816:  72% 58/80 [29:28<07:46, 21.19s/it]iteration: 58, accuracy: 0.47528344671201816\n","iteration: 59, accuracy: 0.473015873015873:  74% 59/80 [29:51<07:31, 21.50s/it]  iteration: 59, accuracy: 0.473015873015873\n","iteration: 60, accuracy: 0.4723356009070295:  75% 60/80 [30:12<07:18, 21.92s/it]iteration: 60, accuracy: 0.4723356009070295\n","iteration: 61, accuracy: 0.47006802721088436:  76% 61/80 [30:33<06:50, 21.62s/it]iteration: 61, accuracy: 0.47006802721088436\n","iteration: 62, accuracy: 0.4714285714285714:  78% 62/80 [30:54<06:25, 21.44s/it] iteration: 62, accuracy: 0.4714285714285714\n","iteration: 63, accuracy: 0.46235827664399093:  79% 63/80 [31:14<06:00, 21.19s/it]iteration: 63, accuracy: 0.46235827664399093\n","iteration: 64, accuracy: 0.4614512471655329:  80% 64/80 [31:32<05:32, 20.76s/it] iteration: 64, accuracy: 0.4614512471655329\n","iteration: 65, accuracy: 0.4634920634920635:  81% 65/80 [31:52<05:00, 20.04s/it]iteration: 65, accuracy: 0.4634920634920635\n","iteration: 66, accuracy: 0.4609977324263039:  82% 66/80 [32:10<04:38, 19.88s/it]iteration: 66, accuracy: 0.4609977324263039\n","iteration: 67, accuracy: 0.4564625850340136:  84% 67/80 [32:30<04:11, 19.34s/it]iteration: 67, accuracy: 0.4564625850340136\n","iteration: 68, accuracy: 0.4594104308390023:  85% 68/80 [32:51<03:58, 19.87s/it]iteration: 68, accuracy: 0.4594104308390023\n","iteration: 69, accuracy: 0.45374149659863944:  86% 69/80 [33:10<03:38, 19.90s/it]iteration: 69, accuracy: 0.45374149659863944\n","iteration: 70, accuracy: 0.45691609977324266:  88% 70/80 [33:29<03:18, 19.85s/it]iteration: 70, accuracy: 0.45691609977324266\n","iteration: 71, accuracy: 0.4546485260770975:  89% 71/80 [33:49<02:56, 19.60s/it] iteration: 71, accuracy: 0.4546485260770975\n","iteration: 72, accuracy: 0.45714285714285713:  90% 72/80 [34:06<02:36, 19.50s/it]iteration: 72, accuracy: 0.45714285714285713\n","iteration: 73, accuracy: 0.4512471655328798:  91% 73/80 [34:26<02:13, 19.11s/it] iteration: 73, accuracy: 0.4512471655328798\n","iteration: 74, accuracy: 0.44693877551020406:  92% 74/80 [34:46<01:55, 19.19s/it]iteration: 74, accuracy: 0.44693877551020406\n","iteration: 75, accuracy: 0.4392290249433107:  94% 75/80 [35:06<01:37, 19.49s/it] iteration: 75, accuracy: 0.4392290249433107\n","iteration: 76, accuracy: 0.43900226757369615:  95% 76/80 [35:25<01:17, 19.46s/it]iteration: 76, accuracy: 0.43900226757369615\n","iteration: 77, accuracy: 0.43945578231292515:  96% 77/80 [35:45<00:57, 19.25s/it]iteration: 77, accuracy: 0.43945578231292515\n","iteration: 78, accuracy: 0.43718820861678004:  98% 78/80 [36:04<00:38, 19.45s/it]iteration: 78, accuracy: 0.43718820861678004\n","iteration: 79, accuracy: 0.4342403628117914:  99% 79/80 [36:24<00:19, 19.59s/it] iteration: 79, accuracy: 0.4342403628117914\n","iteration: 79, accuracy: 0.4342403628117914: 100% 80/80 [36:26<00:00, 27.33s/it]\n","Saving computed projection matrix to: /content/drive/MyDrive/bias-bench-main/results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-race_s-0.pt\n"]}],"source":["\n","! python experiments/inlp_projection_matrix.py --model AlbertModel --model_name_or_path albert-base-v2 --bias_type race --n_classifiers 80"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":112118,"status":"ok","timestamp":1691314829974,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"3d7n27VtrDXC","outputId":"32ddb027-be23-4562-ebf5-0e39501827ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-mix3000_s-0.pt\n"," - load_path: None\n"," - bias_type: gender\n","Downloading: 100% 684/684 [00:00<00:00, 3.91MB/s]\n","Downloading: 100% 45.2M/45.2M [00:00<00:00, 53.9MB/s]\n","Downloading: 100% 742k/742k [00:00<00:00, 14.2MB/s]\n","Downloading: 100% 1.25M/1.25M [00:00<00:00, 20.7MB/s]\n","Evaluating gender examples.\n","100% 262/262 [01:36<00:00,  2.72it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 46.95\n","Stereotype score: 59.12\n","Anti-stereotype score: 28.16\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 46.95\n"]}],"source":["\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-mix3000_s-0.pt --bias_type gender"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":154150,"status":"ok","timestamp":1691314984120,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"egsWn9hErDaO","outputId":"e158f2b5-f71f-408a-d77d-582f5fad308a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-mix3000_s-0.pt\n"," - load_path: None\n"," - bias_type: race\n","Evaluating race-color examples.\n"," 70% 363/516 [01:43<00:30,  5.03it/s]Skipping example 363.\n","100% 515/516 [02:28<00:00,  3.46it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 44.27\n","Stereotype score: 42.8\n","Anti-stereotype score: 60.47\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 44.27\n"]}],"source":["\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-mix3000_s-0.pt --bias_type race"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30435,"status":"ok","timestamp":1691315014552,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"m4t83hfHXL-D","outputId":"1236414e-0baf-4fe2-d140-9f94b953d2a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-mix3000_s-0.pt\n"," - load_path: None\n"," - bias_type: religion\n","Evaluating religion examples.\n","100% 105/105 [00:25<00:00,  4.07it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 58.1\n","Stereotype score: 58.59\n","Anti-stereotype score: 50.0\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 58.1\n"]}],"source":["\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-mix3000_s-0.pt --bias_type religion"]},{"cell_type":"markdown","source":["### 5.3.2 3000*3 80 wikipedia8"],"metadata":{"id":"2xFNWLJ3UMpj"}},{"cell_type":"code","source":["\n","! python experiments/inlp_projection_matrix.py --model AlbertModel --model_name_or_path albert-base-v2 --bias_type race --n_classifiers 80"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fiyHQOtYUTOW","executionInfo":{"status":"ok","timestamp":1692576833116,"user_tz":-60,"elapsed":66224,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"c9875723-1d84-4a1b-ec6a-c3b94b8ce733"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Computing projection matrix:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: AlbertModel\n"," - model_name_or_path: albert-base-v2\n"," - bias_type: race\n"," - n_classifiers: 80\n"," - seed: 0\n","Loading INLP data:   9% 24770/274526 [00:01<00:17, 14010.73it/s]INLP dataset collected:\n"," - Num. male sentences: 3000\n"," - Num. female sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   9% 25346/274526 [00:01<00:18, 13568.96it/s]\n","Loading INLP data:  31% 86069/274526 [00:05<00:18, 9976.30it/s] INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:  32% 86650/274526 [00:05<00:12, 14580.06it/s]\n","Loading INLP data:  39% 105746/274526 [00:08<00:11, 14925.16it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:  39% 106692/274526 [00:09<00:14, 11826.17it/s]\n","Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.dense.weight']\n","- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Encoding male sentences: 100% 3000/3000 [00:43<00:00, 69.39it/s]\n","Encoding female sentences: 100% 3000/3000 [00:40<00:00, 73.93it/s]\n","Encoding neutral sentences: 100% 3000/3000 [00:42<00:00, 69.94it/s]\n","Dataset split sizes:\n","Train size: 4410; Dev size: 1890; Test size: 2700\n","Encoding bias sentences: 100% 3000/3000 [00:42<00:00, 71.41it/s]\n","Encoding neutral sentences: 100% 3000/3000 [00:40<00:00, 73.38it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Encoding bias sentences: 100% 3000/3000 [00:44<00:00, 67.62it/s]\n","Encoding neutral sentences: 100% 3000/3000 [00:40<00:00, 74.84it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Dataset split sizes:\n","Train size: 10290; Dev size: 4410; Test size: 6300\n","iteration: 0, accuracy: 0.6687074829931973:   0% 0/80 [00:32<?, ?it/s]iteration: 0, accuracy: 0.6687074829931973\n","iteration: 1, accuracy: 0.6764172335600908:   1% 1/80 [01:04<44:02, 33.45s/it]iteration: 1, accuracy: 0.6764172335600908\n","iteration: 2, accuracy: 0.6807256235827664:   2% 2/80 [01:30<42:01, 32.32s/it]iteration: 2, accuracy: 0.6807256235827664\n","iteration: 3, accuracy: 0.6791383219954649:   4% 3/80 [01:54<37:52, 29.51s/it]iteration: 3, accuracy: 0.6791383219954649\n","iteration: 4, accuracy: 0.6659863945578232:   5% 4/80 [02:20<34:56, 27.59s/it]iteration: 4, accuracy: 0.6659863945578232\n","iteration: 5, accuracy: 0.655328798185941:   6% 5/80 [02:45<33:34, 26.87s/it] iteration: 5, accuracy: 0.655328798185941\n","iteration: 6, accuracy: 0.6476190476190476:   8% 6/80 [03:09<32:29, 26.35s/it]iteration: 6, accuracy: 0.6476190476190476\n","iteration: 7, accuracy: 0.6326530612244898:   9% 7/80 [03:32<30:57, 25.45s/it]iteration: 7, accuracy: 0.6326530612244898\n","iteration: 8, accuracy: 0.6149659863945578:  10% 8/80 [03:55<29:42, 24.75s/it]iteration: 8, accuracy: 0.6149659863945578\n","iteration: 9, accuracy: 0.6006802721088436:  11% 9/80 [04:15<28:27, 24.05s/it]iteration: 9, accuracy: 0.6006802721088436\n","iteration: 10, accuracy: 0.5922902494331066:  12% 10/80 [04:38<26:44, 22.92s/it]iteration: 10, accuracy: 0.5922902494331066\n","iteration: 11, accuracy: 0.5956916099773243:  14% 11/80 [05:02<26:32, 23.08s/it]iteration: 11, accuracy: 0.5956916099773243\n","iteration: 12, accuracy: 0.582312925170068:  15% 12/80 [05:22<26:13, 23.13s/it] iteration: 12, accuracy: 0.582312925170068\n","iteration: 13, accuracy: 0.5748299319727891:  16% 13/80 [05:43<24:52, 22.27s/it]iteration: 13, accuracy: 0.5748299319727891\n","iteration: 14, accuracy: 0.5705215419501134:  18% 14/80 [06:04<24:23, 22.18s/it]iteration: 14, accuracy: 0.5705215419501134\n","iteration: 15, accuracy: 0.5591836734693878:  19% 15/80 [06:25<23:26, 21.65s/it]iteration: 15, accuracy: 0.5591836734693878\n","iteration: 16, accuracy: 0.5621315192743764:  20% 16/80 [06:46<22:50, 21.41s/it]iteration: 16, accuracy: 0.5621315192743764\n","iteration: 17, accuracy: 0.555328798185941:  21% 17/80 [07:09<22:31, 21.45s/it] iteration: 17, accuracy: 0.555328798185941\n","iteration: 18, accuracy: 0.5507936507936508:  22% 18/80 [07:27<22:14, 21.52s/it]iteration: 18, accuracy: 0.5507936507936508\n","iteration: 19, accuracy: 0.5537414965986395:  24% 19/80 [07:46<20:54, 20.57s/it]iteration: 19, accuracy: 0.5537414965986395\n","iteration: 20, accuracy: 0.5496598639455782:  25% 20/80 [08:05<20:04, 20.08s/it]iteration: 20, accuracy: 0.5496598639455782\n","iteration: 21, accuracy: 0.5430839002267573:  26% 21/80 [08:25<19:23, 19.72s/it]iteration: 21, accuracy: 0.5430839002267573\n","iteration: 22, accuracy: 0.5424036281179139:  28% 22/80 [08:46<19:23, 20.06s/it]iteration: 22, accuracy: 0.5424036281179139\n","iteration: 23, accuracy: 0.5378684807256235:  29% 23/80 [09:05<19:07, 20.14s/it]iteration: 23, accuracy: 0.5378684807256235\n","iteration: 24, accuracy: 0.5399092970521542:  30% 24/80 [09:24<18:22, 19.68s/it]iteration: 24, accuracy: 0.5399092970521542\n","iteration: 25, accuracy: 0.5399092970521542:  31% 25/80 [09:44<18:04, 19.72s/it]iteration: 25, accuracy: 0.5399092970521542\n","iteration: 26, accuracy: 0.5349206349206349:  32% 26/80 [10:04<17:57, 19.96s/it]iteration: 26, accuracy: 0.5349206349206349\n","iteration: 27, accuracy: 0.5303854875283447:  34% 27/80 [10:23<17:30, 19.82s/it]iteration: 27, accuracy: 0.5303854875283447\n","iteration: 28, accuracy: 0.5303854875283447:  35% 28/80 [10:43<16:59, 19.60s/it]iteration: 28, accuracy: 0.5303854875283447\n","iteration: 29, accuracy: 0.5242630385487528:  36% 29/80 [11:02<16:41, 19.64s/it]iteration: 29, accuracy: 0.5242630385487528\n","iteration: 30, accuracy: 0.5201814058956916:  38% 30/80 [11:21<16:13, 19.48s/it]iteration: 30, accuracy: 0.5201814058956916\n","iteration: 31, accuracy: 0.5242630385487528:  39% 31/80 [11:41<15:53, 19.45s/it]iteration: 31, accuracy: 0.5242630385487528\n","iteration: 32, accuracy: 0.5258503401360545:  40% 32/80 [12:00<15:34, 19.47s/it]iteration: 32, accuracy: 0.5258503401360545\n","iteration: 33, accuracy: 0.5238095238095238:  41% 33/80 [12:16<15:01, 19.18s/it]iteration: 33, accuracy: 0.5238095238095238\n","iteration: 34, accuracy: 0.5210884353741496:  42% 34/80 [12:35<14:07, 18.42s/it]iteration: 34, accuracy: 0.5210884353741496\n","iteration: 35, accuracy: 0.52562358276644:  44% 35/80 [12:53<13:47, 18.39s/it]  iteration: 35, accuracy: 0.52562358276644\n","iteration: 36, accuracy: 0.5210884353741496:  45% 36/80 [13:10<13:31, 18.44s/it]iteration: 36, accuracy: 0.5210884353741496\n","iteration: 37, accuracy: 0.5147392290249433:  46% 37/80 [13:27<12:51, 17.95s/it]iteration: 37, accuracy: 0.5147392290249433\n","iteration: 38, accuracy: 0.5149659863945578:  48% 38/80 [13:46<12:33, 17.95s/it]iteration: 38, accuracy: 0.5149659863945578\n","iteration: 39, accuracy: 0.51859410430839:  49% 39/80 [14:03<12:25, 18.18s/it]  iteration: 39, accuracy: 0.51859410430839\n","iteration: 40, accuracy: 0.5217687074829932:  50% 40/80 [14:19<11:48, 17.72s/it]iteration: 40, accuracy: 0.5217687074829932\n","iteration: 41, accuracy: 0.509750566893424:  51% 41/80 [14:36<11:11, 17.22s/it] iteration: 41, accuracy: 0.509750566893424\n","iteration: 42, accuracy: 0.5106575963718821:  52% 42/80 [14:53<10:51, 17.14s/it]iteration: 42, accuracy: 0.5106575963718821\n","iteration: 43, accuracy: 0.508390022675737:  54% 43/80 [15:09<10:32, 17.08s/it] iteration: 43, accuracy: 0.508390022675737\n","iteration: 44, accuracy: 0.5170068027210885:  55% 44/80 [15:25<10:05, 16.81s/it]iteration: 44, accuracy: 0.5170068027210885\n","iteration: 45, accuracy: 0.5140589569160998:  56% 45/80 [15:41<09:38, 16.54s/it]iteration: 45, accuracy: 0.5140589569160998\n","iteration: 46, accuracy: 0.5104308390022676:  57% 46/80 [15:58<09:17, 16.41s/it]iteration: 46, accuracy: 0.5104308390022676\n","iteration: 47, accuracy: 0.5043083900226757:  59% 47/80 [16:15<09:10, 16.69s/it]iteration: 47, accuracy: 0.5043083900226757\n","iteration: 48, accuracy: 0.4997732426303855:  60% 48/80 [16:31<08:50, 16.59s/it]iteration: 48, accuracy: 0.4997732426303855\n","iteration: 49, accuracy: 0.4984126984126984:  61% 49/80 [16:46<08:32, 16.54s/it]iteration: 49, accuracy: 0.4984126984126984\n","iteration: 50, accuracy: 0.49183673469387756:  62% 50/80 [17:02<08:07, 16.24s/it]iteration: 50, accuracy: 0.49183673469387756\n","iteration: 51, accuracy: 0.4909297052154195:  64% 51/80 [17:16<07:45, 16.04s/it] iteration: 51, accuracy: 0.4909297052154195\n","iteration: 52, accuracy: 0.4827664399092971:  65% 52/80 [17:32<07:15, 15.54s/it]iteration: 52, accuracy: 0.4827664399092971\n","iteration: 53, accuracy: 0.4804988662131519:  66% 53/80 [17:48<07:03, 15.68s/it]iteration: 53, accuracy: 0.4804988662131519\n","iteration: 54, accuracy: 0.4852607709750567:  68% 54/80 [18:06<06:47, 15.69s/it]iteration: 54, accuracy: 0.4852607709750567\n","iteration: 55, accuracy: 0.48639455782312924:  69% 55/80 [18:22<06:49, 16.38s/it]iteration: 55, accuracy: 0.48639455782312924\n","iteration: 56, accuracy: 0.48480725623582765:  70% 56/80 [18:37<06:31, 16.30s/it]iteration: 56, accuracy: 0.48480725623582765\n","iteration: 57, accuracy: 0.4816326530612245:  71% 57/80 [18:54<06:08, 16.00s/it] iteration: 57, accuracy: 0.4816326530612245\n","iteration: 58, accuracy: 0.47732426303854875:  72% 58/80 [19:09<05:54, 16.11s/it]iteration: 58, accuracy: 0.47732426303854875\n","iteration: 59, accuracy: 0.4800453514739229:  74% 59/80 [19:25<05:30, 15.75s/it] iteration: 59, accuracy: 0.4800453514739229\n","iteration: 60, accuracy: 0.4804988662131519:  75% 60/80 [19:40<05:16, 15.83s/it]iteration: 60, accuracy: 0.4804988662131519\n","iteration: 61, accuracy: 0.48072562358276644:  76% 61/80 [19:55<04:54, 15.50s/it]iteration: 61, accuracy: 0.48072562358276644\n","iteration: 62, accuracy: 0.4743764172335601:  78% 62/80 [20:11<04:36, 15.37s/it] iteration: 62, accuracy: 0.4743764172335601\n","iteration: 63, accuracy: 0.46757369614512473:  79% 63/80 [20:25<04:23, 15.50s/it]iteration: 63, accuracy: 0.46757369614512473\n","iteration: 64, accuracy: 0.4639455782312925:  80% 64/80 [20:40<04:02, 15.16s/it] iteration: 64, accuracy: 0.4639455782312925\n","iteration: 65, accuracy: 0.45850340136054424:  81% 65/80 [20:54<03:44, 14.98s/it]iteration: 65, accuracy: 0.45850340136054424\n","iteration: 66, accuracy: 0.4598639455782313:  82% 66/80 [21:09<03:27, 14.83s/it] iteration: 66, accuracy: 0.4598639455782313\n","iteration: 67, accuracy: 0.4598639455782313:  84% 67/80 [21:25<03:12, 14.78s/it]iteration: 67, accuracy: 0.4598639455782313\n","iteration: 68, accuracy: 0.4621315192743764:  85% 68/80 [21:40<03:00, 15.01s/it]iteration: 68, accuracy: 0.4621315192743764\n","iteration: 69, accuracy: 0.4526077097505669:  86% 69/80 [21:55<02:45, 15.00s/it]iteration: 69, accuracy: 0.4526077097505669\n","iteration: 70, accuracy: 0.45714285714285713:  88% 70/80 [22:11<02:33, 15.35s/it]iteration: 70, accuracy: 0.45714285714285713\n","iteration: 71, accuracy: 0.44988662131519275:  89% 71/80 [22:27<02:18, 15.43s/it]iteration: 71, accuracy: 0.44988662131519275\n","iteration: 72, accuracy: 0.4471655328798186:  90% 72/80 [22:42<02:05, 15.69s/it] iteration: 72, accuracy: 0.4471655328798186\n","iteration: 73, accuracy: 0.4414965986394558:  91% 73/80 [22:59<01:48, 15.57s/it]iteration: 73, accuracy: 0.4414965986394558\n","iteration: 74, accuracy: 0.4369614512471655:  92% 74/80 [23:13<01:34, 15.77s/it]iteration: 74, accuracy: 0.4369614512471655\n","iteration: 75, accuracy: 0.43174603174603177:  94% 75/80 [23:28<01:16, 15.38s/it]iteration: 75, accuracy: 0.43174603174603177\n","iteration: 76, accuracy: 0.4344671201814059:  95% 76/80 [23:43<01:00, 15.04s/it] iteration: 76, accuracy: 0.4344671201814059\n","iteration: 77, accuracy: 0.427437641723356:  96% 77/80 [23:57<00:44, 14.97s/it] iteration: 77, accuracy: 0.427437641723356\n","iteration: 78, accuracy: 0.4226757369614512:  98% 78/80 [24:10<00:29, 14.69s/it]iteration: 78, accuracy: 0.4226757369614512\n","iteration: 79, accuracy: 0.41609977324263037:  99% 79/80 [24:24<00:14, 14.39s/it]iteration: 79, accuracy: 0.41609977324263037\n","iteration: 79, accuracy: 0.41609977324263037: 100% 80/80 [24:25<00:00, 18.32s/it]\n","Saving computed projection matrix to: /content/drive/MyDrive/bias-bench-main/results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-race_s-0.pt\n"]}]},{"cell_type":"code","source":["! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-mix13000_s-0.pt --bias_type gender"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cqBQq30_UTYp","executionInfo":{"status":"ok","timestamp":1692577226525,"user_tz":-60,"elapsed":27358,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"271d2834-738f-4b52-857b-bf6ff6fc1f27"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-mix13000_s-0.pt\n"," - load_path: None\n"," - bias_type: gender\n","Evaluating gender examples.\n","100% 262/262 [00:18<00:00, 14.27it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 48.09\n","Stereotype score: 54.09\n","Anti-stereotype score: 38.83\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 48.09\n"]}]},{"cell_type":"code","source":["! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-mix13000_s-0.pt --bias_type race"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AYwoeZpLUTdt","executionInfo":{"status":"ok","timestamp":1692577261186,"user_tz":-60,"elapsed":34664,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"4d79f744-f206-4848-a5e0-846f6399591f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-mix13000_s-0.pt\n"," - load_path: None\n"," - bias_type: race\n","Evaluating race-color examples.\n"," 70% 362/516 [00:18<00:08, 17.85it/s]Skipping example 363.\n","100% 515/516 [00:25<00:00, 20.16it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 41.36\n","Stereotype score: 39.62\n","Anti-stereotype score: 60.47\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 41.36\n"]}]},{"cell_type":"code","source":["! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-mix13000_s-0.pt --bias_type religion"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_FhZ9mkBUTkH","executionInfo":{"status":"ok","timestamp":1692577274624,"user_tz":-60,"elapsed":13456,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"e99f4d83-1be3-47d4-9d92-a5e00729debc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-mix13000_s-0.pt\n"," - load_path: None\n"," - bias_type: religion\n","Evaluating religion examples.\n","100% 105/105 [00:04<00:00, 21.39it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 67.62\n","Stereotype score: 69.7\n","Anti-stereotype score: 33.33\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 67.62\n"]}]},{"cell_type":"markdown","source":["### 5.3.3 3000*3 80 wikipedia9"],"metadata":{"id":"4HvFC8TPe3m4"}},{"cell_type":"code","source":["\n","! python experiments/inlp_projection_matrix.py --model AlbertModel --model_name_or_path albert-base-v2 --bias_type race --n_classifiers 80"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RvFdKuVpe37G","executionInfo":{"status":"ok","timestamp":1692627880544,"user_tz":-60,"elapsed":76543,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"ebb6f90c-b132-43b0-ce4e-7c28887c6c37"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Computing projection matrix:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: AlbertModel\n"," - model_name_or_path: albert-base-v2\n"," - bias_type: race\n"," - n_classifiers: 80\n"," - seed: 0\n","Loading INLP data:  17% 23821/137263 [00:02<00:08, 13680.12it/s]INLP dataset collected:\n"," - Num. male sentences: 3000\n"," - Num. female sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:  17% 23887/137263 [00:02<00:11, 9760.36it/s] \n","Loading INLP data:  60% 82382/137263 [00:05<00:03, 15382.63it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:  61% 83086/137263 [00:05<00:03, 15457.54it/s]\n","Loading INLP data:  82% 112141/137263 [00:09<00:02, 9176.26it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:  82% 112566/137263 [00:09<00:02, 11293.83it/s]\n","Downloading: 100% 684/684 [00:00<00:00, 4.39MB/s]\n","Downloading: 100% 45.2M/45.2M [00:03<00:00, 13.6MB/s]\n","Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.bias']\n","- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Downloading: 100% 742k/742k [00:00<00:00, 8.72MB/s]\n","Downloading: 100% 1.25M/1.25M [00:00<00:00, 22.2MB/s]\n","Encoding male sentences: 100% 3000/3000 [00:43<00:00, 68.97it/s]\n","Encoding female sentences: 100% 3000/3000 [00:38<00:00, 78.35it/s]\n","Encoding neutral sentences: 100% 3000/3000 [00:38<00:00, 77.05it/s]\n","Dataset split sizes:\n","Train size: 4410; Dev size: 1890; Test size: 2700\n","Encoding bias sentences: 100% 3000/3000 [00:40<00:00, 74.74it/s]\n","Encoding neutral sentences: 100% 3000/3000 [00:38<00:00, 78.91it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Encoding bias sentences: 100% 3000/3000 [00:39<00:00, 76.73it/s]\n","Encoding neutral sentences: 100% 3000/3000 [00:39<00:00, 75.35it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Dataset split sizes:\n","Train size: 10290; Dev size: 4410; Test size: 6300\n","iteration: 0, accuracy: 0.6750566893424036:   0% 0/80 [00:27<?, ?it/s]iteration: 0, accuracy: 0.6750566893424036\n","iteration: 1, accuracy: 0.6766439909297052:   1% 1/80 [00:54<38:39, 29.36s/it]iteration: 1, accuracy: 0.6766439909297052\n","iteration: 2, accuracy: 0.6768707482993197:   2% 2/80 [01:20<35:52, 27.59s/it]iteration: 2, accuracy: 0.6768707482993197\n","iteration: 3, accuracy: 0.6668934240362812:   4% 3/80 [01:43<34:27, 26.85s/it]iteration: 3, accuracy: 0.6668934240362812\n","iteration: 4, accuracy: 0.6605442176870748:   5% 4/80 [02:05<32:03, 25.32s/it]iteration: 4, accuracy: 0.6605442176870748\n","iteration: 5, accuracy: 0.6478458049886622:   6% 5/80 [02:25<30:01, 24.02s/it]iteration: 5, accuracy: 0.6478458049886622\n","iteration: 6, accuracy: 0.6376417233560091:   8% 6/80 [02:44<27:50, 22.58s/it]iteration: 6, accuracy: 0.6376417233560091\n","iteration: 7, accuracy: 0.6267573696145124:   9% 7/80 [03:05<26:16, 21.59s/it]iteration: 7, accuracy: 0.6267573696145124\n","iteration: 8, accuracy: 0.618140589569161:  10% 8/80 [03:27<25:27, 21.21s/it] iteration: 8, accuracy: 0.618140589569161\n","iteration: 9, accuracy: 0.610204081632653:  11% 9/80 [03:46<25:19, 21.40s/it]iteration: 9, accuracy: 0.610204081632653\n","iteration: 10, accuracy: 0.6022675736961451:  12% 10/80 [04:06<24:26, 20.95s/it]iteration: 10, accuracy: 0.6022675736961451\n","iteration: 11, accuracy: 0.5854875283446712:  14% 11/80 [04:26<23:45, 20.66s/it]iteration: 11, accuracy: 0.5854875283446712\n","iteration: 12, accuracy: 0.5773242630385488:  15% 12/80 [04:47<23:10, 20.45s/it]iteration: 12, accuracy: 0.5773242630385488\n","iteration: 13, accuracy: 0.5712018140589569:  16% 13/80 [05:09<22:47, 20.41s/it]iteration: 13, accuracy: 0.5712018140589569\n","iteration: 14, accuracy: 0.556235827664399:  18% 14/80 [05:29<23:16, 21.16s/it] iteration: 14, accuracy: 0.556235827664399\n","iteration: 15, accuracy: 0.5560090702947846:  19% 15/80 [05:49<22:24, 20.69s/it]iteration: 15, accuracy: 0.5560090702947846\n","iteration: 16, accuracy: 0.5544217687074829:  20% 16/80 [06:07<21:37, 20.27s/it]iteration: 16, accuracy: 0.5544217687074829\n","iteration: 17, accuracy: 0.5528344671201814:  21% 17/80 [06:25<20:45, 19.77s/it]iteration: 17, accuracy: 0.5528344671201814\n","iteration: 18, accuracy: 0.553968253968254:  22% 18/80 [06:45<20:01, 19.38s/it] iteration: 18, accuracy: 0.553968253968254\n","iteration: 19, accuracy: 0.5489795918367347:  24% 19/80 [07:03<19:52, 19.55s/it]iteration: 19, accuracy: 0.5489795918367347\n","iteration: 20, accuracy: 0.5435374149659864:  25% 20/80 [07:22<19:10, 19.18s/it]iteration: 20, accuracy: 0.5435374149659864\n","iteration: 21, accuracy: 0.5437641723356009:  26% 21/80 [07:39<18:26, 18.75s/it]iteration: 21, accuracy: 0.5437641723356009\n","iteration: 22, accuracy: 0.5371882086167801:  28% 22/80 [07:57<17:50, 18.47s/it]iteration: 22, accuracy: 0.5371882086167801\n","iteration: 23, accuracy: 0.5378684807256235:  29% 23/80 [08:14<17:12, 18.11s/it]iteration: 23, accuracy: 0.5378684807256235\n","iteration: 24, accuracy: 0.5369614512471655:  30% 24/80 [08:31<16:40, 17.87s/it]iteration: 24, accuracy: 0.5369614512471655\n","iteration: 25, accuracy: 0.5371882086167801:  31% 25/80 [08:48<16:13, 17.70s/it]iteration: 25, accuracy: 0.5371882086167801\n","iteration: 26, accuracy: 0.5360544217687074:  32% 26/80 [09:06<15:43, 17.46s/it]iteration: 26, accuracy: 0.5360544217687074\n","iteration: 27, accuracy: 0.5392290249433107:  34% 27/80 [09:24<15:34, 17.63s/it]iteration: 27, accuracy: 0.5392290249433107\n","iteration: 28, accuracy: 0.5380952380952381:  35% 28/80 [09:43<15:34, 17.96s/it]iteration: 28, accuracy: 0.5380952380952381\n","iteration: 29, accuracy: 0.5356009070294785:  36% 29/80 [10:02<15:23, 18.10s/it]iteration: 29, accuracy: 0.5356009070294785\n","iteration: 30, accuracy: 0.5399092970521542:  38% 30/80 [10:18<15:09, 18.18s/it]iteration: 30, accuracy: 0.5399092970521542\n","iteration: 31, accuracy: 0.535374149659864:  39% 31/80 [10:34<14:20, 17.57s/it] iteration: 31, accuracy: 0.535374149659864\n","iteration: 32, accuracy: 0.527891156462585:  40% 32/80 [10:52<13:47, 17.24s/it]iteration: 32, accuracy: 0.527891156462585\n","iteration: 33, accuracy: 0.5263038548752834:  41% 33/80 [11:08<13:32, 17.28s/it]iteration: 33, accuracy: 0.5263038548752834\n","iteration: 34, accuracy: 0.527437641723356:  42% 34/80 [11:24<13:00, 16.96s/it] iteration: 34, accuracy: 0.527437641723356\n","iteration: 35, accuracy: 0.5301587301587302:  44% 35/80 [11:41<12:34, 16.76s/it]iteration: 35, accuracy: 0.5301587301587302\n","iteration: 36, accuracy: 0.519501133786848:  45% 36/80 [11:57<12:14, 16.69s/it] iteration: 36, accuracy: 0.519501133786848\n","iteration: 37, accuracy: 0.5251700680272109:  46% 37/80 [12:14<11:54, 16.62s/it]iteration: 37, accuracy: 0.5251700680272109\n","iteration: 38, accuracy: 0.5233560090702948:  48% 38/80 [12:30<11:45, 16.79s/it]iteration: 38, accuracy: 0.5233560090702948\n","iteration: 39, accuracy: 0.518140589569161:  49% 39/80 [12:47<11:15, 16.47s/it] iteration: 39, accuracy: 0.518140589569161\n","iteration: 40, accuracy: 0.5201814058956916:  50% 40/80 [13:04<11:03, 16.59s/it]iteration: 40, accuracy: 0.5201814058956916\n","iteration: 41, accuracy: 0.519501133786848:  51% 41/80 [13:19<10:47, 16.60s/it] iteration: 41, accuracy: 0.519501133786848\n","iteration: 42, accuracy: 0.5147392290249433:  52% 42/80 [13:37<10:19, 16.30s/it]iteration: 42, accuracy: 0.5147392290249433\n","iteration: 43, accuracy: 0.5136054421768708:  54% 43/80 [13:52<10:16, 16.65s/it]iteration: 43, accuracy: 0.5136054421768708\n","iteration: 44, accuracy: 0.5106575963718821:  55% 44/80 [14:07<09:40, 16.12s/it]iteration: 44, accuracy: 0.5106575963718821\n","iteration: 45, accuracy: 0.5077097505668934:  56% 45/80 [14:22<09:19, 15.99s/it]iteration: 45, accuracy: 0.5077097505668934\n","iteration: 46, accuracy: 0.5056689342403629:  57% 46/80 [14:38<08:53, 15.69s/it]iteration: 46, accuracy: 0.5056689342403629\n","iteration: 47, accuracy: 0.5061224489795918:  59% 47/80 [14:55<08:41, 15.81s/it]iteration: 47, accuracy: 0.5061224489795918\n","iteration: 48, accuracy: 0.4961451247165533:  60% 48/80 [15:09<08:34, 16.08s/it]iteration: 48, accuracy: 0.4961451247165533\n","iteration: 49, accuracy: 0.49455782312925173:  61% 49/80 [15:27<08:06, 15.68s/it]iteration: 49, accuracy: 0.49455782312925173\n","iteration: 50, accuracy: 0.49569160997732425:  62% 50/80 [15:43<08:03, 16.12s/it]iteration: 50, accuracy: 0.49569160997732425\n","iteration: 51, accuracy: 0.490702947845805:  64% 51/80 [15:59<07:47, 16.13s/it]  iteration: 51, accuracy: 0.490702947845805\n","iteration: 52, accuracy: 0.4931972789115646:  65% 52/80 [16:13<07:29, 16.04s/it]iteration: 52, accuracy: 0.4931972789115646\n","iteration: 53, accuracy: 0.49160997732426304:  66% 53/80 [16:29<07:01, 15.60s/it]iteration: 53, accuracy: 0.49160997732426304\n","iteration: 54, accuracy: 0.4888888888888889:  68% 54/80 [16:43<06:52, 15.88s/it] iteration: 54, accuracy: 0.4888888888888889\n","iteration: 55, accuracy: 0.49229024943310656:  69% 55/80 [16:57<06:18, 15.14s/it]iteration: 55, accuracy: 0.49229024943310656\n","iteration: 56, accuracy: 0.4909297052154195:  70% 56/80 [17:10<05:48, 14.50s/it] iteration: 56, accuracy: 0.4909297052154195\n","iteration: 57, accuracy: 0.4832199546485261:  71% 57/80 [17:25<05:28, 14.29s/it]iteration: 57, accuracy: 0.4832199546485261\n","iteration: 58, accuracy: 0.4879818594104308:  72% 58/80 [17:38<05:15, 14.33s/it]iteration: 58, accuracy: 0.4879818594104308\n","iteration: 59, accuracy: 0.49047619047619045:  74% 59/80 [17:53<04:57, 14.15s/it]iteration: 59, accuracy: 0.49047619047619045\n","iteration: 60, accuracy: 0.48412698412698413:  75% 60/80 [18:07<04:43, 14.16s/it]iteration: 60, accuracy: 0.48412698412698413\n","iteration: 61, accuracy: 0.4746031746031746:  76% 61/80 [18:22<04:31, 14.27s/it] iteration: 61, accuracy: 0.4746031746031746\n","iteration: 62, accuracy: 0.48027210884353744:  78% 62/80 [18:37<04:24, 14.69s/it]iteration: 62, accuracy: 0.48027210884353744\n","iteration: 63, accuracy: 0.47732426303854875:  79% 63/80 [18:52<04:09, 14.67s/it]iteration: 63, accuracy: 0.47732426303854875\n","iteration: 64, accuracy: 0.46984126984126984:  80% 64/80 [19:06<03:59, 15.00s/it]iteration: 64, accuracy: 0.46984126984126984\n","iteration: 65, accuracy: 0.4666666666666667:  81% 65/80 [19:20<03:38, 14.58s/it] iteration: 65, accuracy: 0.4666666666666667\n","iteration: 66, accuracy: 0.4600907029478458:  82% 66/80 [19:33<03:18, 14.17s/it]iteration: 66, accuracy: 0.4600907029478458\n","iteration: 67, accuracy: 0.46598639455782315:  84% 67/80 [19:48<03:00, 13.87s/it]iteration: 67, accuracy: 0.46598639455782315\n","iteration: 68, accuracy: 0.46077097505668935:  85% 68/80 [20:02<02:48, 14.03s/it]iteration: 68, accuracy: 0.46077097505668935\n","iteration: 69, accuracy: 0.4526077097505669:  86% 69/80 [20:16<02:35, 14.10s/it] iteration: 69, accuracy: 0.4526077097505669\n","iteration: 70, accuracy: 0.4505668934240363:  88% 70/80 [20:28<02:22, 14.20s/it]iteration: 70, accuracy: 0.4505668934240363\n","iteration: 71, accuracy: 0.4485260770975057:  89% 71/80 [20:42<02:03, 13.73s/it]iteration: 71, accuracy: 0.4485260770975057\n","iteration: 72, accuracy: 0.4485260770975057:  90% 72/80 [20:55<01:47, 13.47s/it]iteration: 72, accuracy: 0.4485260770975057\n","iteration: 73, accuracy: 0.4444444444444444:  91% 73/80 [21:09<01:34, 13.53s/it]iteration: 73, accuracy: 0.4444444444444444\n","iteration: 74, accuracy: 0.44058956916099773:  92% 74/80 [21:23<01:21, 13.52s/it]iteration: 74, accuracy: 0.44058956916099773\n","iteration: 75, accuracy: 0.4392290249433107:  94% 75/80 [21:36<01:08, 13.73s/it] iteration: 75, accuracy: 0.4392290249433107\n","iteration: 76, accuracy: 0.4396825396825397:  95% 76/80 [21:49<00:54, 13.55s/it]iteration: 76, accuracy: 0.4396825396825397\n","iteration: 77, accuracy: 0.43219954648526077:  96% 77/80 [22:03<00:41, 13.67s/it]iteration: 77, accuracy: 0.43219954648526077\n","iteration: 78, accuracy: 0.42970521541950113:  98% 78/80 [22:17<00:27, 13.74s/it]iteration: 78, accuracy: 0.42970521541950113\n","iteration: 79, accuracy: 0.4222222222222222:  99% 79/80 [22:31<00:13, 13.61s/it] iteration: 79, accuracy: 0.4222222222222222\n","iteration: 79, accuracy: 0.4222222222222222: 100% 80/80 [22:32<00:00, 16.91s/it]\n","Saving computed projection matrix to: /content/drive/MyDrive/bias-bench-main/results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-race_s-0.pt\n"]}]},{"cell_type":"code","source":["! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-mix23000_s-0.pt --bias_type gender"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RqsiqZx6e4FB","executionInfo":{"status":"ok","timestamp":1692628779161,"user_tz":-60,"elapsed":28332,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"4adcbbfb-48d8-46ac-e839-0921439aaef7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-mix23000_s-0.pt\n"," - load_path: None\n"," - bias_type: gender\n","Evaluating gender examples.\n","100% 262/262 [00:17<00:00, 14.91it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 47.71\n","Stereotype score: 63.52\n","Anti-stereotype score: 23.3\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 47.71\n"]}]},{"cell_type":"code","source":["! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-mix23000_s-0.pt --bias_type race"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AJdVEd6X9LJj","executionInfo":{"status":"ok","timestamp":1692628814205,"user_tz":-60,"elapsed":33039,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"7e1f669e-7ef5-4759-95a4-a40ab8ddc113"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-mix23000_s-0.pt\n"," - load_path: None\n"," - bias_type: race\n","Evaluating race-color examples.\n"," 70% 362/516 [00:16<00:05, 30.41it/s]Skipping example 363.\n","100% 515/516 [00:25<00:00, 20.29it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 48.35\n","Stereotype score: 47.77\n","Anti-stereotype score: 55.81\n","Num. neutral: 0.19\n","====================================================================================================\n","\n","Metric: 48.35\n"]}]},{"cell_type":"code","source":["! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-mix23000_s-0.pt --bias_type religion"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cyTv3yvb9LUl","executionInfo":{"status":"ok","timestamp":1692628825993,"user_tz":-60,"elapsed":11803,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"a1c01420-da63-42bb-fbf2-25c1728690b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-mix23000_s-0.pt\n"," - load_path: None\n"," - bias_type: religion\n","Evaluating religion examples.\n","100% 105/105 [00:04<00:00, 22.98it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 64.76\n","Stereotype score: 65.66\n","Anti-stereotype score: 50.0\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 64.76\n"]}]},{"cell_type":"markdown","source":["## 5.4 Verifying the effect of multi-type debiasing: change label\n","\n"],"metadata":{"id":"3Zbwc2H0XrMs"}},{"cell_type":"markdown","source":["### change label"],"metadata":{"id":"7RAs-VhVBRQd"}},{"cell_type":"markdown","metadata":{"id":"7sq-vWdTPnlH"},"source":["random"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3616056,"status":"ok","timestamp":1691321182325,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"gl9AZ72IXMA9","outputId":"3b535bdc-8b25-4c5f-dbf5-d783bd1c6735"},"outputs":[{"name":"stdout","output_type":"stream","text":["Computing projection matrix:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: AlbertModel\n"," - model_name_or_path: albert-base-v2\n"," - bias_type: race\n"," - n_classifiers: 80\n"," - seed: 0\n","Loading INLP data:   2% 26751/1372632 [00:01<01:16, 17649.70it/s]INLP dataset collected:\n"," - Num. male sentences: 3000\n"," - Num. female sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   2% 27135/1372632 [00:01<01:21, 16587.79it/s]\n","Loading INLP data:   6% 85534/1372632 [00:06<01:11, 17926.09it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   6% 86562/1372632 [00:06<01:35, 13396.69it/s]\n","Loading INLP data:   8% 108056/1372632 [00:05<01:06, 19070.43it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   8% 110011/1372632 [00:05<01:06, 18954.33it/s]\n","Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias']\n","- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Encoding male sentences: 100% 3000/3000 [05:17<00:00,  9.43it/s]\n","Encoding female sentences: 100% 3000/3000 [05:20<00:00,  9.36it/s]\n","Encoding neutral sentences: 100% 3000/3000 [04:42<00:00, 10.63it/s]\n","Dataset split sizes:\n","Train size: 4410; Dev size: 1890; Test size: 2700\n","Encoding bias sentences: 100% 3000/3000 [06:00<00:00,  8.32it/s]\n","Encoding neutral sentences: 100% 3000/3000 [04:47<00:00, 10.44it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Encoding bias sentences: 100% 3000/3000 [05:54<00:00,  8.46it/s]\n","Encoding neutral sentences: 100% 3000/3000 [04:47<00:00, 10.44it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Dataset split sizes:\n","Train size: 10290; Dev size: 4410; Test size: 6300\n","iteration: 0, accuracy: 0.6505668934240363:   0% 0/80 [00:27<?, ?it/s]iteration: 0, accuracy: 0.6505668934240363\n","iteration: 1, accuracy: 0.6480725623582766:   1% 1/80 [00:58<37:36, 28.56s/it]iteration: 1, accuracy: 0.6480725623582766\n","iteration: 2, accuracy: 0.6374149659863946:   2% 2/80 [01:22<38:20, 29.49s/it]iteration: 2, accuracy: 0.6374149659863946\n","iteration: 3, accuracy: 0.6460317460317461:   4% 3/80 [01:46<35:04, 27.33s/it]iteration: 3, accuracy: 0.6460317460317461\n","iteration: 4, accuracy: 0.6360544217687075:   5% 4/80 [02:06<32:35, 25.73s/it]iteration: 4, accuracy: 0.6360544217687075\n","iteration: 5, accuracy: 0.6312925170068027:   6% 5/80 [02:29<29:56, 23.95s/it]iteration: 5, accuracy: 0.6312925170068027\n","iteration: 6, accuracy: 0.6190476190476191:   8% 6/80 [02:49<28:48, 23.36s/it]iteration: 6, accuracy: 0.6190476190476191\n","iteration: 7, accuracy: 0.610204081632653:   9% 7/80 [03:10<27:29, 22.59s/it] iteration: 7, accuracy: 0.610204081632653\n","iteration: 8, accuracy: 0.5936507936507937:  10% 8/80 [03:30<26:10, 21.81s/it]iteration: 8, accuracy: 0.5936507936507937\n","iteration: 9, accuracy: 0.5843537414965987:  11% 9/80 [03:49<25:11, 21.29s/it]iteration: 9, accuracy: 0.5843537414965987\n","iteration: 10, accuracy: 0.5702947845804989:  12% 10/80 [04:10<24:12, 20.75s/it]iteration: 10, accuracy: 0.5702947845804989\n","iteration: 11, accuracy: 0.5566893424036281:  14% 11/80 [04:32<23:39, 20.58s/it]iteration: 11, accuracy: 0.5566893424036281\n","iteration: 12, accuracy: 0.546031746031746:  15% 12/80 [04:52<24:05, 21.25s/it] iteration: 12, accuracy: 0.546031746031746\n","iteration: 13, accuracy: 0.5369614512471655:  16% 13/80 [05:12<23:13, 20.79s/it]iteration: 13, accuracy: 0.5369614512471655\n","iteration: 14, accuracy: 0.5387755102040817:  18% 14/80 [05:33<22:29, 20.45s/it]iteration: 14, accuracy: 0.5387755102040817\n","iteration: 15, accuracy: 0.526984126984127:  19% 15/80 [05:54<22:33, 20.82s/it] iteration: 15, accuracy: 0.526984126984127\n","iteration: 16, accuracy: 0.5131519274376417:  20% 16/80 [06:12<22:07, 20.74s/it]iteration: 16, accuracy: 0.5131519274376417\n","iteration: 17, accuracy: 0.5020408163265306:  21% 17/80 [06:31<20:52, 19.88s/it]iteration: 17, accuracy: 0.5020408163265306\n","iteration: 18, accuracy: 0.48820861678004535:  22% 18/80 [06:51<20:29, 19.82s/it]iteration: 18, accuracy: 0.48820861678004535\n","iteration: 19, accuracy: 0.4852607709750567:  24% 19/80 [07:10<19:55, 19.60s/it] iteration: 19, accuracy: 0.4852607709750567\n","iteration: 20, accuracy: 0.47619047619047616:  25% 20/80 [07:28<19:31, 19.53s/it]iteration: 20, accuracy: 0.47619047619047616\n","iteration: 21, accuracy: 0.47891156462585033:  26% 21/80 [07:47<18:41, 19.01s/it]iteration: 21, accuracy: 0.47891156462585033\n","iteration: 22, accuracy: 0.47913832199546486:  28% 22/80 [08:05<18:30, 19.14s/it]iteration: 22, accuracy: 0.47913832199546486\n","iteration: 23, accuracy: 0.46757369614512473:  29% 23/80 [08:23<17:40, 18.60s/it]iteration: 23, accuracy: 0.46757369614512473\n","iteration: 24, accuracy: 0.4718820861678005:  30% 24/80 [08:42<17:21, 18.60s/it] iteration: 24, accuracy: 0.4718820861678005\n","iteration: 25, accuracy: 0.4689342403628118:  31% 25/80 [09:00<17:10, 18.75s/it]iteration: 25, accuracy: 0.4689342403628118\n","iteration: 26, accuracy: 0.4689342403628118:  32% 26/80 [09:18<16:41, 18.55s/it]iteration: 26, accuracy: 0.4689342403628118\n","iteration: 27, accuracy: 0.46825396825396826:  34% 27/80 [09:36<16:16, 18.42s/it]iteration: 27, accuracy: 0.46825396825396826\n","iteration: 28, accuracy: 0.4666666666666667:  35% 28/80 [09:53<15:42, 18.12s/it] iteration: 28, accuracy: 0.4666666666666667\n","iteration: 29, accuracy: 0.4616780045351474:  36% 29/80 [10:12<15:17, 17.99s/it]iteration: 29, accuracy: 0.4616780045351474\n","iteration: 30, accuracy: 0.45736961451247166:  38% 30/80 [10:30<15:08, 18.17s/it]iteration: 30, accuracy: 0.45736961451247166\n","iteration: 31, accuracy: 0.4528344671201814:  39% 31/80 [10:49<14:50, 18.17s/it] iteration: 31, accuracy: 0.4528344671201814\n","iteration: 32, accuracy: 0.4575963718820862:  40% 32/80 [11:07<14:35, 18.24s/it]iteration: 32, accuracy: 0.4575963718820862\n","iteration: 33, accuracy: 0.454421768707483:  41% 33/80 [11:26<14:26, 18.43s/it] iteration: 33, accuracy: 0.454421768707483\n","iteration: 34, accuracy: 0.45850340136054424:  42% 34/80 [11:44<14:05, 18.39s/it]iteration: 34, accuracy: 0.45850340136054424\n","iteration: 35, accuracy: 0.455328798185941:  44% 35/80 [12:02<13:46, 18.36s/it]  iteration: 35, accuracy: 0.455328798185941\n","iteration: 36, accuracy: 0.4496598639455782:  45% 36/80 [12:20<13:19, 18.16s/it]iteration: 36, accuracy: 0.4496598639455782\n","iteration: 37, accuracy: 0.45102040816326533:  46% 37/80 [12:39<13:07, 18.31s/it]iteration: 37, accuracy: 0.45102040816326533\n","iteration: 38, accuracy: 0.4523809523809524:  48% 38/80 [12:57<12:57, 18.52s/it] iteration: 38, accuracy: 0.4523809523809524\n","iteration: 39, accuracy: 0.4478458049886621:  49% 39/80 [13:14<12:29, 18.27s/it]iteration: 39, accuracy: 0.4478458049886621\n","iteration: 40, accuracy: 0.44693877551020406:  50% 40/80 [13:31<11:50, 17.77s/it]iteration: 40, accuracy: 0.44693877551020406\n","iteration: 41, accuracy: 0.4494331065759637:  51% 41/80 [13:47<11:26, 17.60s/it] iteration: 41, accuracy: 0.4494331065759637\n","iteration: 42, accuracy: 0.45034013605442175:  52% 42/80 [14:03<10:45, 16.98s/it]iteration: 42, accuracy: 0.45034013605442175\n","iteration: 43, accuracy: 0.4489795918367347:  54% 43/80 [14:18<10:21, 16.80s/it] iteration: 43, accuracy: 0.4489795918367347\n","iteration: 44, accuracy: 0.4478458049886621:  55% 44/80 [14:34<09:47, 16.33s/it]iteration: 44, accuracy: 0.4478458049886621\n","iteration: 45, accuracy: 0.44603174603174606:  56% 45/80 [14:50<09:28, 16.25s/it]iteration: 45, accuracy: 0.44603174603174606\n","iteration: 46, accuracy: 0.445124716553288:  57% 46/80 [15:05<09:12, 16.24s/it]  iteration: 46, accuracy: 0.445124716553288\n","iteration: 47, accuracy: 0.445578231292517:  59% 47/80 [15:19<08:40, 15.77s/it]iteration: 47, accuracy: 0.445578231292517\n","iteration: 48, accuracy: 0.44240362811791384:  60% 48/80 [15:35<08:11, 15.36s/it]iteration: 48, accuracy: 0.44240362811791384\n","iteration: 49, accuracy: 0.4414965986394558:  61% 49/80 [15:51<07:56, 15.38s/it] iteration: 49, accuracy: 0.4414965986394558\n","iteration: 50, accuracy: 0.4414965986394558:  62% 50/80 [16:06<07:47, 15.60s/it]iteration: 50, accuracy: 0.4414965986394558\n","iteration: 51, accuracy: 0.4437641723356009:  64% 51/80 [16:21<07:27, 15.44s/it]iteration: 51, accuracy: 0.4437641723356009\n","iteration: 52, accuracy: 0.4403628117913832:  65% 52/80 [16:36<07:07, 15.26s/it]iteration: 52, accuracy: 0.4403628117913832\n","iteration: 53, accuracy: 0.43741496598639457:  66% 53/80 [16:50<06:49, 15.18s/it]iteration: 53, accuracy: 0.43741496598639457\n","iteration: 54, accuracy: 0.44013605442176873:  68% 54/80 [17:05<06:27, 14.92s/it]iteration: 54, accuracy: 0.44013605442176873\n","iteration: 55, accuracy: 0.4396825396825397:  69% 55/80 [17:18<06:12, 14.92s/it] iteration: 55, accuracy: 0.4396825396825397\n","iteration: 56, accuracy: 0.436281179138322:  70% 56/80 [17:34<05:48, 14.52s/it] iteration: 56, accuracy: 0.436281179138322\n","iteration: 57, accuracy: 0.43537414965986393:  71% 57/80 [17:49<05:41, 14.86s/it]iteration: 57, accuracy: 0.43537414965986393\n","iteration: 58, accuracy: 0.4331065759637188:  72% 58/80 [18:03<05:29, 14.97s/it] iteration: 58, accuracy: 0.4331065759637188\n","iteration: 59, accuracy: 0.43718820861678004:  74% 59/80 [18:19<05:08, 14.67s/it]iteration: 59, accuracy: 0.43718820861678004\n","iteration: 60, accuracy: 0.43038548752834466:  75% 60/80 [18:32<04:57, 14.86s/it]iteration: 60, accuracy: 0.43038548752834466\n","iteration: 61, accuracy: 0.4310657596371882:  76% 61/80 [18:46<04:33, 14.41s/it] iteration: 61, accuracy: 0.4310657596371882\n","iteration: 62, accuracy: 0.4324263038548753:  78% 62/80 [19:01<04:17, 14.29s/it]iteration: 62, accuracy: 0.4324263038548753\n","iteration: 63, accuracy: 0.43038548752834466:  79% 63/80 [19:15<04:08, 14.62s/it]iteration: 63, accuracy: 0.43038548752834466\n","iteration: 64, accuracy: 0.4272108843537415:  80% 64/80 [19:29<03:49, 14.32s/it] iteration: 64, accuracy: 0.4272108843537415\n","iteration: 65, accuracy: 0.42448979591836733:  81% 65/80 [19:44<03:31, 14.13s/it]iteration: 65, accuracy: 0.42448979591836733\n","iteration: 66, accuracy: 0.42426303854875286:  82% 66/80 [19:59<03:22, 14.45s/it]iteration: 66, accuracy: 0.42426303854875286\n","iteration: 67, accuracy: 0.4215419501133787:  84% 67/80 [20:13<03:11, 14.76s/it] iteration: 67, accuracy: 0.4215419501133787\n","iteration: 68, accuracy: 0.41564625850340137:  85% 68/80 [20:27<02:53, 14.49s/it]iteration: 68, accuracy: 0.41564625850340137\n","iteration: 69, accuracy: 0.4129251700680272:  86% 69/80 [20:41<02:35, 14.13s/it] iteration: 69, accuracy: 0.4129251700680272\n","iteration: 70, accuracy: 0.408843537414966:  88% 70/80 [20:55<02:21, 14.19s/it] iteration: 70, accuracy: 0.408843537414966\n","iteration: 71, accuracy: 0.4036281179138322:  89% 71/80 [21:09<02:07, 14.11s/it]iteration: 71, accuracy: 0.4036281179138322\n","iteration: 72, accuracy: 0.3961451247165533:  90% 72/80 [21:23<01:53, 14.18s/it]iteration: 72, accuracy: 0.3961451247165533\n","iteration: 73, accuracy: 0.3975056689342404:  91% 73/80 [21:37<01:37, 14.00s/it]iteration: 73, accuracy: 0.3975056689342404\n","iteration: 74, accuracy: 0.3893424036281179:  92% 74/80 [21:50<01:24, 14.00s/it]iteration: 74, accuracy: 0.3893424036281179\n","iteration: 75, accuracy: 0.3866213151927438:  94% 75/80 [22:05<01:09, 13.83s/it]iteration: 75, accuracy: 0.3866213151927438\n","iteration: 76, accuracy: 0.3866213151927438:  95% 76/80 [22:18<00:55, 13.90s/it]iteration: 76, accuracy: 0.3866213151927438\n","iteration: 77, accuracy: 0.38798185941043084:  96% 77/80 [22:31<00:41, 13.74s/it]iteration: 77, accuracy: 0.38798185941043084\n","iteration: 78, accuracy: 0.38979591836734695:  98% 78/80 [22:45<00:27, 13.62s/it]iteration: 78, accuracy: 0.38979591836734695\n","iteration: 79, accuracy: 0.3746031746031746:  99% 79/80 [22:58<00:13, 13.55s/it] iteration: 79, accuracy: 0.3746031746031746\n","iteration: 79, accuracy: 0.3746031746031746: 100% 80/80 [22:59<00:00, 17.25s/it]\n","Saving computed projection matrix to: /content/drive/MyDrive/bias-bench-main/results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-race_s-0.pt\n"]}],"source":["! python experiments/inlp_projection_matrix.py --model AlbertModel --model_name_or_path albert-base-v2 --bias_type race --n_classifiers 80"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":106183,"status":"ok","timestamp":1691321451965,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"SBoqh2BtOSvo","outputId":"72d70101-efc9-41c0-e367-4d2987f58381"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-changeYmix3000_s-0.pt\n"," - load_path: None\n"," - bias_type: gender\n","Evaluating gender examples.\n","100% 262/262 [01:39<00:00,  2.62it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 43.51\n","Stereotype score: 44.65\n","Anti-stereotype score: 41.75\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 43.51\n"]}],"source":["\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-changeYmix3000_s-0.pt --bias_type gender"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":160367,"status":"ok","timestamp":1691321612300,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"p-SwxszEOSyA","outputId":"92b75110-b01a-45f7-b6da-90479eacc4f7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-changeYmix3000_s-0.pt\n"," - load_path: None\n"," - bias_type: race\n","Evaluating race-color examples.\n"," 70% 363/516 [01:47<00:32,  4.75it/s]Skipping example 363.\n","100% 515/516 [02:34<00:00,  3.33it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 42.33\n","Stereotype score: 40.68\n","Anti-stereotype score: 60.47\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 42.33\n"]}],"source":["\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-changeYmix3000_s-0.pt --bias_type race"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32676,"status":"ok","timestamp":1691321644966,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"Kpbe7z6DOS0n","outputId":"afeaa61f-cd03-497c-f4ba-c077e292a555"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-changeYmix3000_s-0.pt\n"," - load_path: None\n"," - bias_type: religion\n","Evaluating religion examples.\n","100% 105/105 [00:27<00:00,  3.84it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 74.29\n","Stereotype score: 75.76\n","Anti-stereotype score: 50.0\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 74.29\n"]}],"source":["\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-changeYmix3000_s-0.pt --bias_type religion"]},{"cell_type":"markdown","source":["### change label 0123456"],"metadata":{"id":"Q72_Rj9CjV2q"}},{"cell_type":"code","source":["! python experiments/inlp_projection_matrix.py --model AlbertModel --model_name_or_path albert-base-v2 --bias_type race --n_classifiers 80"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tg_FvDqgjWdI","executionInfo":{"status":"ok","timestamp":1692631891710,"user_tz":-60,"elapsed":478215,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"915449b8-dd89-4d5c-eb3a-9b052d122583"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Computing projection matrix:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: AlbertModel\n"," - model_name_or_path: albert-base-v2\n"," - bias_type: race\n"," - n_classifiers: 80\n"," - seed: 0\n","Loading INLP data:   2% 26465/1372632 [00:01<01:32, 14620.48it/s]INLP dataset collected:\n"," - Num. male sentences: 3000\n"," - Num. female sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   2% 27135/1372632 [00:02<01:39, 13495.00it/s]\n","Loading INLP data:   6% 86336/1372632 [00:06<01:24, 15137.38it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   6% 86562/1372632 [00:06<01:35, 13410.34it/s]\n","Loading INLP data:   8% 109337/1372632 [00:09<02:11, 9577.45it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   8% 110011/1372632 [00:09<01:45, 12021.44it/s]\n","Downloading: 100% 684/684 [00:00<00:00, 3.34MB/s]\n","Downloading: 100% 45.2M/45.2M [00:00<00:00, 87.8MB/s]\n","Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.bias', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.weight']\n","- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Downloading: 100% 742k/742k [00:00<00:00, 11.9MB/s]\n","Downloading: 100% 1.25M/1.25M [00:00<00:00, 22.1MB/s]\n","Encoding male sentences: 100% 3000/3000 [00:41<00:00, 71.67it/s]\n","Encoding female sentences: 100% 3000/3000 [00:37<00:00, 79.24it/s]\n","Encoding neutral sentences: 100% 3000/3000 [00:37<00:00, 80.27it/s]\n","Dataset split sizes:\n","Train size: 4410; Dev size: 1890; Test size: 2700\n","Encoding bias sentences: 100% 3000/3000 [00:39<00:00, 76.90it/s]\n","Encoding neutral sentences: 100% 3000/3000 [00:37<00:00, 80.69it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Encoding bias sentences: 100% 3000/3000 [00:37<00:00, 79.54it/s]\n","Encoding neutral sentences: 100% 3000/3000 [00:39<00:00, 76.36it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Dataset split sizes:\n","Train size: 10290; Dev size: 4410; Test size: 6300\n","iteration: 0, accuracy: 0.5260770975056689:   0% 0/80 [00:54<?, ?it/s]iteration: 0, accuracy: 0.5260770975056689\n","iteration: 1, accuracy: 0.5478458049886621:   1% 1/80 [01:45<1:12:17, 54.90s/it]iteration: 1, accuracy: 0.5478458049886621\n","iteration: 2, accuracy: 0.5446712018140589:   2% 2/80 [02:27<1:09:27, 53.43s/it]iteration: 2, accuracy: 0.5446712018140589\n","iteration: 3, accuracy: 0.5385487528344671:   4% 3/80 [03:07<1:01:07, 47.63s/it]iteration: 3, accuracy: 0.5385487528344671\n","iteration: 4, accuracy: 0.5351473922902494:   5% 4/80 [03:44<56:41, 44.76s/it]iteration: 4, accuracy: 0.5351473922902494\n","iteration: 5, accuracy: 0.5303854875283447:   6% 5/80 [04:25<52:32, 42.03s/it]iteration: 5, accuracy: 0.5303854875283447\n","iteration: 6, accuracy: 0.5174603174603175:   8% 6/80 [05:04<51:25, 41.70s/it]iteration: 6, accuracy: 0.5174603174603175\n","iteration: 7, accuracy: 0.5018140589569161:   9% 7/80 [05:40<49:33, 40.73s/it]iteration: 7, accuracy: 0.5018140589569161\n","iteration: 8, accuracy: 0.48934240362811793:  10% 8/80 [06:17<47:13, 39.35s/it]iteration: 8, accuracy: 0.48934240362811793\n","iteration: 9, accuracy: 0.47369614512471653:  11% 9/80 [06:51<45:25, 38.39s/it]iteration: 9, accuracy: 0.47369614512471653\n","iteration: 10, accuracy: 0.4587301587301587:  12% 10/80 [07:26<43:28, 37.26s/it]iteration: 10, accuracy: 0.4587301587301587\n","iteration: 11, accuracy: 0.4453514739229025:  14% 11/80 [08:03<41:57, 36.49s/it]iteration: 11, accuracy: 0.4453514739229025\n","iteration: 12, accuracy: 0.4301587301587302:  15% 12/80 [08:40<41:41, 36.79s/it]iteration: 12, accuracy: 0.4301587301587302\n","iteration: 13, accuracy: 0.4140589569160998:  16% 13/80 [09:13<40:46, 36.52s/it]iteration: 13, accuracy: 0.4140589569160998\n","iteration: 14, accuracy: 0.40589569160997735:  18% 14/80 [09:49<39:11, 35.63s/it]iteration: 14, accuracy: 0.40589569160997735\n","iteration: 15, accuracy: 0.3934240362811791:  19% 15/80 [10:24<38:47, 35.81s/it] iteration: 15, accuracy: 0.3934240362811791\n","iteration: 16, accuracy: 0.3832199546485261:  20% 16/80 [11:00<37:58, 35.60s/it]iteration: 16, accuracy: 0.3832199546485261\n","iteration: 17, accuracy: 0.3798185941043084:  21% 17/80 [11:34<37:25, 35.64s/it]iteration: 17, accuracy: 0.3798185941043084\n","iteration: 18, accuracy: 0.3691609977324263:  22% 18/80 [12:08<36:07, 34.95s/it]iteration: 18, accuracy: 0.3691609977324263\n","iteration: 19, accuracy: 0.35306122448979593:  24% 19/80 [12:40<35:30, 34.93s/it]iteration: 19, accuracy: 0.35306122448979593\n","iteration: 20, accuracy: 0.3437641723356009:  25% 20/80 [13:13<34:00, 34.02s/it] iteration: 20, accuracy: 0.3437641723356009\n","iteration: 21, accuracy: 0.3292517006802721:  26% 21/80 [13:45<32:55, 33.49s/it]iteration: 21, accuracy: 0.3292517006802721\n","iteration: 22, accuracy: 0.32222222222222224:  28% 22/80 [14:17<31:58, 33.07s/it]iteration: 22, accuracy: 0.32222222222222224\n","iteration: 23, accuracy: 0.31700680272108844:  29% 23/80 [14:52<31:19, 32.98s/it]iteration: 23, accuracy: 0.31700680272108844\n","iteration: 24, accuracy: 0.30680272108843537:  30% 24/80 [15:24<31:11, 33.41s/it]iteration: 24, accuracy: 0.30680272108843537\n","iteration: 25, accuracy: 0.29977324263038546:  31% 25/80 [15:57<30:09, 32.90s/it]iteration: 25, accuracy: 0.29977324263038546\n","iteration: 26, accuracy: 0.29931972789115646:  32% 26/80 [16:30<29:55, 33.26s/it]iteration: 26, accuracy: 0.29931972789115646\n","iteration: 27, accuracy: 0.28820861678004533:  34% 27/80 [17:01<29:11, 33.04s/it]iteration: 27, accuracy: 0.28820861678004533\n","iteration: 28, accuracy: 0.28798185941043086:  35% 28/80 [17:33<28:12, 32.54s/it]iteration: 28, accuracy: 0.28798185941043086\n","iteration: 29, accuracy: 0.273015873015873:  36% 29/80 [18:06<27:26, 32.28s/it]  iteration: 29, accuracy: 0.273015873015873\n","iteration: 30, accuracy: 0.2639455782312925:  38% 30/80 [18:37<27:16, 32.73s/it]iteration: 30, accuracy: 0.2639455782312925\n","iteration: 31, accuracy: 0.2585034013605442:  39% 31/80 [19:10<26:13, 32.11s/it]iteration: 31, accuracy: 0.2585034013605442\n","iteration: 32, accuracy: 0.2528344671201814:  40% 32/80 [19:41<25:46, 32.22s/it]iteration: 32, accuracy: 0.2528344671201814\n","iteration: 33, accuracy: 0.25011337868480726:  41% 33/80 [20:09<25:10, 32.14s/it]iteration: 33, accuracy: 0.25011337868480726\n","iteration: 34, accuracy: 0.24217687074829933:  42% 34/80 [20:38<23:22, 30.49s/it]iteration: 34, accuracy: 0.24217687074829933\n","iteration: 35, accuracy: 0.23605442176870747:  44% 35/80 [21:07<22:37, 30.16s/it]iteration: 35, accuracy: 0.23605442176870747\n","iteration: 36, accuracy: 0.2308390022675737:  45% 36/80 [21:37<21:47, 29.72s/it] iteration: 36, accuracy: 0.2308390022675737\n","iteration: 37, accuracy: 0.2165532879818594:  46% 37/80 [22:06<21:27, 29.95s/it]iteration: 37, accuracy: 0.2165532879818594\n","iteration: 38, accuracy: 0.22267573696145124:  48% 38/80 [22:32<20:44, 29.64s/it]iteration: 38, accuracy: 0.22267573696145124\n","iteration: 39, accuracy: 0.22653061224489796:  49% 39/80 [23:00<19:32, 28.59s/it]iteration: 39, accuracy: 0.22653061224489796\n","iteration: 40, accuracy: 0.21700680272108844:  50% 40/80 [23:26<18:51, 28.30s/it]iteration: 40, accuracy: 0.21700680272108844\n","iteration: 41, accuracy: 0.21972789115646257:  51% 41/80 [23:53<18:04, 27.80s/it]iteration: 41, accuracy: 0.21972789115646257\n","iteration: 42, accuracy: 0.2163265306122449:  52% 42/80 [24:20<17:27, 27.57s/it] iteration: 42, accuracy: 0.2163265306122449\n","iteration: 43, accuracy: 0.2111111111111111:  54% 43/80 [24:48<16:44, 27.14s/it]iteration: 43, accuracy: 0.2111111111111111\n","iteration: 44, accuracy: 0.20430839002267573:  55% 44/80 [25:14<16:24, 27.34s/it]iteration: 44, accuracy: 0.20430839002267573\n","iteration: 45, accuracy: 0.21201814058956917:  56% 45/80 [25:41<15:57, 27.36s/it]iteration: 45, accuracy: 0.21201814058956917\n","iteration: 46, accuracy: 0.20294784580498867:  57% 46/80 [26:09<15:17, 27.00s/it]iteration: 46, accuracy: 0.20294784580498867\n","iteration: 47, accuracy: 0.1909297052154195:  59% 47/80 [26:36<15:02, 27.34s/it] iteration: 47, accuracy: 0.1909297052154195\n","iteration: 48, accuracy: 0.19047619047619047:  60% 48/80 [27:00<14:31, 27.22s/it]iteration: 48, accuracy: 0.19047619047619047\n","iteration: 49, accuracy: 0.18775510204081633:  61% 49/80 [27:26<13:34, 26.26s/it]iteration: 49, accuracy: 0.18775510204081633\n","iteration: 50, accuracy: 0.18117913832199548:  62% 50/80 [27:50<13:01, 26.06s/it]iteration: 50, accuracy: 0.18117913832199548\n","iteration: 51, accuracy: 0.17687074829931973:  64% 51/80 [28:15<12:21, 25.58s/it]iteration: 51, accuracy: 0.17687074829931973\n","iteration: 52, accuracy: 0.1800453514739229:  65% 52/80 [28:36<11:45, 25.21s/it] iteration: 52, accuracy: 0.1800453514739229\n","iteration: 53, accuracy: 0.17233560090702948:  66% 53/80 [28:58<10:46, 23.95s/it]iteration: 53, accuracy: 0.17233560090702948\n","iteration: 54, accuracy: 0.17482993197278912:  68% 54/80 [29:21<10:16, 23.70s/it]iteration: 54, accuracy: 0.17482993197278912\n","iteration: 55, accuracy: 0.1782312925170068:  69% 55/80 [29:43<09:38, 23.13s/it] iteration: 55, accuracy: 0.1782312925170068\n","iteration: 56, accuracy: 0.17029478458049888:  70% 56/80 [30:06<09:16, 23.18s/it]iteration: 56, accuracy: 0.17029478458049888\n","iteration: 57, accuracy: 0.163718820861678:  71% 57/80 [30:28<08:46, 22.90s/it]  iteration: 57, accuracy: 0.163718820861678\n","iteration: 58, accuracy: 0.1621315192743764:  72% 58/80 [30:51<08:21, 22.80s/it]iteration: 58, accuracy: 0.1621315192743764\n","iteration: 59, accuracy: 0.16439909297052155:  74% 59/80 [31:13<07:55, 22.66s/it]iteration: 59, accuracy: 0.16439909297052155\n","iteration: 60, accuracy: 0.15963718820861678:  75% 60/80 [31:36<07:31, 22.60s/it]iteration: 60, accuracy: 0.15963718820861678\n","iteration: 61, accuracy: 0.1600907029478458:  76% 61/80 [31:55<07:11, 22.69s/it] iteration: 61, accuracy: 0.1600907029478458\n","iteration: 62, accuracy: 0.16621315192743763:  78% 62/80 [32:14<06:27, 21.54s/it]iteration: 62, accuracy: 0.16621315192743763\n","iteration: 63, accuracy: 0.1619047619047619:  79% 63/80 [32:33<05:54, 20.86s/it] iteration: 63, accuracy: 0.1619047619047619\n","iteration: 64, accuracy: 0.1600907029478458:  80% 64/80 [32:52<05:23, 20.23s/it]iteration: 64, accuracy: 0.1600907029478458\n","iteration: 65, accuracy: 0.15147392290249434:  81% 65/80 [33:12<05:01, 20.10s/it]iteration: 65, accuracy: 0.15147392290249434\n","iteration: 66, accuracy: 0.14875283446712018:  82% 66/80 [33:31<04:38, 19.92s/it]iteration: 66, accuracy: 0.14875283446712018\n","iteration: 67, accuracy: 0.15941043083900228:  84% 67/80 [33:51<04:15, 19.62s/it]iteration: 67, accuracy: 0.15941043083900228\n","iteration: 68, accuracy: 0.14943310657596373:  85% 68/80 [34:09<03:53, 19.49s/it]iteration: 68, accuracy: 0.14943310657596373\n","iteration: 69, accuracy: 0.15306122448979592:  86% 69/80 [34:30<03:34, 19.47s/it]iteration: 69, accuracy: 0.15306122448979592\n","iteration: 70, accuracy: 0.15147392290249434:  88% 70/80 [34:49<03:17, 19.74s/it]iteration: 70, accuracy: 0.15147392290249434\n","iteration: 71, accuracy: 0.16167800453514738:  89% 71/80 [35:07<02:55, 19.55s/it]iteration: 71, accuracy: 0.16167800453514738\n","iteration: 72, accuracy: 0.16122448979591836:  90% 72/80 [35:26<02:32, 19.07s/it]iteration: 72, accuracy: 0.16122448979591836\n","iteration: 73, accuracy: 0.16938775510204082:  91% 73/80 [35:43<02:11, 18.83s/it]iteration: 73, accuracy: 0.16938775510204082\n","iteration: 74, accuracy: 0.15532879818594103:  92% 74/80 [36:01<01:50, 18.41s/it]iteration: 74, accuracy: 0.15532879818594103\n","iteration: 75, accuracy: 0.1528344671201814:  94% 75/80 [36:20<01:32, 18.53s/it] iteration: 75, accuracy: 0.1528344671201814\n","iteration: 76, accuracy: 0.15034013605442176:  95% 76/80 [36:38<01:14, 18.53s/it]iteration: 76, accuracy: 0.15034013605442176\n","iteration: 77, accuracy: 0.14965986394557823:  96% 77/80 [36:54<00:54, 18.18s/it]iteration: 77, accuracy: 0.14965986394557823\n","iteration: 78, accuracy: 0.15396825396825398:  98% 78/80 [37:11<00:35, 17.70s/it]iteration: 78, accuracy: 0.15396825396825398\n","iteration: 79, accuracy: 0.1523809523809524:  99% 79/80 [37:26<00:17, 17.23s/it] iteration: 79, accuracy: 0.1523809523809524\n","iteration: 79, accuracy: 0.1523809523809524: 100% 80/80 [37:27<00:00, 28.09s/it]\n","Saving computed projection matrix to: /content/drive/MyDrive/bias-bench-main/results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-race_s-0.pt\n"]}]},{"cell_type":"code","source":["\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-changeY0123456mix3000_s-0.pt --bias_type gender"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S_gT1Fevjcwj","executionInfo":{"status":"ok","timestamp":1692632290061,"user_tz":-60,"elapsed":23331,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"f4cb67a6-fd8b-4522-b758-1d233393362d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-changeY0123456mix3000_s-0.pt\n"," - load_path: None\n"," - bias_type: gender\n","Evaluating gender examples.\n","100% 262/262 [00:15<00:00, 16.49it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 43.51\n","Stereotype score: 45.28\n","Anti-stereotype score: 40.78\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 43.51\n"]}]},{"cell_type":"code","source":["\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-changeY0123456mix3000_s-0.pt --bias_type race"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OH_N2J4tjc1S","executionInfo":{"status":"ok","timestamp":1692632321745,"user_tz":-60,"elapsed":31708,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"25a372cc-f5c7-4c68-9b1b-2bae7813114a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-changeY0123456mix3000_s-0.pt\n"," - load_path: None\n"," - bias_type: race\n","Evaluating race-color examples.\n"," 70% 361/516 [00:16<00:05, 30.71it/s]Skipping example 363.\n","100% 515/516 [00:22<00:00, 22.74it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 58.64\n","Stereotype score: 58.9\n","Anti-stereotype score: 55.81\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 58.64\n"]}]},{"cell_type":"code","source":["\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-changeY0123456mix3000_s-0.pt --bias_type religion"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1LrC66kLjc-4","executionInfo":{"status":"ok","timestamp":1692632264863,"user_tz":-60,"elapsed":16429,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"8d1e93fa-1e3f-4b04-eea3-4a2bc758fdd4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-changeY0123456mix3000_s-0.pt\n"," - load_path: None\n"," - bias_type: religion\n","Evaluating religion examples.\n","100% 105/105 [00:07<00:00, 14.43it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 48.57\n","Stereotype score: 47.47\n","Anti-stereotype score: 66.67\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 48.57\n"]}]},{"cell_type":"markdown","source":["### change data change label 0123456"],"metadata":{"id":"MKiDXCwUwF7L"}},{"cell_type":"code","source":["! python experiments/inlp_projection_matrix.py --model AlbertModel --model_name_or_path albert-base-v2 --bias_type race --n_classifiers 80"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b-EeX1ynwGQt","executionInfo":{"status":"ok","timestamp":1692634924605,"user_tz":-60,"elapsed":2484367,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"aab72d08-9fc0-4054-accd-5bddf08d7f25"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Computing projection matrix:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: AlbertModel\n"," - model_name_or_path: albert-base-v2\n"," - bias_type: race\n"," - n_classifiers: 80\n"," - seed: 0\n","Loading INLP data:  17% 23407/137263 [00:01<00:07, 14684.83it/s]INLP dataset collected:\n"," - Num. male sentences: 3000\n"," - Num. female sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:  17% 23887/137263 [00:01<00:07, 14190.14it/s]\n","Loading INLP data:  60% 82542/137263 [00:05<00:03, 15536.94it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:  61% 83086/137263 [00:05<00:03, 16053.23it/s]\n","Loading INLP data:  82% 112330/137263 [00:10<00:01, 15805.38it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:  82% 112566/137263 [00:11<00:02, 10223.30it/s]\n","Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.weight', 'predictions.decoder.weight', 'predictions.LayerNorm.bias', 'predictions.dense.bias']\n","- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Encoding male sentences: 100% 3000/3000 [00:38<00:00, 76.94it/s]\n","Encoding female sentences: 100% 3000/3000 [00:39<00:00, 75.65it/s]\n","Encoding neutral sentences: 100% 3000/3000 [00:40<00:00, 74.92it/s]\n","Dataset split sizes:\n","Train size: 4410; Dev size: 1890; Test size: 2700\n","Encoding bias sentences: 100% 3000/3000 [00:39<00:00, 75.82it/s]\n","Encoding neutral sentences: 100% 3000/3000 [00:38<00:00, 77.67it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Encoding bias sentences: 100% 3000/3000 [00:37<00:00, 79.47it/s]\n","Encoding neutral sentences: 100% 3000/3000 [00:37<00:00, 79.10it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Dataset split sizes:\n","Train size: 10290; Dev size: 4410; Test size: 6300\n","iteration: 0, accuracy: 0.5312925170068027:   0% 0/80 [00:48<?, ?it/s]iteration: 0, accuracy: 0.5312925170068027\n","iteration: 1, accuracy: 0.5471655328798186:   1% 1/80 [01:45<1:05:56, 50.09s/it]iteration: 1, accuracy: 0.5471655328798186\n","iteration: 2, accuracy: 0.5507936507936508:   2% 2/80 [02:23<1:10:00, 53.85s/it]iteration: 2, accuracy: 0.5507936507936508\n","iteration: 3, accuracy: 0.5494331065759637:   4% 3/80 [03:00<59:50, 46.63s/it]iteration: 3, accuracy: 0.5494331065759637\n","iteration: 4, accuracy: 0.5401360544217687:   5% 4/80 [03:35<54:04, 42.69s/it]iteration: 4, accuracy: 0.5401360544217687\n","iteration: 5, accuracy: 0.5265306122448979:   6% 5/80 [04:11<49:56, 39.96s/it]iteration: 5, accuracy: 0.5265306122448979\n","iteration: 6, accuracy: 0.5156462585034014:   8% 6/80 [04:46<47:53, 38.84s/it]iteration: 6, accuracy: 0.5156462585034014\n","iteration: 7, accuracy: 0.5002267573696145:   9% 7/80 [05:20<45:37, 37.50s/it]iteration: 7, accuracy: 0.5002267573696145\n","iteration: 8, accuracy: 0.490702947845805:  10% 8/80 [05:54<43:42, 36.42s/it] iteration: 8, accuracy: 0.490702947845805\n","iteration: 9, accuracy: 0.47392290249433106:  11% 9/80 [06:28<42:13, 35.68s/it]iteration: 9, accuracy: 0.47392290249433106\n","iteration: 10, accuracy: 0.4557823129251701:  12% 10/80 [07:04<41:09, 35.28s/it]iteration: 10, accuracy: 0.4557823129251701\n","iteration: 11, accuracy: 0.4417233560090703:  14% 11/80 [07:41<40:33, 35.27s/it]iteration: 11, accuracy: 0.4417233560090703\n","iteration: 12, accuracy: 0.4287981859410431:  15% 12/80 [08:18<40:32, 35.77s/it]iteration: 12, accuracy: 0.4287981859410431\n","iteration: 13, accuracy: 0.42108843537414964:  16% 13/80 [08:56<40:20, 36.12s/it]iteration: 13, accuracy: 0.42108843537414964\n","iteration: 14, accuracy: 0.3975056689342404:  18% 14/80 [09:34<40:34, 36.89s/it] iteration: 14, accuracy: 0.3975056689342404\n","iteration: 15, accuracy: 0.39138321995464853:  19% 15/80 [10:08<40:09, 37.07s/it]iteration: 15, accuracy: 0.39138321995464853\n","iteration: 16, accuracy: 0.37256235827664397:  20% 16/80 [10:42<38:34, 36.16s/it]iteration: 16, accuracy: 0.37256235827664397\n","iteration: 17, accuracy: 0.36961451247165533:  21% 17/80 [11:14<37:31, 35.74s/it]iteration: 17, accuracy: 0.36961451247165533\n","iteration: 18, accuracy: 0.3578231292517007:  22% 18/80 [11:46<35:38, 34.49s/it] iteration: 18, accuracy: 0.3578231292517007\n","iteration: 19, accuracy: 0.34625850340136055:  24% 19/80 [12:19<34:17, 33.74s/it]iteration: 19, accuracy: 0.34625850340136055\n","iteration: 20, accuracy: 0.33582766439909295:  25% 20/80 [12:54<33:23, 33.39s/it]iteration: 20, accuracy: 0.33582766439909295\n","iteration: 21, accuracy: 0.326984126984127:  26% 21/80 [13:25<33:21, 33.92s/it]  iteration: 21, accuracy: 0.326984126984127\n","iteration: 22, accuracy: 0.31405895691609975:  28% 22/80 [13:57<31:50, 32.94s/it]iteration: 22, accuracy: 0.31405895691609975\n","iteration: 23, accuracy: 0.30839002267573695:  29% 23/80 [14:29<30:56, 32.58s/it]iteration: 23, accuracy: 0.30839002267573695\n","iteration: 24, accuracy: 0.3020408163265306:  30% 24/80 [14:58<30:15, 32.42s/it] iteration: 24, accuracy: 0.3020408163265306\n","iteration: 25, accuracy: 0.29591836734693877:  31% 25/80 [15:29<28:51, 31.49s/it]iteration: 25, accuracy: 0.29591836734693877\n","iteration: 26, accuracy: 0.28616780045351475:  32% 26/80 [16:00<28:21, 31.51s/it]iteration: 26, accuracy: 0.28616780045351475\n","iteration: 27, accuracy: 0.28208616780045354:  34% 27/80 [16:31<27:45, 31.42s/it]iteration: 27, accuracy: 0.28208616780045354\n","iteration: 28, accuracy: 0.27414965986394557:  35% 28/80 [17:03<27:04, 31.25s/it]iteration: 28, accuracy: 0.27414965986394557\n","iteration: 29, accuracy: 0.26961451247165535:  36% 29/80 [17:32<26:30, 31.19s/it]iteration: 29, accuracy: 0.26961451247165535\n","iteration: 30, accuracy: 0.26099773242630386:  38% 30/80 [18:03<25:39, 30.79s/it]iteration: 30, accuracy: 0.26099773242630386\n","iteration: 31, accuracy: 0.2528344671201814:  39% 31/80 [18:34<24:57, 30.56s/it] iteration: 31, accuracy: 0.2528344671201814\n","iteration: 32, accuracy: 0.2519274376417234:  40% 32/80 [19:02<24:41, 30.87s/it]iteration: 32, accuracy: 0.2519274376417234\n","iteration: 33, accuracy: 0.2566893424036281:  41% 33/80 [19:31<23:32, 30.05s/it]iteration: 33, accuracy: 0.2566893424036281\n","iteration: 34, accuracy: 0.24875283446712018:  42% 34/80 [20:00<22:38, 29.53s/it]iteration: 34, accuracy: 0.24875283446712018\n","iteration: 35, accuracy: 0.24285714285714285:  44% 35/80 [20:27<22:03, 29.42s/it]iteration: 35, accuracy: 0.24285714285714285\n","iteration: 36, accuracy: 0.24240362811791383:  45% 36/80 [20:55<21:18, 29.06s/it]iteration: 36, accuracy: 0.24240362811791383\n","iteration: 37, accuracy: 0.227437641723356:  46% 37/80 [21:21<20:27, 28.54s/it]  iteration: 37, accuracy: 0.227437641723356\n","iteration: 38, accuracy: 0.2253968253968254:  48% 38/80 [21:47<19:34, 27.97s/it]iteration: 38, accuracy: 0.2253968253968254\n","iteration: 39, accuracy: 0.22517006802721087:  49% 39/80 [22:14<18:34, 27.19s/it]iteration: 39, accuracy: 0.22517006802721087\n","iteration: 40, accuracy: 0.2219954648526077:  50% 40/80 [22:43<18:09, 27.25s/it] iteration: 40, accuracy: 0.2219954648526077\n","iteration: 41, accuracy: 0.218140589569161:  51% 41/80 [23:10<17:52, 27.51s/it] iteration: 41, accuracy: 0.218140589569161\n","iteration: 42, accuracy: 0.21020408163265306:  52% 42/80 [23:34<17:20, 27.38s/it]iteration: 42, accuracy: 0.21020408163265306\n","iteration: 43, accuracy: 0.20793650793650795:  54% 43/80 [23:59<16:15, 26.36s/it]iteration: 43, accuracy: 0.20793650793650795\n","iteration: 44, accuracy: 0.209297052154195:  55% 44/80 [24:23<15:35, 25.98s/it]  iteration: 44, accuracy: 0.209297052154195\n","iteration: 45, accuracy: 0.2036281179138322:  56% 45/80 [24:48<14:49, 25.42s/it]iteration: 45, accuracy: 0.2036281179138322\n","iteration: 46, accuracy: 0.19931972789115646:  57% 46/80 [25:11<14:22, 25.38s/it]iteration: 46, accuracy: 0.19931972789115646\n","iteration: 47, accuracy: 0.2015873015873016:  59% 47/80 [25:34<13:28, 24.50s/it] iteration: 47, accuracy: 0.2015873015873016\n","iteration: 48, accuracy: 0.18979591836734694:  60% 48/80 [25:58<12:58, 24.34s/it]iteration: 48, accuracy: 0.18979591836734694\n","iteration: 49, accuracy: 0.18662131519274378:  61% 49/80 [26:21<12:20, 23.88s/it]iteration: 49, accuracy: 0.18662131519274378\n","iteration: 50, accuracy: 0.17777777777777778:  62% 50/80 [26:43<11:50, 23.67s/it]iteration: 50, accuracy: 0.17777777777777778\n","iteration: 51, accuracy: 0.17233560090702948:  64% 51/80 [27:08<11:21, 23.51s/it]iteration: 51, accuracy: 0.17233560090702948\n","iteration: 52, accuracy: 0.17029478458049888:  65% 52/80 [27:33<11:10, 23.95s/it]iteration: 52, accuracy: 0.17029478458049888\n","iteration: 53, accuracy: 0.17188208616780046:  66% 53/80 [27:56<10:48, 24.00s/it]iteration: 53, accuracy: 0.17188208616780046\n","iteration: 54, accuracy: 0.17324263038548754:  68% 54/80 [28:19<10:22, 23.94s/it]iteration: 54, accuracy: 0.17324263038548754\n","iteration: 55, accuracy: 0.16848072562358277:  69% 55/80 [28:41<09:43, 23.34s/it]iteration: 55, accuracy: 0.16848072562358277\n","iteration: 56, accuracy: 0.16870748299319727:  70% 56/80 [29:03<09:09, 22.88s/it]iteration: 56, accuracy: 0.16870748299319727\n","iteration: 57, accuracy: 0.17029478458049888:  71% 57/80 [29:26<08:44, 22.80s/it]iteration: 57, accuracy: 0.17029478458049888\n","iteration: 58, accuracy: 0.16303854875283447:  72% 58/80 [29:49<08:22, 22.84s/it]iteration: 58, accuracy: 0.16303854875283447\n","iteration: 59, accuracy: 0.1528344671201814:  74% 59/80 [30:12<07:59, 22.84s/it] iteration: 59, accuracy: 0.1528344671201814\n","iteration: 60, accuracy: 0.15034013605442176:  75% 60/80 [30:34<07:41, 23.08s/it]iteration: 60, accuracy: 0.15034013605442176\n","iteration: 61, accuracy: 0.15396825396825398:  76% 61/80 [30:54<07:10, 22.64s/it]iteration: 61, accuracy: 0.15396825396825398\n","iteration: 62, accuracy: 0.15306122448979592:  78% 62/80 [31:16<06:34, 21.93s/it]iteration: 62, accuracy: 0.15306122448979592\n","iteration: 63, accuracy: 0.1523809523809524:  79% 63/80 [31:37<06:14, 22.04s/it] iteration: 63, accuracy: 0.1523809523809524\n","iteration: 64, accuracy: 0.14875283446712018:  80% 64/80 [31:57<05:43, 21.46s/it]iteration: 64, accuracy: 0.14875283446712018\n","iteration: 65, accuracy: 0.14875283446712018:  81% 65/80 [32:15<05:14, 20.98s/it]iteration: 65, accuracy: 0.14875283446712018\n","iteration: 66, accuracy: 0.14308390022675738:  82% 66/80 [32:34<04:42, 20.21s/it]iteration: 66, accuracy: 0.14308390022675738\n","iteration: 67, accuracy: 0.1523809523809524:  84% 67/80 [32:53<04:18, 19.86s/it] iteration: 67, accuracy: 0.1523809523809524\n","iteration: 68, accuracy: 0.15532879818594103:  85% 68/80 [33:11<03:55, 19.62s/it]iteration: 68, accuracy: 0.15532879818594103\n","iteration: 69, accuracy: 0.14671201814058957:  86% 69/80 [33:29<03:31, 19.18s/it]iteration: 69, accuracy: 0.14671201814058957\n","iteration: 70, accuracy: 0.15170068027210884:  88% 70/80 [33:47<03:10, 19.06s/it]iteration: 70, accuracy: 0.15170068027210884\n","iteration: 71, accuracy: 0.1458049886621315:  89% 71/80 [34:05<02:47, 18.66s/it] iteration: 71, accuracy: 0.1458049886621315\n","iteration: 72, accuracy: 0.1580498866213152:  90% 72/80 [34:23<02:27, 18.40s/it]iteration: 72, accuracy: 0.1580498866213152\n","iteration: 73, accuracy: 0.1528344671201814:  91% 73/80 [34:40<02:07, 18.18s/it]iteration: 73, accuracy: 0.1528344671201814\n","iteration: 74, accuracy: 0.15578231292517006:  92% 74/80 [34:57<01:48, 18.01s/it]iteration: 74, accuracy: 0.15578231292517006\n","iteration: 75, accuracy: 0.15691609977324264:  94% 75/80 [35:14<01:28, 17.74s/it]iteration: 75, accuracy: 0.15691609977324264\n","iteration: 76, accuracy: 0.1489795918367347:  95% 76/80 [35:31<01:09, 17.46s/it] iteration: 76, accuracy: 0.1489795918367347\n","iteration: 77, accuracy: 0.14965986394557823:  96% 77/80 [35:48<00:52, 17.34s/it]iteration: 77, accuracy: 0.14965986394557823\n","iteration: 78, accuracy: 0.14875283446712018:  98% 78/80 [36:05<00:34, 17.32s/it]iteration: 78, accuracy: 0.14875283446712018\n","iteration: 79, accuracy: 0.14943310657596373:  99% 79/80 [36:20<00:17, 17.02s/it]iteration: 79, accuracy: 0.14943310657596373\n","iteration: 79, accuracy: 0.14943310657596373: 100% 80/80 [36:22<00:00, 27.28s/it]\n","Saving computed projection matrix to: /content/drive/MyDrive/bias-bench-main/results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-race_s-0.pt\n"]}]},{"cell_type":"code","source":["\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-changeYchangedata0123456mix3000_s-0.pt --bias_type gender"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U4ecskyswGiG","executionInfo":{"status":"ok","timestamp":1692635266092,"user_tz":-60,"elapsed":25355,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"062312a1-87a1-4a77-e25b-f785e625b95a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-changeYchangedata0123456mix3000_s-0.pt\n"," - load_path: None\n"," - bias_type: gender\n","Evaluating gender examples.\n","100% 262/262 [00:16<00:00, 16.04it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 47.33\n","Stereotype score: 47.8\n","Anti-stereotype score: 46.6\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 47.33\n"]}]},{"cell_type":"code","source":["\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-changeYchangedata0123456mix3000_s-0.pt --bias_type race"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yUhnomDgwcpF","executionInfo":{"status":"ok","timestamp":1692635298847,"user_tz":-60,"elapsed":32770,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"3d0a9028-e932-4931-907a-1867f226804b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-changeYchangedata0123456mix3000_s-0.pt\n"," - load_path: None\n"," - bias_type: race\n","Evaluating race-color examples.\n"," 70% 362/516 [00:17<00:04, 30.95it/s]Skipping example 363.\n","100% 515/516 [00:25<00:00, 20.16it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 48.93\n","Stereotype score: 48.09\n","Anti-stereotype score: 58.14\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 48.93\n"]}]},{"cell_type":"code","source":["\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-changeYchangedata0123456mix3000_s-0.pt --bias_type religion"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZkLPEGwawcxz","executionInfo":{"status":"ok","timestamp":1692635220491,"user_tz":-60,"elapsed":14197,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"93ff2261-1fd6-430e-b452-ec9ceafd8efe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-changeYchangedata0123456mix3000_s-0.pt\n"," - load_path: None\n"," - bias_type: religion\n","Evaluating religion examples.\n","100% 105/105 [00:06<00:00, 17.08it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 50.48\n","Stereotype score: 51.52\n","Anti-stereotype score: 33.33\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 50.48\n"]}]},{"cell_type":"markdown","source":["### change label 0 1\n"],"metadata":{"id":"0FNBXQexFuoM"}},{"cell_type":"code","source":["! python experiments/inlp_projection_matrix.py --model AlbertModel --model_name_or_path albert-base-v2 --bias_type race --n_classifiers 80"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZyageSYWFx5s","executionInfo":{"status":"ok","timestamp":1692924822697,"user_tz":-60,"elapsed":1009,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"a98895c3-7c76-4690-da40-9ca5d232c0a1"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Computing projection matrix:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: AlbertModel\n"," - model_name_or_path: albert-base-v2\n"," - bias_type: race\n"," - n_classifiers: 80\n"," - seed: 0\n","Loading INLP data:   2% 25884/1372632 [00:01<01:34, 14178.34it/s]INLP dataset collected:\n"," - Num. male sentences: 3000\n"," - Num. female sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   2% 27135/1372632 [00:02<01:39, 13458.47it/s]\n","Loading INLP data:   6% 86141/1372632 [00:08<02:24, 8925.18it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   6% 86562/1372632 [00:08<02:00, 10717.04it/s]\n","Loading INLP data:   8% 109691/1372632 [00:09<02:26, 8616.41it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   8% 110011/1372632 [00:09<01:46, 11832.54it/s]\n","Data saved to race_data.csv\n","Downloading: 100% 684/684 [00:00<00:00, 3.44MB/s]\n","Downloading: 100% 45.2M/45.2M [00:01<00:00, 46.0MB/s]\n","Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.dense.weight', 'predictions.LayerNorm.bias']\n","- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Downloading: 100% 742k/742k [00:00<00:00, 3.95MB/s]\n","Downloading: 100% 1.25M/1.25M [00:00<00:00, 6.57MB/s]\n","Encoding male sentences: 100% 3000/3000 [00:41<00:00, 72.93it/s]\n","Encoding female sentences: 100% 3000/3000 [00:39<00:00, 75.08it/s]\n","Encoding neutral sentences: 100% 3000/3000 [00:38<00:00, 78.89it/s]\n","Dataset split sizes:\n","Train size: 4410; Dev size: 1890; Test size: 2700\n","Encoding bias sentences: 100% 3000/3000 [00:41<00:00, 72.29it/s]\n","Encoding neutral sentences: 100% 3000/3000 [00:39<00:00, 76.46it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Encoding bias sentences: 100% 3000/3000 [00:37<00:00, 80.21it/s]\n","Encoding neutral sentences: 100% 3000/3000 [00:39<00:00, 76.58it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Dataset split sizes:\n","Train size: 10290; Dev size: 4410; Test size: 6300\n","iteration: 0, accuracy: 0.8124716553287982:   0% 0/80 [00:11<?, ?it/s]iteration: 0, accuracy: 0.8124716553287982\n","iteration: 1, accuracy: 0.8158730158730159:   1% 1/80 [00:37<17:57, 13.64s/it]iteration: 1, accuracy: 0.8158730158730159\n","iteration: 2, accuracy: 0.8133786848072563:   2% 2/80 [00:52<26:05, 20.07s/it]iteration: 2, accuracy: 0.8133786848072563\n","iteration: 3, accuracy: 0.810204081632653:   4% 3/80 [01:01<23:05, 18.00s/it] iteration: 3, accuracy: 0.810204081632653\n","iteration: 4, accuracy: 0.8045351473922903:   5% 4/80 [01:12<18:23, 14.52s/it]iteration: 4, accuracy: 0.8045351473922903\n","iteration: 5, accuracy: 0.8063492063492064:   6% 5/80 [01:20<16:13, 12.98s/it]iteration: 5, accuracy: 0.8063492063492064\n","iteration: 6, accuracy: 0.7979591836734694:   8% 6/80 [01:29<14:08, 11.47s/it]iteration: 6, accuracy: 0.7979591836734694\n","iteration: 7, accuracy: 0.792063492063492:   9% 7/80 [01:38<12:59, 10.67s/it] iteration: 7, accuracy: 0.792063492063492\n","iteration: 8, accuracy: 0.7804988662131519:  10% 8/80 [01:52<12:12, 10.18s/it]iteration: 8, accuracy: 0.7804988662131519\n","iteration: 9, accuracy: 0.7689342403628118:  11% 9/80 [02:02<13:18, 11.25s/it]iteration: 9, accuracy: 0.7689342403628118\n","iteration: 10, accuracy: 0.7680272108843538:  12% 10/80 [02:14<12:32, 10.75s/it]iteration: 10, accuracy: 0.7680272108843538\n","iteration: 11, accuracy: 0.7537414965986394:  14% 11/80 [02:25<12:57, 11.27s/it]iteration: 11, accuracy: 0.7537414965986394\n","iteration: 12, accuracy: 0.753061224489796:  15% 12/80 [02:37<12:58, 11.44s/it] iteration: 12, accuracy: 0.753061224489796\n","iteration: 13, accuracy: 0.7585034013605442:  16% 13/80 [02:48<12:52, 11.54s/it]iteration: 13, accuracy: 0.7585034013605442\n","iteration: 14, accuracy: 0.7521541950113378:  18% 14/80 [02:57<12:16, 11.16s/it]iteration: 14, accuracy: 0.7521541950113378\n","iteration: 15, accuracy: 0.7421768707482993:  19% 15/80 [03:05<11:13, 10.36s/it]iteration: 15, accuracy: 0.7421768707482993\n","iteration: 16, accuracy: 0.735827664399093:  20% 16/80 [03:13<10:28,  9.83s/it] iteration: 16, accuracy: 0.735827664399093\n","iteration: 17, accuracy: 0.7346938775510204:  21% 17/80 [03:22<09:47,  9.32s/it]iteration: 17, accuracy: 0.7346938775510204\n","iteration: 18, accuracy: 0.7290249433106576:  22% 18/80 [03:30<09:18,  9.01s/it]iteration: 18, accuracy: 0.7290249433106576\n","iteration: 19, accuracy: 0.7258503401360544:  24% 19/80 [03:38<09:04,  8.93s/it]iteration: 19, accuracy: 0.7258503401360544\n","iteration: 20, accuracy: 0.7090702947845805:  25% 20/80 [03:45<08:41,  8.69s/it]iteration: 20, accuracy: 0.7090702947845805\n","iteration: 21, accuracy: 0.7070294784580499:  26% 21/80 [03:54<08:09,  8.29s/it]iteration: 21, accuracy: 0.7070294784580499\n","iteration: 22, accuracy: 0.6975056689342404:  28% 22/80 [04:00<07:57,  8.24s/it]iteration: 22, accuracy: 0.6975056689342404\n","iteration: 23, accuracy: 0.6913832199546486:  29% 23/80 [04:10<07:19,  7.71s/it]iteration: 23, accuracy: 0.6913832199546486\n","iteration: 24, accuracy: 0.6884353741496598:  30% 24/80 [04:16<07:35,  8.13s/it]iteration: 24, accuracy: 0.6884353741496598\n","iteration: 25, accuracy: 0.6827664399092971:  31% 25/80 [04:25<06:58,  7.61s/it]iteration: 25, accuracy: 0.6827664399092971\n","iteration: 26, accuracy: 0.6800453514739229:  32% 26/80 [04:32<07:10,  7.98s/it]iteration: 26, accuracy: 0.6800453514739229\n","iteration: 27, accuracy: 0.6752834467120181:  34% 27/80 [04:41<06:56,  7.87s/it]iteration: 27, accuracy: 0.6752834467120181\n","iteration: 28, accuracy: 0.6764172335600908:  35% 28/80 [04:47<06:53,  7.95s/it]iteration: 28, accuracy: 0.6764172335600908\n","iteration: 29, accuracy: 0.6682539682539682:  36% 29/80 [04:55<06:28,  7.62s/it]iteration: 29, accuracy: 0.6682539682539682\n","iteration: 30, accuracy: 0.6707482993197279:  38% 30/80 [05:02<06:27,  7.75s/it]iteration: 30, accuracy: 0.6707482993197279\n","iteration: 31, accuracy: 0.6693877551020408:  39% 31/80 [05:10<06:06,  7.48s/it]iteration: 31, accuracy: 0.6693877551020408\n","iteration: 32, accuracy: 0.6616780045351474:  40% 32/80 [05:20<06:24,  8.00s/it]iteration: 32, accuracy: 0.6616780045351474\n","iteration: 33, accuracy: 0.6666666666666666:  41% 33/80 [05:26<06:16,  8.02s/it]iteration: 33, accuracy: 0.6666666666666666\n","iteration: 34, accuracy: 0.6648526077097505:  42% 34/80 [05:35<05:51,  7.64s/it]iteration: 34, accuracy: 0.6648526077097505\n","iteration: 35, accuracy: 0.6557823129251701:  44% 35/80 [05:42<05:52,  7.83s/it]iteration: 35, accuracy: 0.6557823129251701\n","iteration: 36, accuracy: 0.654421768707483:  45% 36/80 [05:48<05:34,  7.59s/it] iteration: 36, accuracy: 0.654421768707483\n","iteration: 37, accuracy: 0.653968253968254:  46% 37/80 [05:56<05:21,  7.48s/it]iteration: 37, accuracy: 0.653968253968254\n","iteration: 38, accuracy: 0.6560090702947846:  48% 38/80 [06:05<05:14,  7.49s/it]iteration: 38, accuracy: 0.6560090702947846\n","iteration: 39, accuracy: 0.6505668934240363:  49% 39/80 [06:12<05:22,  7.86s/it]iteration: 39, accuracy: 0.6505668934240363\n","iteration: 40, accuracy: 0.6507936507936508:  50% 40/80 [06:19<05:05,  7.64s/it]iteration: 40, accuracy: 0.6507936507936508\n","iteration: 41, accuracy: 0.6437641723356009:  51% 41/80 [06:26<04:50,  7.46s/it]iteration: 41, accuracy: 0.6437641723356009\n","iteration: 42, accuracy: 0.6408163265306123:  52% 42/80 [06:33<04:33,  7.20s/it]iteration: 42, accuracy: 0.6408163265306123\n","iteration: 43, accuracy: 0.6349206349206349:  54% 43/80 [06:41<04:36,  7.47s/it]iteration: 43, accuracy: 0.6349206349206349\n","iteration: 44, accuracy: 0.6242630385487529:  55% 44/80 [06:47<04:22,  7.30s/it]iteration: 44, accuracy: 0.6242630385487529\n","iteration: 45, accuracy: 0.6204081632653061:  56% 45/80 [06:55<04:05,  7.01s/it]iteration: 45, accuracy: 0.6204081632653061\n","iteration: 46, accuracy: 0.6136054421768707:  57% 46/80 [07:01<04:07,  7.28s/it]iteration: 46, accuracy: 0.6136054421768707\n","iteration: 47, accuracy: 0.6170068027210884:  59% 47/80 [07:09<03:51,  7.01s/it]iteration: 47, accuracy: 0.6170068027210884\n","iteration: 48, accuracy: 0.6117913832199546:  60% 48/80 [07:15<03:48,  7.15s/it]iteration: 48, accuracy: 0.6117913832199546\n","iteration: 49, accuracy: 0.6068027210884354:  61% 49/80 [07:21<03:33,  6.88s/it]iteration: 49, accuracy: 0.6068027210884354\n","iteration: 50, accuracy: 0.599092970521542:  62% 50/80 [07:29<03:23,  6.78s/it] iteration: 50, accuracy: 0.599092970521542\n","iteration: 51, accuracy: 0.6020408163265306:  64% 51/80 [07:35<03:22,  6.98s/it]iteration: 51, accuracy: 0.6020408163265306\n","iteration: 52, accuracy: 0.6034013605442177:  65% 52/80 [07:43<03:11,  6.85s/it]iteration: 52, accuracy: 0.6034013605442177\n","iteration: 53, accuracy: 0.6047619047619047:  66% 53/80 [07:51<03:15,  7.24s/it]iteration: 53, accuracy: 0.6047619047619047\n","iteration: 54, accuracy: 0.6056689342403628:  68% 54/80 [07:57<03:04,  7.09s/it]iteration: 54, accuracy: 0.6056689342403628\n","iteration: 55, accuracy: 0.6056689342403628:  69% 55/80 [08:05<02:54,  6.99s/it]iteration: 55, accuracy: 0.6056689342403628\n","iteration: 56, accuracy: 0.6040816326530613:  70% 56/80 [08:11<02:52,  7.19s/it]iteration: 56, accuracy: 0.6040816326530613\n","iteration: 57, accuracy: 0.5995464852607709:  71% 57/80 [08:18<02:41,  7.01s/it]iteration: 57, accuracy: 0.5995464852607709\n","iteration: 58, accuracy: 0.5995464852607709:  72% 58/80 [08:25<02:31,  6.90s/it]iteration: 58, accuracy: 0.5995464852607709\n","iteration: 59, accuracy: 0.5993197278911565:  74% 59/80 [08:31<02:21,  6.72s/it]iteration: 59, accuracy: 0.5993197278911565\n","iteration: 60, accuracy: 0.599092970521542:  75% 60/80 [08:38<02:17,  6.90s/it] iteration: 60, accuracy: 0.599092970521542\n","iteration: 61, accuracy: 0.5997732426303855:  76% 61/80 [08:43<02:08,  6.74s/it]iteration: 61, accuracy: 0.5997732426303855\n","iteration: 62, accuracy: 0.5947845804988662:  78% 62/80 [08:50<01:55,  6.41s/it]iteration: 62, accuracy: 0.5947845804988662\n","iteration: 63, accuracy: 0.6009070294784581:  79% 63/80 [08:56<01:47,  6.31s/it]iteration: 63, accuracy: 0.6009070294784581\n","iteration: 64, accuracy: 0.5995464852607709:  80% 64/80 [09:02<01:39,  6.20s/it]iteration: 64, accuracy: 0.5995464852607709\n","iteration: 65, accuracy: 0.5922902494331066:  81% 65/80 [09:09<01:35,  6.34s/it]iteration: 65, accuracy: 0.5922902494331066\n","iteration: 66, accuracy: 0.5888888888888889:  82% 66/80 [09:15<01:29,  6.40s/it]iteration: 66, accuracy: 0.5888888888888889\n","iteration: 67, accuracy: 0.5929705215419501:  84% 67/80 [09:22<01:21,  6.25s/it]iteration: 67, accuracy: 0.5929705215419501\n","iteration: 68, accuracy: 0.5866213151927437:  85% 68/80 [09:29<01:17,  6.43s/it]iteration: 68, accuracy: 0.5866213151927437\n","iteration: 69, accuracy: 0.5854875283446712:  86% 69/80 [09:36<01:12,  6.57s/it]iteration: 69, accuracy: 0.5854875283446712\n","iteration: 70, accuracy: 0.582312925170068:  88% 70/80 [09:44<01:10,  7.05s/it] iteration: 70, accuracy: 0.582312925170068\n","iteration: 71, accuracy: 0.5834467120181406:  89% 71/80 [09:49<01:02,  6.97s/it]iteration: 71, accuracy: 0.5834467120181406\n","iteration: 72, accuracy: 0.5841269841269842:  90% 72/80 [09:56<00:54,  6.80s/it]iteration: 72, accuracy: 0.5841269841269842\n","iteration: 73, accuracy: 0.5800453514739229:  91% 73/80 [10:02<00:45,  6.49s/it]iteration: 73, accuracy: 0.5800453514739229\n","iteration: 74, accuracy: 0.5780045351473923:  92% 74/80 [10:10<00:38,  6.44s/it]iteration: 74, accuracy: 0.5780045351473923\n","iteration: 75, accuracy: 0.5723356009070295:  94% 75/80 [10:17<00:35,  7.08s/it]iteration: 75, accuracy: 0.5723356009070295\n","iteration: 76, accuracy: 0.5682539682539682:  95% 76/80 [10:23<00:27,  6.82s/it]iteration: 76, accuracy: 0.5682539682539682\n","iteration: 77, accuracy: 0.5698412698412698:  96% 77/80 [10:29<00:19,  6.52s/it]iteration: 77, accuracy: 0.5698412698412698\n","iteration: 78, accuracy: 0.573015873015873:  98% 78/80 [10:36<00:13,  6.67s/it] iteration: 78, accuracy: 0.573015873015873\n","iteration: 79, accuracy: 0.5702947845804989:  99% 79/80 [10:42<00:06,  6.44s/it]iteration: 79, accuracy: 0.5702947845804989\n","iteration: 79, accuracy: 0.5702947845804989: 100% 80/80 [10:44<00:00,  8.06s/it]\n","Saving computed projection matrix to: /content/drive/MyDrive/bias-bench-main/results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-race_s-0.pt\n"]}]},{"cell_type":"code","source":["\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-changeY10mix3000_s-0.pt --bias_type gender"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UUfRmxAGFyER","executionInfo":{"status":"ok","timestamp":1692925009061,"user_tz":-60,"elapsed":24332,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"baf63413-a466-4e08-8187-8ee1857d651c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-changeY10mix3000_s-0.pt\n"," - load_path: None\n"," - bias_type: gender\n","Evaluating gender examples.\n","100% 262/262 [00:15<00:00, 16.41it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 53.05\n","Stereotype score: 47.8\n","Anti-stereotype score: 61.17\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 53.05\n"]}]},{"cell_type":"code","source":["\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-changeY10mix3000_s-0.pt --bias_type race"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LT37kTyzFyLH","executionInfo":{"status":"ok","timestamp":1692925042263,"user_tz":-60,"elapsed":33205,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"0956bb9d-fbce-4111-ecb6-e57853d6a84f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-changeY10mix3000_s-0.pt\n"," - load_path: None\n"," - bias_type: race\n","Evaluating race-color examples.\n"," 70% 362/516 [00:17<00:07, 21.63it/s]Skipping example 363.\n","100% 515/516 [00:24<00:00, 20.85it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 53.79\n","Stereotype score: 54.87\n","Anti-stereotype score: 41.86\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 53.79\n"]}]},{"cell_type":"code","source":["\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-changeY10mix3000_s-0.pt --bias_type religion"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X0iNadBfFyT8","executionInfo":{"status":"ok","timestamp":1692924983382,"user_tz":-60,"elapsed":17471,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"05bcc08e-cbb6-4939-c4c2-347c74bc2e01"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-changeY10mix3000_s-0.pt\n"," - load_path: None\n"," - bias_type: religion\n","Evaluating religion examples.\n","100% 105/105 [00:04<00:00, 22.04it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 58.1\n","Stereotype score: 58.59\n","Anti-stereotype score: 50.0\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 58.1\n"]}]},{"cell_type":"markdown","source":["## Other try"],"metadata":{"id":"b7iuPWecYTl6"}},{"cell_type":"markdown","metadata":{"id":"mnozmsjrGY5c"},"source":["### gender race religion 1000 30\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1292587,"status":"ok","timestamp":1690728343937,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"pykCAt2qGYTC","outputId":"c3c39428-42de-436c-f83f-801d510544b9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Computing projection matrix:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: AlbertModel\n"," - model_name_or_path: albert-base-v2\n"," - bias_type: race\n"," - n_classifiers: 30\n"," - seed: 0\n","Loading INLP data:   1% 8732/1372632 [00:00<01:52, 12072.20it/s]INLP dataset collected:\n"," - Num. male sentences: 1000\n"," - Num. female sentences: 1000\n"," - Num. neutral sentences: 1000\n","Loading INLP data:   1% 9399/1372632 [00:00<02:04, 10951.95it/s]\n","Loading INLP data:   2% 26632/1372632 [00:02<01:41, 13215.69it/s]INLP dataset collected:\n"," - Num. bias sentences: 1000\n"," - Num. neutral sentences: 1000\n","Loading INLP data:   2% 27691/1372632 [00:02<01:43, 12960.15it/s]\n","Loading INLP data:   3% 37175/1372632 [00:04<02:54, 7643.58it/s]INLP dataset collected:\n"," - Num. bias sentences: 1000\n"," - Num. neutral sentences: 1000\n","Loading INLP data:   3% 37378/1372632 [00:04<02:50, 7837.51it/s]\n","Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.bias', 'predictions.dense.weight', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.decoder.weight']\n","- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Encoding male sentences: 100% 1000/1000 [02:22<00:00,  7.02it/s]\n","Encoding female sentences: 100% 1000/1000 [02:21<00:00,  7.04it/s]\n","Encoding neutral sentences: 100% 1000/1000 [02:08<00:00,  7.76it/s]\n","Dataset split sizes:\n","Train size: 1470; Dev size: 630; Test size: 900\n","Encoding bias sentences: 100% 1000/1000 [02:43<00:00,  6.10it/s]\n","Encoding neutral sentences: 100% 1000/1000 [02:09<00:00,  7.74it/s]\n","Dataset split sizes:\n","Train size: 980; Dev size: 420; Test size: 600\n","Encoding bias sentences: 100% 1000/1000 [02:41<00:00,  6.20it/s]\n","Encoding neutral sentences: 100% 1000/1000 [02:08<00:00,  7.81it/s]\n","Dataset split sizes:\n","Train size: 980; Dev size: 420; Test size: 600\n","Dataset split sizes:\n","Train size: 3430; Dev size: 1470; Test size: 2100\n","iteration: 0, accuracy: 0.6244897959183674:   0% 0/30 [00:11<?, ?it/s]iteration: 0, accuracy: 0.6244897959183674\n","iteration: 1, accuracy: 0.6646258503401361:   3% 1/30 [00:24<05:41, 11.79s/it]iteration: 1, accuracy: 0.6646258503401361\n","iteration: 2, accuracy: 0.6591836734693878:   7% 2/30 [00:35<06:01, 12.89s/it]iteration: 2, accuracy: 0.6591836734693878\n","iteration: 3, accuracy: 0.6482993197278911:  10% 3/30 [00:45<05:25, 12.05s/it]iteration: 3, accuracy: 0.6482993197278911\n","iteration: 4, accuracy: 0.6530612244897959:  13% 4/30 [00:55<04:47, 11.08s/it]iteration: 4, accuracy: 0.6530612244897959\n","iteration: 5, accuracy: 0.6510204081632653:  17% 5/30 [01:05<04:33, 10.93s/it]iteration: 5, accuracy: 0.6510204081632653\n","iteration: 6, accuracy: 0.6360544217687075:  20% 6/30 [01:15<04:08, 10.35s/it]iteration: 6, accuracy: 0.6360544217687075\n","iteration: 7, accuracy: 0.6387755102040816:  23% 7/30 [01:25<03:55, 10.25s/it]iteration: 7, accuracy: 0.6387755102040816\n","iteration: 8, accuracy: 0.6408163265306123:  27% 8/30 [01:35<03:41, 10.05s/it]iteration: 8, accuracy: 0.6408163265306123\n","iteration: 9, accuracy: 0.6156462585034014:  30% 9/30 [01:43<03:31, 10.09s/it]iteration: 9, accuracy: 0.6156462585034014\n","iteration: 10, accuracy: 0.608843537414966:  33% 10/30 [01:53<03:12,  9.60s/it]iteration: 10, accuracy: 0.608843537414966\n","iteration: 11, accuracy: 0.6047619047619047:  37% 11/30 [02:02<03:02,  9.62s/it]iteration: 11, accuracy: 0.6047619047619047\n","iteration: 12, accuracy: 0.5897959183673469:  40% 12/30 [02:11<02:50,  9.46s/it]iteration: 12, accuracy: 0.5897959183673469\n","iteration: 13, accuracy: 0.5972789115646259:  43% 13/30 [02:20<02:38,  9.32s/it]iteration: 13, accuracy: 0.5972789115646259\n","iteration: 14, accuracy: 0.5775510204081633:  47% 14/30 [02:29<02:26,  9.18s/it]iteration: 14, accuracy: 0.5775510204081633\n","iteration: 15, accuracy: 0.5755102040816327:  50% 15/30 [02:37<02:17,  9.17s/it]iteration: 15, accuracy: 0.5755102040816327\n","iteration: 16, accuracy: 0.5700680272108843:  53% 16/30 [02:46<02:05,  8.97s/it]iteration: 16, accuracy: 0.5700680272108843\n","iteration: 17, accuracy: 0.5639455782312925:  57% 17/30 [02:54<01:55,  8.86s/it]iteration: 17, accuracy: 0.5639455782312925\n","iteration: 18, accuracy: 0.5571428571428572:  60% 18/30 [03:03<01:45,  8.76s/it]iteration: 18, accuracy: 0.5571428571428572\n","iteration: 19, accuracy: 0.5557823129251701:  63% 19/30 [03:12<01:36,  8.75s/it]iteration: 19, accuracy: 0.5557823129251701\n","iteration: 20, accuracy: 0.5503401360544218:  67% 20/30 [03:20<01:27,  8.76s/it]iteration: 20, accuracy: 0.5503401360544218\n","iteration: 21, accuracy: 0.5326530612244897:  70% 21/30 [03:28<01:17,  8.59s/it]iteration: 21, accuracy: 0.5326530612244897\n","iteration: 22, accuracy: 0.5462585034013605:  73% 22/30 [03:36<01:07,  8.40s/it]iteration: 22, accuracy: 0.5462585034013605\n","iteration: 23, accuracy: 0.5340136054421769:  77% 23/30 [03:43<00:57,  8.26s/it]iteration: 23, accuracy: 0.5340136054421769\n","iteration: 24, accuracy: 0.5292517006802722:  80% 24/30 [03:51<00:47,  7.87s/it]iteration: 24, accuracy: 0.5292517006802722\n","iteration: 25, accuracy: 0.5340136054421769:  83% 25/30 [03:59<00:40,  8.06s/it]iteration: 25, accuracy: 0.5340136054421769\n","iteration: 26, accuracy: 0.5319727891156463:  87% 26/30 [04:06<00:31,  7.78s/it]iteration: 26, accuracy: 0.5319727891156463\n","iteration: 27, accuracy: 0.5292517006802722:  90% 27/30 [04:14<00:23,  7.78s/it]iteration: 27, accuracy: 0.5292517006802722\n","iteration: 28, accuracy: 0.5244897959183673:  93% 28/30 [04:20<00:15,  7.64s/it]iteration: 28, accuracy: 0.5244897959183673\n","iteration: 29, accuracy: 0.5176870748299319:  97% 29/30 [04:28<00:07,  7.40s/it]iteration: 29, accuracy: 0.5176870748299319\n","iteration: 29, accuracy: 0.5176870748299319: 100% 30/30 [04:29<00:00,  8.98s/it]\n","Saving computed projection matrix to: /content/drive/MyDrive/bias-bench-main/results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-race_s-0.pt\n"]}],"source":["\n","! python experiments/inlp_projection_matrix.py --model AlbertModel --model_name_or_path albert-base-v2 --bias_type race --n_classifiers 30"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":176759,"status":"ok","timestamp":1690812163753,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"BqGCKPLwolXT","outputId":"43e00e1d-b895-4d2a-dc64-78226122adaf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-genderracereligion_s-0.pt\n"," - load_path: None\n"," - bias_type: gender\n","Downloading: 100% 684/684 [00:00<00:00, 2.47MB/s]\n","Downloading: 100% 45.2M/45.2M [00:00<00:00, 58.4MB/s]\n","Downloading: 100% 742k/742k [00:00<00:00, 8.76MB/s]\n","Downloading: 100% 1.25M/1.25M [00:00<00:00, 6.91MB/s]\n","Evaluating gender examples.\n","100% 262/262 [02:31<00:00,  1.73it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 49.62\n","Stereotype score: 52.83\n","Anti-stereotype score: 44.66\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 49.62\n"]}],"source":["# albert的inlp crows_debias --bias_type gender 测试混合gender race religion matrix\n","\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-genderracereligion_s-0.pt --bias_type gender"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":290566,"status":"ok","timestamp":1690812805434,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"oc3DOtmTolZq","outputId":"25b13d4c-c01a-4a7c-df6f-5a2495c36f8b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-genderracereligion_s-0.pt\n"," - load_path: None\n"," - bias_type: race\n","Evaluating race-color examples.\n"," 70% 363/516 [03:24<01:01,  2.50it/s]Skipping example 363.\n","100% 515/516 [04:40<00:00,  1.84it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 38.25\n","Stereotype score: 37.71\n","Anti-stereotype score: 44.19\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 38.25\n"]}],"source":["# albert的inlp crows_debias --bias_type race\n","\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-genderracereligion_s-0.pt --bias_type race"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44410,"status":"ok","timestamp":1690729921500,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"nvpx1MaiolcI","outputId":"39bc9ac9-8bb0-470f-931c-04d95ae83481"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/My Drive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-race_s-0.pt\n"," - load_path: None\n"," - bias_type: religion\n","Evaluating religion examples.\n","100% 105/105 [00:35<00:00,  2.92it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 76.19\n","Stereotype score: 78.79\n","Anti-stereotype score: 33.33\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 76.19\n"]}],"source":["# albert的inlp crows_debias --bias_type religion\n","\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-genderracereligion_s-0.pt --bias_type religion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lHypYz5VoleU"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["###  race gender religion 1000 30"],"metadata":{"id":"_2MM8-kj_8Xp"}},{"cell_type":"markdown","metadata":{"id":"O_FD8Ph6QTyX"},"source":["3、将三个数据集按照race gender religion链接起来，放进矩阵生成中。\n","修改context_nullspace_projection.py和inlp_projection_matrix.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1516214,"status":"ok","timestamp":1690814342758,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"PbuUT_18olg7","outputId":"096b47fe-5a09-4a3f-f1a1-c43d97c953f3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Computing projection matrix:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: AlbertModel\n"," - model_name_or_path: albert-base-v2\n"," - bias_type: race\n"," - n_classifiers: 30\n"," - seed: 0\n","Loading INLP data:   1% 8543/1372632 [00:00<01:54, 11925.39it/s]INLP dataset collected:\n"," - Num. male sentences: 1000\n"," - Num. female sentences: 1000\n"," - Num. neutral sentences: 1000\n","Loading INLP data:   1% 9399/1372632 [00:00<02:09, 10540.55it/s]\n","Loading INLP data:   2% 26872/1372632 [00:02<01:45, 12763.18it/s]INLP dataset collected:\n"," - Num. bias sentences: 1000\n"," - Num. neutral sentences: 1000\n","Loading INLP data:   2% 27691/1372632 [00:02<01:44, 12866.01it/s]\n","Loading INLP data:   3% 37162/1372632 [00:04<02:34, 8643.96it/s]INLP dataset collected:\n"," - Num. bias sentences: 1000\n"," - Num. neutral sentences: 1000\n","Loading INLP data:   3% 37378/1372632 [00:04<02:24, 9243.96it/s]\n","Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.bias']\n","- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Encoding male sentences: 100% 1000/1000 [02:50<00:00,  5.88it/s]\n","Encoding female sentences: 100% 1000/1000 [02:46<00:00,  6.02it/s]\n","Encoding neutral sentences: 100% 1000/1000 [02:25<00:00,  6.86it/s]\n","Dataset split sizes:\n","Train size: 1470; Dev size: 630; Test size: 900\n","Encoding bias sentences: 100% 1000/1000 [03:15<00:00,  5.13it/s]\n","Encoding neutral sentences: 100% 1000/1000 [02:28<00:00,  6.72it/s]\n","Dataset split sizes:\n","Train size: 980; Dev size: 420; Test size: 600\n","Encoding bias sentences: 100% 1000/1000 [03:06<00:00,  5.37it/s]\n","Encoding neutral sentences: 100% 1000/1000 [02:35<00:00,  6.44it/s]\n","Dataset split sizes:\n","Train size: 980; Dev size: 420; Test size: 600\n","Dataset split sizes:\n","Train size: 3430; Dev size: 1470; Test size: 2100\n","iteration: 0, accuracy: 0.6238095238095238:   0% 0/30 [00:13<?, ?it/s]iteration: 0, accuracy: 0.6238095238095238\n","iteration: 1, accuracy: 0.6646258503401361:   3% 1/30 [00:31<06:53, 14.25s/it]iteration: 1, accuracy: 0.6646258503401361\n","iteration: 2, accuracy: 0.6591836734693878:   7% 2/30 [00:45<07:38, 16.39s/it]iteration: 2, accuracy: 0.6591836734693878\n","iteration: 3, accuracy: 0.6482993197278911:  10% 3/30 [00:56<06:47, 15.08s/it]iteration: 3, accuracy: 0.6482993197278911\n","iteration: 4, accuracy: 0.6510204081632653:  13% 4/30 [01:08<06:01, 13.92s/it]iteration: 4, accuracy: 0.6510204081632653\n","iteration: 5, accuracy: 0.6496598639455783:  17% 5/30 [01:19<05:21, 12.86s/it]iteration: 5, accuracy: 0.6496598639455783\n","iteration: 6, accuracy: 0.6360544217687075:  20% 6/30 [01:30<04:56, 12.35s/it]iteration: 6, accuracy: 0.6360544217687075\n","iteration: 7, accuracy: 0.6401360544217687:  23% 7/30 [01:42<04:35, 11.98s/it]iteration: 7, accuracy: 0.6401360544217687\n","iteration: 8, accuracy: 0.6414965986394557:  27% 8/30 [01:52<04:19, 11.78s/it]iteration: 8, accuracy: 0.6414965986394557\n","iteration: 9, accuracy: 0.6149659863945578:  30% 9/30 [02:02<04:03, 11.60s/it]iteration: 9, accuracy: 0.6149659863945578\n","iteration: 10, accuracy: 0.6061224489795919:  33% 10/30 [02:13<03:37, 10.89s/it]iteration: 10, accuracy: 0.6061224489795919\n","iteration: 11, accuracy: 0.6047619047619047:  37% 11/30 [02:24<03:29, 11.05s/it]iteration: 11, accuracy: 0.6047619047619047\n","iteration: 12, accuracy: 0.5904761904761905:  40% 12/30 [02:35<03:18, 11.04s/it]iteration: 12, accuracy: 0.5904761904761905\n","iteration: 13, accuracy: 0.5952380952380952:  43% 13/30 [02:45<03:02, 10.76s/it]iteration: 13, accuracy: 0.5952380952380952\n","iteration: 14, accuracy: 0.5761904761904761:  47% 14/30 [02:54<02:50, 10.68s/it]iteration: 14, accuracy: 0.5761904761904761\n","iteration: 15, accuracy: 0.5741496598639456:  50% 15/30 [03:04<02:34, 10.28s/it]iteration: 15, accuracy: 0.5741496598639456\n","iteration: 16, accuracy: 0.5700680272108843:  53% 16/30 [03:14<02:24, 10.32s/it]iteration: 16, accuracy: 0.5700680272108843\n","iteration: 17, accuracy: 0.563265306122449:  57% 17/30 [03:24<02:07,  9.83s/it] iteration: 17, accuracy: 0.563265306122449\n","iteration: 18, accuracy: 0.5551020408163265:  60% 18/30 [03:32<01:58,  9.89s/it]iteration: 18, accuracy: 0.5551020408163265\n","iteration: 19, accuracy: 0.5557823129251701:  63% 19/30 [03:42<01:44,  9.54s/it]iteration: 19, accuracy: 0.5557823129251701\n","iteration: 20, accuracy: 0.5503401360544218:  67% 20/30 [03:51<01:36,  9.67s/it]iteration: 20, accuracy: 0.5503401360544218\n","iteration: 21, accuracy: 0.5346938775510204:  70% 21/30 [04:01<01:25,  9.52s/it]iteration: 21, accuracy: 0.5346938775510204\n","iteration: 22, accuracy: 0.5503401360544218:  73% 22/30 [04:08<01:16,  9.54s/it]iteration: 22, accuracy: 0.5503401360544218\n","iteration: 23, accuracy: 0.5319727891156463:  77% 23/30 [04:17<01:02,  8.94s/it]iteration: 23, accuracy: 0.5319727891156463\n","iteration: 24, accuracy: 0.5312925170068027:  80% 24/30 [04:25<00:53,  8.91s/it]iteration: 24, accuracy: 0.5312925170068027\n","iteration: 25, accuracy: 0.5374149659863946:  83% 25/30 [04:34<00:42,  8.44s/it]iteration: 25, accuracy: 0.5374149659863946\n","iteration: 26, accuracy: 0.5312925170068027:  87% 26/30 [04:42<00:34,  8.65s/it]iteration: 26, accuracy: 0.5312925170068027\n","iteration: 27, accuracy: 0.5312925170068027:  90% 27/30 [04:50<00:25,  8.35s/it]iteration: 27, accuracy: 0.5312925170068027\n","iteration: 28, accuracy: 0.5217687074829932:  93% 28/30 [04:58<00:17,  8.52s/it]iteration: 28, accuracy: 0.5217687074829932\n","iteration: 29, accuracy: 0.5149659863945578:  97% 29/30 [05:07<00:08,  8.29s/it]iteration: 29, accuracy: 0.5149659863945578\n","iteration: 29, accuracy: 0.5149659863945578: 100% 30/30 [05:08<00:00, 10.28s/it]\n","Saving computed projection matrix to: /content/drive/MyDrive/bias-bench-main/results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-race_s-0.pt\n"]}],"source":["\n","! python experiments/inlp_projection_matrix.py --model AlbertModel --model_name_or_path albert-base-v2 --bias_type race --n_classifiers 30"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":187564,"status":"ok","timestamp":1690814632261,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"toJq5Py8QSn0","outputId":"3fcf8d41-370a-4ddf-9f60-d25b0a82e06e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-racegenderreligion_s-0.pt\n"," - load_path: None\n"," - bias_type: gender\n","Evaluating gender examples.\n","100% 262/262 [02:58<00:00,  1.47it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 48.47\n","Stereotype score: 50.94\n","Anti-stereotype score: 44.66\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 48.47\n"]}],"source":["# albert的inlp crows_debias --bias_type gender 测试混合 race gender religion matrix\n","\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-racegenderreligion_s-0.pt --bias_type gender"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":251122,"status":"ok","timestamp":1690814892327,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"C8ooexwIQSqW","outputId":"af80f5cd-9f1d-42a6-94a4-a57be23c4854"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-racegenderreligion_s-0.pt\n"," - load_path: None\n"," - bias_type: race\n","Evaluating race-color examples.\n"," 70% 363/516 [02:48<00:44,  3.41it/s]Skipping example 363.\n","100% 515/516 [04:01<00:00,  2.13it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 39.03\n","Stereotype score: 38.56\n","Anti-stereotype score: 44.19\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 39.03\n"]}],"source":["# albert的inlp crows_debias --bias_type race 测试混合 race gender religion matrix\n","\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-racegenderreligion_s-0.pt --bias_type race"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":241,"status":"ok","timestamp":1690854885939,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"D7KOlG1EQSuW","outputId":"a5dd05c8-0164-4995-bd46-fc6c8f4717cf"},"outputs":[{"name":"stdout","output_type":"stream","text":["shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n","shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n","python3: can't open file 'experiments/crows_debias.py': [Errno 107] Transport endpoint is not connected\n"]}],"source":["# albert的inlp crows_debias --bias_type religion 测试混合 race gender religion matrix\n","\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-racegenderreligion_s-0.pt --bias_type religion"]},{"cell_type":"markdown","source":["###  religion race 3000 80"],"metadata":{"id":"JAncqaA6A457"}},{"cell_type":"markdown","metadata":{"id":"6e6lcddkXNOp"},"source":["5、将2个数据集按照religion race链接起来，放进矩阵生成中。 修改context_nullspace_projection.py和inlp_projection_matrix.py\n","3000 80"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1101372,"status":"ok","timestamp":1690903881685,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"B3MrLDOlQS1y","outputId":"47eed711-7bfb-4cdd-c3e4-dcbd1c0f05cc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Computing projection matrix:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: AlbertModel\n"," - model_name_or_path: albert-base-v2\n"," - bias_type: race\n"," - n_classifiers: 80\n"," - seed: 0\n","Loading INLP data:   6% 86518/1372632 [00:08<01:42, 12550.18it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   6% 86562/1372632 [00:08<02:01, 10590.22it/s]\n","Loading INLP data:   8% 109816/1372632 [00:12<01:35, 13283.53it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   8% 110011/1372632 [00:12<02:20, 8995.63it/s] \n","Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.dense.weight', 'predictions.decoder.bias', 'predictions.decoder.weight', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.dense.bias']\n","- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Encoding bias sentences: 100% 3000/3000 [10:22<00:00,  4.82it/s]\n","Encoding neutral sentences: 100% 3000/3000 [07:55<00:00,  6.31it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Encoding bias sentences: 100% 3000/3000 [09:51<00:00,  5.07it/s]\n","Encoding neutral sentences: 100% 3000/3000 [07:51<00:00,  6.36it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Dataset split sizes:\n","Train size: 5880; Dev size: 2520; Test size: 3600\n","iteration: 0, accuracy: 0.8456349206349206:   0% 0/80 [00:06<?, ?it/s]iteration: 0, accuracy: 0.8456349206349206\n","iteration: 1, accuracy: 0.8539682539682539:   1% 1/80 [00:18<09:24,  7.15s/it]iteration: 1, accuracy: 0.8539682539682539\n","iteration: 2, accuracy: 0.8551587301587301:   2% 2/80 [00:25<13:01, 10.02s/it]iteration: 2, accuracy: 0.8551587301587301\n","iteration: 3, accuracy: 0.8492063492063492:   4% 3/80 [00:34<11:21,  8.85s/it]iteration: 3, accuracy: 0.8492063492063492\n","iteration: 4, accuracy: 0.8468253968253968:   5% 4/80 [00:42<11:04,  8.74s/it]iteration: 4, accuracy: 0.8468253968253968\n","iteration: 5, accuracy: 0.8511904761904762:   6% 5/80 [00:49<10:42,  8.56s/it]iteration: 5, accuracy: 0.8511904761904762\n","iteration: 6, accuracy: 0.8408730158730159:   8% 6/80 [00:57<10:08,  8.22s/it]iteration: 6, accuracy: 0.8408730158730159\n","iteration: 7, accuracy: 0.8448412698412698:   9% 7/80 [01:03<09:31,  7.82s/it]iteration: 7, accuracy: 0.8448412698412698\n","iteration: 8, accuracy: 0.8325396825396826:  10% 8/80 [01:11<08:58,  7.48s/it]iteration: 8, accuracy: 0.8325396825396826\n","iteration: 9, accuracy: 0.823015873015873:  11% 9/80 [01:18<08:55,  7.54s/it] iteration: 9, accuracy: 0.823015873015873\n","iteration: 10, accuracy: 0.8067460317460318:  12% 10/80 [01:26<08:46,  7.52s/it]iteration: 10, accuracy: 0.8067460317460318\n","iteration: 11, accuracy: 0.8087301587301587:  14% 11/80 [01:32<08:31,  7.42s/it]iteration: 11, accuracy: 0.8087301587301587\n","iteration: 12, accuracy: 0.7964285714285714:  15% 12/80 [01:39<07:53,  6.96s/it]iteration: 12, accuracy: 0.7964285714285714\n","iteration: 13, accuracy: 0.7928571428571428:  16% 13/80 [01:47<08:09,  7.30s/it]iteration: 13, accuracy: 0.7928571428571428\n","iteration: 14, accuracy: 0.7853174603174603:  18% 14/80 [01:54<07:54,  7.19s/it]iteration: 14, accuracy: 0.7853174603174603\n","iteration: 15, accuracy: 0.7829365079365079:  19% 15/80 [02:02<08:10,  7.55s/it]iteration: 15, accuracy: 0.7829365079365079\n","iteration: 16, accuracy: 0.7738095238095238:  20% 16/80 [02:09<07:56,  7.45s/it]iteration: 16, accuracy: 0.7738095238095238\n","iteration: 17, accuracy: 0.7674603174603175:  21% 17/80 [02:16<07:43,  7.36s/it]iteration: 17, accuracy: 0.7674603174603175\n","iteration: 18, accuracy: 0.7658730158730159:  22% 18/80 [02:23<07:16,  7.05s/it]iteration: 18, accuracy: 0.7658730158730159\n","iteration: 19, accuracy: 0.7496031746031746:  24% 19/80 [02:29<07:10,  7.06s/it]iteration: 19, accuracy: 0.7496031746031746\n","iteration: 20, accuracy: 0.7420634920634921:  25% 20/80 [02:37<07:03,  7.05s/it]iteration: 20, accuracy: 0.7420634920634921\n","iteration: 21, accuracy: 0.7420634920634921:  26% 21/80 [02:45<06:58,  7.09s/it]iteration: 21, accuracy: 0.7420634920634921\n","iteration: 22, accuracy: 0.7345238095238096:  28% 22/80 [02:52<07:12,  7.46s/it]iteration: 22, accuracy: 0.7345238095238096\n","iteration: 23, accuracy: 0.7226190476190476:  29% 23/80 [02:58<06:48,  7.17s/it]iteration: 23, accuracy: 0.7226190476190476\n","iteration: 24, accuracy: 0.7234126984126984:  30% 24/80 [03:05<06:23,  6.85s/it]iteration: 24, accuracy: 0.7234126984126984\n","iteration: 25, accuracy: 0.7103174603174603:  31% 25/80 [03:13<06:22,  6.95s/it]iteration: 25, accuracy: 0.7103174603174603\n","iteration: 26, accuracy: 0.7075396825396826:  32% 26/80 [03:19<06:29,  7.22s/it]iteration: 26, accuracy: 0.7075396825396826\n","iteration: 27, accuracy: 0.7103174603174603:  34% 27/80 [03:27<06:12,  7.04s/it]iteration: 27, accuracy: 0.7103174603174603\n","iteration: 28, accuracy: 0.7083333333333334:  35% 28/80 [03:34<06:06,  7.04s/it]iteration: 28, accuracy: 0.7083333333333334\n","iteration: 29, accuracy: 0.7087301587301588:  36% 29/80 [03:41<06:05,  7.16s/it]iteration: 29, accuracy: 0.7087301587301588\n","iteration: 30, accuracy: 0.6932539682539682:  38% 30/80 [03:48<05:53,  7.07s/it]iteration: 30, accuracy: 0.6932539682539682\n","iteration: 31, accuracy: 0.6976190476190476:  39% 31/80 [03:56<05:44,  7.03s/it]iteration: 31, accuracy: 0.6976190476190476\n","iteration: 32, accuracy: 0.6904761904761905:  40% 32/80 [04:01<05:49,  7.28s/it]iteration: 32, accuracy: 0.6904761904761905\n","iteration: 33, accuracy: 0.6904761904761905:  41% 33/80 [04:08<05:17,  6.76s/it]iteration: 33, accuracy: 0.6904761904761905\n","iteration: 34, accuracy: 0.682936507936508:  42% 34/80 [04:17<05:22,  7.02s/it] iteration: 34, accuracy: 0.682936507936508\n","iteration: 35, accuracy: 0.6781746031746032:  44% 35/80 [04:24<05:29,  7.33s/it]iteration: 35, accuracy: 0.6781746031746032\n","iteration: 36, accuracy: 0.6623015873015873:  45% 36/80 [04:33<05:31,  7.53s/it]iteration: 36, accuracy: 0.6623015873015873\n","iteration: 37, accuracy: 0.6646825396825397:  46% 37/80 [04:39<05:25,  7.56s/it]iteration: 37, accuracy: 0.6646825396825397\n","iteration: 38, accuracy: 0.6579365079365079:  48% 38/80 [04:47<05:03,  7.24s/it]iteration: 38, accuracy: 0.6579365079365079\n","iteration: 39, accuracy: 0.651984126984127:  49% 39/80 [04:54<05:05,  7.45s/it] iteration: 39, accuracy: 0.651984126984127\n","iteration: 40, accuracy: 0.6535714285714286:  50% 40/80 [05:02<04:50,  7.25s/it]iteration: 40, accuracy: 0.6535714285714286\n","iteration: 41, accuracy: 0.6599206349206349:  51% 41/80 [05:09<04:54,  7.56s/it]iteration: 41, accuracy: 0.6599206349206349\n","iteration: 42, accuracy: 0.6615079365079365:  52% 42/80 [05:15<04:40,  7.37s/it]iteration: 42, accuracy: 0.6615079365079365\n","iteration: 43, accuracy: 0.6587301587301587:  54% 43/80 [05:21<04:22,  7.08s/it]iteration: 43, accuracy: 0.6587301587301587\n","iteration: 44, accuracy: 0.6460317460317461:  55% 44/80 [05:27<04:00,  6.67s/it]iteration: 44, accuracy: 0.6460317460317461\n","iteration: 45, accuracy: 0.6440476190476191:  56% 45/80 [05:33<03:46,  6.48s/it]iteration: 45, accuracy: 0.6440476190476191\n","iteration: 46, accuracy: 0.6440476190476191:  57% 46/80 [05:40<03:34,  6.31s/it]iteration: 46, accuracy: 0.6440476190476191\n","iteration: 47, accuracy: 0.6448412698412699:  59% 47/80 [05:45<03:31,  6.42s/it]iteration: 47, accuracy: 0.6448412698412699\n","iteration: 48, accuracy: 0.6440476190476191:  60% 48/80 [05:51<03:18,  6.19s/it]iteration: 48, accuracy: 0.6440476190476191\n","iteration: 49, accuracy: 0.6329365079365079:  61% 49/80 [05:57<03:14,  6.28s/it]iteration: 49, accuracy: 0.6329365079365079\n","iteration: 50, accuracy: 0.6297619047619047:  62% 50/80 [06:02<02:59,  6.00s/it]iteration: 50, accuracy: 0.6297619047619047\n","iteration: 51, accuracy: 0.6373015873015873:  64% 51/80 [06:08<02:45,  5.72s/it]iteration: 51, accuracy: 0.6373015873015873\n","iteration: 52, accuracy: 0.6333333333333333:  65% 52/80 [06:14<02:46,  5.96s/it]iteration: 52, accuracy: 0.6333333333333333\n","iteration: 53, accuracy: 0.6261904761904762:  66% 53/80 [06:20<02:35,  5.78s/it]iteration: 53, accuracy: 0.6261904761904762\n","iteration: 54, accuracy: 0.6373015873015873:  68% 54/80 [06:27<02:36,  6.01s/it]iteration: 54, accuracy: 0.6373015873015873\n","iteration: 55, accuracy: 0.6206349206349207:  69% 55/80 [06:34<02:32,  6.11s/it]iteration: 55, accuracy: 0.6206349206349207\n","iteration: 56, accuracy: 0.6214285714285714:  70% 56/80 [06:39<02:30,  6.25s/it]iteration: 56, accuracy: 0.6214285714285714\n","iteration: 57, accuracy: 0.6162698412698413:  71% 57/80 [06:47<02:25,  6.33s/it]iteration: 57, accuracy: 0.6162698412698413\n","iteration: 58, accuracy: 0.6146825396825397:  72% 58/80 [06:53<02:24,  6.58s/it]iteration: 58, accuracy: 0.6146825396825397\n","iteration: 59, accuracy: 0.6126984126984127:  74% 59/80 [06:59<02:09,  6.18s/it]iteration: 59, accuracy: 0.6126984126984127\n","iteration: 60, accuracy: 0.6091269841269841:  75% 60/80 [07:05<02:09,  6.49s/it]iteration: 60, accuracy: 0.6091269841269841\n","iteration: 61, accuracy: 0.6107142857142858:  76% 61/80 [07:13<01:58,  6.24s/it]iteration: 61, accuracy: 0.6107142857142858\n","iteration: 62, accuracy: 0.6079365079365079:  78% 62/80 [07:20<02:01,  6.75s/it]iteration: 62, accuracy: 0.6079365079365079\n","iteration: 63, accuracy: 0.5956349206349206:  79% 63/80 [07:26<01:53,  6.70s/it]iteration: 63, accuracy: 0.5956349206349206\n","iteration: 64, accuracy: 0.5988095238095238:  80% 64/80 [07:33<01:44,  6.53s/it]iteration: 64, accuracy: 0.5988095238095238\n","iteration: 65, accuracy: 0.6051587301587301:  81% 65/80 [07:40<01:41,  6.78s/it]iteration: 65, accuracy: 0.6051587301587301\n","iteration: 66, accuracy: 0.6027777777777777:  82% 66/80 [07:46<01:32,  6.59s/it]iteration: 66, accuracy: 0.6027777777777777\n","iteration: 67, accuracy: 0.6103174603174604:  84% 67/80 [07:54<01:27,  6.69s/it]iteration: 67, accuracy: 0.6103174603174604\n","iteration: 68, accuracy: 0.6051587301587301:  85% 68/80 [08:01<01:22,  6.86s/it]iteration: 68, accuracy: 0.6051587301587301\n","iteration: 69, accuracy: 0.5976190476190476:  86% 69/80 [08:07<01:16,  6.98s/it]iteration: 69, accuracy: 0.5976190476190476\n","iteration: 70, accuracy: 0.5992063492063492:  88% 70/80 [08:13<01:07,  6.71s/it]iteration: 70, accuracy: 0.5992063492063492\n","iteration: 71, accuracy: 0.5908730158730159:  89% 71/80 [08:18<00:58,  6.46s/it]iteration: 71, accuracy: 0.5908730158730159\n","iteration: 72, accuracy: 0.5805555555555556:  90% 72/80 [08:25<00:49,  6.15s/it]iteration: 72, accuracy: 0.5805555555555556\n","iteration: 73, accuracy: 0.5817460317460318:  91% 73/80 [08:31<00:44,  6.39s/it]iteration: 73, accuracy: 0.5817460317460318\n","iteration: 74, accuracy: 0.5746031746031746:  92% 74/80 [08:37<00:37,  6.18s/it]iteration: 74, accuracy: 0.5746031746031746\n","iteration: 75, accuracy: 0.573015873015873:  94% 75/80 [08:44<00:30,  6.18s/it] iteration: 75, accuracy: 0.573015873015873\n","iteration: 76, accuracy: 0.5678571428571428:  95% 76/80 [08:50<00:25,  6.49s/it]iteration: 76, accuracy: 0.5678571428571428\n","iteration: 77, accuracy: 0.5630952380952381:  96% 77/80 [08:55<00:18,  6.19s/it]iteration: 77, accuracy: 0.5630952380952381\n","iteration: 78, accuracy: 0.5734126984126984:  98% 78/80 [09:01<00:12,  6.09s/it]iteration: 78, accuracy: 0.5734126984126984\n","iteration: 79, accuracy: 0.5603174603174603:  99% 79/80 [09:07<00:05,  5.89s/it]iteration: 79, accuracy: 0.5603174603174603\n","iteration: 79, accuracy: 0.5603174603174603: 100% 80/80 [09:08<00:00,  6.86s/it]\n","Saving computed projection matrix to: /content/drive/MyDrive/bias-bench-main/results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-race_s-0.pt\n"]}],"source":["# albert的inlp matrix生成 --bias_type religion race 3000*2 80\n","\n","! python experiments/inlp_projection_matrix.py --model AlbertModel --model_name_or_path albert-base-v2 --bias_type race --n_classifiers 80"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14869,"status":"ok","timestamp":1690904167468,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"wA8EzOT6XLxQ","outputId":"e8bbd6e4-1eec-40f4-cfa1-5dca034aef16"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religionrace3000_s-0.pt\n"," - load_path: None\n"," - bias_type: gender\n","Evaluating gender examples.\n","100% 262/262 [03:31<00:00,  1.24it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 44.66\n","Stereotype score: 55.35\n","Anti-stereotype score: 28.16\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 44.66\n"]}],"source":["# albert的inlp crows_debias --bias_type gender 测试混合 religion race matrix 3000 80\n","\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religionrace3000_s-0.pt --bias_type gender"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":176072,"status":"ok","timestamp":1690904431120,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"gRo6MnEUXL0c","outputId":"6cca4018-20f1-490f-ea50-fdf95b7e04f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religionrace3000_s-0.pt\n"," - load_path: None\n"," - bias_type: race\n","Evaluating race-color examples.\n"," 70% 363/516 [02:59<01:03,  2.39it/s]Skipping example 363.\n","100% 515/516 [04:16<00:00,  2.01it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 52.43\n","Stereotype score: 52.54\n","Anti-stereotype score: 51.16\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 52.43\n"]}],"source":["# albert的inlp crows_debias --bias_type race 测试混合 religion race matrix 3000 80\n","\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religionrace3000_s-0.pt --bias_type race"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":52926,"status":"ok","timestamp":1690904484045,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"LEeVdZigXL4H","outputId":"c55f2b6b-a773-4a77-dadf-2696bfdca66c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religionrace3000_s-0.pt\n"," - load_path: None\n"," - bias_type: religion\n","Evaluating religion examples.\n","100% 105/105 [00:46<00:00,  2.28it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 66.67\n","Stereotype score: 66.67\n","Anti-stereotype score: 66.67\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 66.67\n"]}],"source":["# albert的inlp crows_debias --bias_type religion 测试混合 religion race matrix 3000 80\n","\n","! python experiments/crows_debias.py --model INLPAlbertForMaskedLM --model_name_or_path albert-base-v2 --projection_matrix ./results/projection_matrix/projection_m-AlbertModel_c-albert-base-v2_t-religionrace3000_s-0.pt --bias_type religion"]},{"cell_type":"markdown","source":["## BERT INLP"],"metadata":{"id":"bM1xpJLDBWTW"}},{"cell_type":"markdown","source":["### bert mix"],"metadata":{"id":"wiXaPTWEY1Tm"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2861908,"status":"ok","timestamp":1691325486279,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"Kjs9IObHOS5t","outputId":"c4332bb7-4443-42bb-f7fa-04d646c0752d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Computing projection matrix:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: BertModel\n"," - model_name_or_path: bert-base-uncased\n"," - bias_type: race\n"," - n_classifiers: 80\n"," - seed: 0\n","Loading INLP data:   2% 25546/1372632 [00:01<01:16, 17503.58it/s]INLP dataset collected:\n"," - Num. male sentences: 3000\n"," - Num. female sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   2% 27135/1372632 [00:01<01:20, 16649.96it/s]\n","Loading INLP data:   6% 85429/1372632 [00:04<01:26, 14907.31it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   6% 86562/1372632 [00:04<01:09, 18533.03it/s]\n","Loading INLP data:   8% 109374/1372632 [00:06<01:05, 19431.59it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   8% 110011/1372632 [00:06<01:14, 17029.88it/s]\n","Downloading: 100% 570/570 [00:00<00:00, 4.32MB/s]\n","Downloading: 100% 420M/420M [00:08<00:00, 54.1MB/s]\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Downloading: 100% 28.0/28.0 [00:00<00:00, 191kB/s]\n","Downloading: 100% 226k/226k [00:00<00:00, 3.90MB/s]\n","Downloading: 100% 455k/455k [00:00<00:00, 10.2MB/s]\n","Encoding male sentences: 100% 3000/3000 [04:59<00:00, 10.01it/s]\n","Encoding female sentences: 100% 3000/3000 [04:56<00:00, 10.10it/s]\n","Encoding neutral sentences: 100% 3000/3000 [04:22<00:00, 11.44it/s]\n","Dataset split sizes:\n","Train size: 4410; Dev size: 1890; Test size: 2700\n","Encoding bias sentences: 100% 3000/3000 [05:36<00:00,  8.92it/s]\n","Encoding neutral sentences: 100% 3000/3000 [04:26<00:00, 11.27it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Encoding bias sentences: 100% 3000/3000 [05:29<00:00,  9.11it/s]\n","Encoding neutral sentences: 100% 3000/3000 [04:28<00:00, 11.17it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Dataset split sizes:\n","Train size: 10290; Dev size: 4410; Test size: 6300\n","iteration: 0, accuracy: 0.6802721088435374:   0% 0/80 [00:11<?, ?it/s]iteration: 0, accuracy: 0.6802721088435374\n","iteration: 1, accuracy: 0.6852607709750567:   1% 1/80 [00:23<16:18, 12.38s/it]iteration: 1, accuracy: 0.6852607709750567\n","iteration: 2, accuracy: 0.6746031746031746:   2% 2/80 [00:35<15:55, 12.26s/it]iteration: 2, accuracy: 0.6746031746031746\n","iteration: 3, accuracy: 0.6659863945578232:   4% 3/80 [00:46<15:03, 11.74s/it]iteration: 3, accuracy: 0.6659863945578232\n","iteration: 4, accuracy: 0.6566893424036281:   5% 4/80 [00:59<15:06, 11.92s/it]iteration: 4, accuracy: 0.6566893424036281\n","iteration: 5, accuracy: 0.6335600907029478:   6% 5/80 [01:10<15:00, 12.00s/it]iteration: 5, accuracy: 0.6335600907029478\n","iteration: 6, accuracy: 0.6197278911564625:   8% 6/80 [01:21<14:22, 11.65s/it]iteration: 6, accuracy: 0.6197278911564625\n","iteration: 7, accuracy: 0.6099773242630385:   9% 7/80 [01:33<13:58, 11.49s/it]iteration: 7, accuracy: 0.6099773242630385\n","iteration: 8, accuracy: 0.6018140589569161:  10% 8/80 [01:44<13:59, 11.66s/it]iteration: 8, accuracy: 0.6018140589569161\n","iteration: 9, accuracy: 0.5807256235827665:  11% 9/80 [01:56<13:32, 11.44s/it]iteration: 9, accuracy: 0.5807256235827665\n","iteration: 10, accuracy: 0.5789115646258504:  12% 10/80 [02:07<13:23, 11.48s/it]iteration: 10, accuracy: 0.5789115646258504\n","iteration: 11, accuracy: 0.5619047619047619:  14% 11/80 [02:18<13:05, 11.38s/it]iteration: 11, accuracy: 0.5619047619047619\n","iteration: 12, accuracy: 0.5448979591836735:  15% 12/80 [02:29<12:53, 11.37s/it]iteration: 12, accuracy: 0.5448979591836735\n","iteration: 13, accuracy: 0.5426303854875284:  16% 13/80 [02:39<12:42, 11.39s/it]iteration: 13, accuracy: 0.5426303854875284\n","iteration: 14, accuracy: 0.5260770975056689:  18% 14/80 [02:49<12:00, 10.91s/it]iteration: 14, accuracy: 0.5260770975056689\n","iteration: 15, accuracy: 0.5265306122448979:  19% 15/80 [02:59<11:34, 10.68s/it]iteration: 15, accuracy: 0.5265306122448979\n","iteration: 16, accuracy: 0.5179138321995465:  20% 16/80 [03:10<11:16, 10.56s/it]iteration: 16, accuracy: 0.5179138321995465\n","iteration: 17, accuracy: 0.5192743764172335:  21% 17/80 [03:19<10:52, 10.36s/it]iteration: 17, accuracy: 0.5192743764172335\n","iteration: 18, accuracy: 0.5122448979591837:  22% 18/80 [03:28<10:20, 10.01s/it]iteration: 18, accuracy: 0.5122448979591837\n","iteration: 19, accuracy: 0.509750566893424:  24% 19/80 [03:38<10:03,  9.89s/it] iteration: 19, accuracy: 0.509750566893424\n","iteration: 20, accuracy: 0.4997732426303855:  25% 20/80 [03:48<09:44,  9.74s/it]iteration: 20, accuracy: 0.4997732426303855\n","iteration: 21, accuracy: 0.4954648526077097:  26% 21/80 [03:58<09:39,  9.83s/it]iteration: 21, accuracy: 0.4954648526077097\n","iteration: 22, accuracy: 0.4875283446712018:  28% 22/80 [04:07<09:35,  9.92s/it]iteration: 22, accuracy: 0.4875283446712018\n","iteration: 23, accuracy: 0.4886621315192744:  29% 23/80 [04:17<09:16,  9.75s/it]iteration: 23, accuracy: 0.4886621315192744\n","iteration: 24, accuracy: 0.4804988662131519:  30% 24/80 [04:26<09:07,  9.78s/it]iteration: 24, accuracy: 0.4804988662131519\n","iteration: 25, accuracy: 0.4743764172335601:  31% 25/80 [04:36<08:48,  9.61s/it]iteration: 25, accuracy: 0.4743764172335601\n","iteration: 26, accuracy: 0.47619047619047616:  32% 26/80 [04:45<08:32,  9.50s/it]iteration: 26, accuracy: 0.47619047619047616\n","iteration: 27, accuracy: 0.47505668934240364:  34% 27/80 [04:54<08:22,  9.48s/it]iteration: 27, accuracy: 0.47505668934240364\n","iteration: 28, accuracy: 0.4743764172335601:  35% 28/80 [05:03<08:05,  9.33s/it] iteration: 28, accuracy: 0.4743764172335601\n","iteration: 29, accuracy: 0.4657596371882086:  36% 29/80 [05:13<07:51,  9.24s/it]iteration: 29, accuracy: 0.4657596371882086\n","iteration: 30, accuracy: 0.45691609977324266:  38% 30/80 [05:23<07:52,  9.46s/it]iteration: 30, accuracy: 0.45691609977324266\n","iteration: 31, accuracy: 0.445124716553288:  39% 31/80 [05:34<07:58,  9.76s/it]  iteration: 31, accuracy: 0.445124716553288\n","iteration: 32, accuracy: 0.43900226757369615:  40% 32/80 [05:44<07:56,  9.93s/it]iteration: 32, accuracy: 0.43900226757369615\n","iteration: 33, accuracy: 0.43673469387755104:  41% 33/80 [05:53<07:47,  9.95s/it]iteration: 33, accuracy: 0.43673469387755104\n","iteration: 34, accuracy: 0.43900226757369615:  42% 34/80 [06:03<07:37,  9.95s/it]iteration: 34, accuracy: 0.43900226757369615\n","iteration: 35, accuracy: 0.43786848072562357:  44% 35/80 [06:13<07:23,  9.85s/it]iteration: 35, accuracy: 0.43786848072562357\n","iteration: 36, accuracy: 0.4301587301587302:  45% 36/80 [06:23<07:12,  9.83s/it] iteration: 36, accuracy: 0.4301587301587302\n","iteration: 37, accuracy: 0.43560090702947846:  46% 37/80 [06:33<07:01,  9.80s/it]iteration: 37, accuracy: 0.43560090702947846\n","iteration: 38, accuracy: 0.4290249433106576:  48% 38/80 [06:43<07:00, 10.01s/it] iteration: 38, accuracy: 0.4290249433106576\n","iteration: 39, accuracy: 0.4204081632653061:  49% 39/80 [06:53<06:44,  9.87s/it]iteration: 39, accuracy: 0.4204081632653061\n","iteration: 40, accuracy: 0.4174603174603175:  50% 40/80 [07:02<06:37,  9.94s/it]iteration: 40, accuracy: 0.4174603174603175\n","iteration: 41, accuracy: 0.39909297052154197:  51% 41/80 [07:12<06:16,  9.67s/it]iteration: 41, accuracy: 0.39909297052154197\n","iteration: 42, accuracy: 0.3961451247165533:  52% 42/80 [07:20<06:09,  9.72s/it] iteration: 42, accuracy: 0.3961451247165533\n","iteration: 43, accuracy: 0.3981859410430839:  54% 43/80 [07:29<05:44,  9.30s/it]iteration: 43, accuracy: 0.3981859410430839\n","iteration: 44, accuracy: 0.3956916099773243:  55% 44/80 [07:37<05:31,  9.20s/it]iteration: 44, accuracy: 0.3956916099773243\n","iteration: 45, accuracy: 0.38571428571428573:  56% 45/80 [07:47<05:11,  8.91s/it]iteration: 45, accuracy: 0.38571428571428573\n","iteration: 46, accuracy: 0.38049886621315193:  57% 46/80 [07:55<05:07,  9.03s/it]iteration: 46, accuracy: 0.38049886621315193\n","iteration: 47, accuracy: 0.38095238095238093:  59% 47/80 [08:04<04:57,  9.01s/it]iteration: 47, accuracy: 0.38095238095238093\n","iteration: 48, accuracy: 0.3750566893424036:  60% 48/80 [08:13<04:41,  8.78s/it] iteration: 48, accuracy: 0.3750566893424036\n","iteration: 49, accuracy: 0.3743764172335601:  61% 49/80 [08:21<04:34,  8.85s/it]iteration: 49, accuracy: 0.3743764172335601\n","iteration: 50, accuracy: 0.37029478458049886:  62% 50/80 [08:31<04:23,  8.79s/it]iteration: 50, accuracy: 0.37029478458049886\n","iteration: 51, accuracy: 0.36666666666666664:  64% 51/80 [08:40<04:19,  8.95s/it]iteration: 51, accuracy: 0.36666666666666664\n","iteration: 52, accuracy: 0.3625850340136054:  65% 52/80 [08:48<04:13,  9.04s/it] iteration: 52, accuracy: 0.3625850340136054\n","iteration: 53, accuracy: 0.3505668934240363:  66% 53/80 [08:57<03:58,  8.84s/it]iteration: 53, accuracy: 0.3505668934240363\n","iteration: 54, accuracy: 0.35827664399092973:  68% 54/80 [09:06<03:49,  8.82s/it]iteration: 54, accuracy: 0.35827664399092973\n","iteration: 55, accuracy: 0.35079365079365077:  69% 55/80 [09:15<03:41,  8.84s/it]iteration: 55, accuracy: 0.35079365079365077\n","iteration: 56, accuracy: 0.3492063492063492:  70% 56/80 [09:24<03:30,  8.79s/it] iteration: 56, accuracy: 0.3492063492063492\n","iteration: 57, accuracy: 0.33945578231292517:  71% 57/80 [09:32<03:23,  8.84s/it]iteration: 57, accuracy: 0.33945578231292517\n","iteration: 58, accuracy: 0.3326530612244898:  72% 58/80 [09:41<03:11,  8.71s/it] iteration: 58, accuracy: 0.3326530612244898\n","iteration: 59, accuracy: 0.3247165532879819:  74% 59/80 [09:50<03:05,  8.82s/it]iteration: 59, accuracy: 0.3247165532879819\n","iteration: 60, accuracy: 0.3224489795918367:  75% 60/80 [09:59<02:55,  8.76s/it]iteration: 60, accuracy: 0.3224489795918367\n","iteration: 61, accuracy: 0.31723356009070297:  76% 61/80 [10:08<02:47,  8.80s/it]iteration: 61, accuracy: 0.31723356009070297\n","iteration: 62, accuracy: 0.3090702947845805:  78% 62/80 [10:16<02:40,  8.90s/it] iteration: 62, accuracy: 0.3090702947845805\n","iteration: 63, accuracy: 0.3126984126984127:  79% 63/80 [10:25<02:29,  8.80s/it]iteration: 63, accuracy: 0.3126984126984127\n","iteration: 64, accuracy: 0.3197278911564626:  80% 64/80 [10:34<02:19,  8.74s/it]iteration: 64, accuracy: 0.3197278911564626\n","iteration: 65, accuracy: 0.31768707482993197:  81% 65/80 [10:42<02:10,  8.69s/it]iteration: 65, accuracy: 0.31768707482993197\n","iteration: 66, accuracy: 0.3126984126984127:  82% 66/80 [10:51<02:02,  8.72s/it] iteration: 66, accuracy: 0.3126984126984127\n","iteration: 67, accuracy: 0.31065759637188206:  84% 67/80 [11:00<01:53,  8.75s/it]iteration: 67, accuracy: 0.31065759637188206\n","iteration: 68, accuracy: 0.3165532879818594:  85% 68/80 [11:08<01:43,  8.65s/it] iteration: 68, accuracy: 0.3165532879818594\n","iteration: 69, accuracy: 0.3111111111111111:  86% 69/80 [11:17<01:35,  8.70s/it]iteration: 69, accuracy: 0.3111111111111111\n","iteration: 70, accuracy: 0.31020408163265306:  88% 70/80 [11:25<01:25,  8.57s/it]iteration: 70, accuracy: 0.31020408163265306\n","iteration: 71, accuracy: 0.31519274376417233:  89% 71/80 [11:34<01:17,  8.61s/it]iteration: 71, accuracy: 0.31519274376417233\n","iteration: 72, accuracy: 0.32154195011337866:  90% 72/80 [11:43<01:09,  8.64s/it]iteration: 72, accuracy: 0.32154195011337866\n","iteration: 73, accuracy: 0.3321995464852608:  91% 73/80 [11:52<01:00,  8.69s/it] iteration: 73, accuracy: 0.3321995464852608\n","iteration: 74, accuracy: 0.31882086167800455:  92% 74/80 [12:00<00:52,  8.74s/it]iteration: 74, accuracy: 0.31882086167800455\n","iteration: 75, accuracy: 0.32448979591836735:  94% 75/80 [12:08<00:42,  8.51s/it]iteration: 75, accuracy: 0.32448979591836735\n","iteration: 76, accuracy: 0.3201814058956916:  95% 76/80 [12:16<00:33,  8.50s/it] iteration: 76, accuracy: 0.3201814058956916\n","iteration: 77, accuracy: 0.3240362811791383:  96% 77/80 [12:25<00:25,  8.38s/it]iteration: 77, accuracy: 0.3240362811791383\n","iteration: 78, accuracy: 0.3247165532879819:  98% 78/80 [12:33<00:16,  8.42s/it]iteration: 78, accuracy: 0.3247165532879819\n","iteration: 79, accuracy: 0.3183673469387755:  99% 79/80 [12:41<00:08,  8.37s/it]iteration: 79, accuracy: 0.3183673469387755\n","iteration: 79, accuracy: 0.3183673469387755: 100% 80/80 [12:42<00:00,  9.53s/it]\n","Saving computed projection matrix to: /content/drive/MyDrive/bias-bench-main/results/projection_matrix/projection_m-BertModel_c-bert-base-uncased_t-race_s-0.pt\n"]}],"source":["# bert inlp matrix --bias_type genderracereligion mix 3000*3 80\n","\n","! python experiments/inlp_projection_matrix.py --model BertModel --model_name_or_path bert-base-uncased --bias_type race --n_classifiers 80"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":133035,"status":"ok","timestamp":1691332480213,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"9TmHWWl-rToR","outputId":"69211214-abff-489c-b2b5-92c69cafcfa0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPBertForMaskedLM\n"," - model_name_or_path: bert-base-uncased\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-BertModel_c-bert-base-uncased_t-mix3000_s-0.pt\n"," - load_path: None\n"," - bias_type: gender\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Evaluating gender examples.\n","100% 262/262 [02:03<00:00,  2.13it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 53.05\n","Stereotype score: 43.4\n","Anti-stereotype score: 67.96\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 53.05\n"]}],"source":["\n","! python experiments/crows_debias.py --model INLPBertForMaskedLM --model_name_or_path bert-base-uncased --projection_matrix ./results/projection_matrix/projection_m-BertModel_c-bert-base-uncased_t-mix3000_s-0.pt --bias_type gender"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":189410,"status":"ok","timestamp":1691332669620,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"wYX3ZlpzrTsB","outputId":"86a52d53-9a32-460d-8346-e6e6791dfafb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPBertForMaskedLM\n"," - model_name_or_path: bert-base-uncased\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-BertModel_c-bert-base-uncased_t-mix3000_s-0.pt\n"," - load_path: None\n"," - bias_type: race\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Evaluating race-color examples.\n"," 70% 363/516 [02:10<00:38,  3.99it/s]Skipping example 363.\n","100% 515/516 [03:03<00:00,  2.81it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 55.15\n","Stereotype score: 56.14\n","Anti-stereotype score: 44.19\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 55.15\n"]}],"source":["\n","! python experiments/crows_debias.py --model INLPBertForMaskedLM --model_name_or_path bert-base-uncased --projection_matrix ./results/projection_matrix/projection_m-BertModel_c-bert-base-uncased_t-mix3000_s-0.pt --bias_type race"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37955,"status":"ok","timestamp":1691332813295,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"ojuIH5eUOS8h","outputId":"db900fd1-7b95-445c-a9ac-c5a9de7e16d4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPBertForMaskedLM\n"," - model_name_or_path: bert-base-uncased\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-BertModel_c-bert-base-uncased_t-mix3000_s-0.pt\n"," - load_path: None\n"," - bias_type: religion\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Evaluating religion examples.\n","100% 105/105 [00:31<00:00,  3.36it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 51.43\n","Stereotype score: 49.49\n","Anti-stereotype score: 83.33\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 51.43\n"]}],"source":["\n","! python experiments/crows_debias.py --model INLPBertForMaskedLM --model_name_or_path bert-base-uncased --projection_matrix ./results/projection_matrix/projection_m-BertModel_c-bert-base-uncased_t-mix3000_s-0.pt --bias_type religion"]},{"cell_type":"markdown","metadata":{"id":"cQApSU3OEbCE"},"source":["### bert 132"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2865720,"status":"ok","timestamp":1691332102066,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"X2Yd9qeTOTGs","outputId":"5e22f555-cc6d-4735-8195-dab6ffec135e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Computing projection matrix:\n"," - persistent_dir: /content/drive/My Drive/bias-bench-main\n"," - model: BertModel\n"," - model_name_or_path: bert-base-uncased\n"," - bias_type: race\n"," - n_classifiers: 80\n"," - seed: 0\n","Loading INLP data:   2% 26223/1372632 [00:02<02:23, 9356.22it/s]INLP dataset collected:\n"," - Num. male sentences: 3000\n"," - Num. female sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   2% 27135/1372632 [00:02<02:13, 10043.31it/s]\n","Loading INLP data:   6% 86477/1372632 [00:04<01:10, 18355.20it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   6% 86562/1372632 [00:04<01:09, 18400.23it/s]\n","Loading INLP data:   8% 109967/1372632 [00:07<01:08, 18356.82it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   8% 110011/1372632 [00:07<01:30, 13983.10it/s]\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Encoding male sentences: 100% 3000/3000 [05:01<00:00,  9.95it/s]\n","Encoding female sentences: 100% 3000/3000 [05:00<00:00,  9.99it/s]\n","Encoding neutral sentences: 100% 3000/3000 [04:25<00:00, 11.28it/s]\n","Dataset split sizes:\n","Train size: 4410; Dev size: 1890; Test size: 2700\n","Encoding bias sentences: 100% 3000/3000 [05:41<00:00,  8.78it/s]\n","Encoding neutral sentences: 100% 3000/3000 [04:30<00:00, 11.07it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Encoding bias sentences: 100% 3000/3000 [05:30<00:00,  9.09it/s]\n","Encoding neutral sentences: 100% 3000/3000 [04:28<00:00, 11.17it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Dataset split sizes:\n","Train size: 10290; Dev size: 4410; Test size: 6300\n","iteration: 0, accuracy: 0.6802721088435374:   0% 0/80 [00:11<?, ?it/s]iteration: 0, accuracy: 0.6802721088435374\n","iteration: 1, accuracy: 0.6854875283446712:   1% 1/80 [00:22<15:57, 12.12s/it]iteration: 1, accuracy: 0.6854875283446712\n","iteration: 2, accuracy: 0.6755102040816326:   2% 2/80 [00:32<14:56, 11.50s/it]iteration: 2, accuracy: 0.6755102040816326\n","iteration: 3, accuracy: 0.6668934240362812:   4% 3/80 [00:43<14:17, 11.14s/it]iteration: 3, accuracy: 0.6668934240362812\n","iteration: 4, accuracy: 0.6569160997732426:   5% 4/80 [00:56<13:41, 10.80s/it]iteration: 4, accuracy: 0.6569160997732426\n","iteration: 5, accuracy: 0.6353741496598639:   6% 5/80 [01:07<14:24, 11.53s/it]iteration: 5, accuracy: 0.6353741496598639\n","iteration: 6, accuracy: 0.6197278911564625:   8% 6/80 [01:17<14:01, 11.37s/it]iteration: 6, accuracy: 0.6197278911564625\n","iteration: 7, accuracy: 0.6108843537414966:   9% 7/80 [01:28<13:16, 10.92s/it]iteration: 7, accuracy: 0.6108843537414966\n","iteration: 8, accuracy: 0.6020408163265306:  10% 8/80 [01:39<13:07, 10.94s/it]iteration: 8, accuracy: 0.6020408163265306\n","iteration: 9, accuracy: 0.580952380952381:  11% 9/80 [01:50<12:58, 10.97s/it] iteration: 9, accuracy: 0.580952380952381\n","iteration: 10, accuracy: 0.5791383219954649:  12% 10/80 [02:01<12:55, 11.08s/it]iteration: 10, accuracy: 0.5791383219954649\n","iteration: 11, accuracy: 0.5628117913832199:  14% 11/80 [02:12<12:33, 10.92s/it]iteration: 11, accuracy: 0.5628117913832199\n","iteration: 12, accuracy: 0.546031746031746:  15% 12/80 [02:23<12:24, 10.95s/it] iteration: 12, accuracy: 0.546031746031746\n","iteration: 13, accuracy: 0.5430839002267573:  16% 13/80 [02:33<12:15, 10.97s/it]iteration: 13, accuracy: 0.5430839002267573\n","iteration: 14, accuracy: 0.5251700680272109:  18% 14/80 [02:44<11:57, 10.87s/it]iteration: 14, accuracy: 0.5251700680272109\n","iteration: 15, accuracy: 0.5267573696145125:  19% 15/80 [02:54<11:33, 10.68s/it]iteration: 15, accuracy: 0.5267573696145125\n","iteration: 16, accuracy: 0.5165532879818594:  20% 16/80 [03:04<11:12, 10.51s/it]iteration: 16, accuracy: 0.5165532879818594\n","iteration: 17, accuracy: 0.5208616780045352:  21% 17/80 [03:13<11:02, 10.51s/it]iteration: 17, accuracy: 0.5208616780045352\n","iteration: 18, accuracy: 0.5124716553287982:  22% 18/80 [03:22<10:16,  9.94s/it]iteration: 18, accuracy: 0.5124716553287982\n","iteration: 19, accuracy: 0.5099773242630385:  24% 19/80 [03:31<09:57,  9.80s/it]iteration: 19, accuracy: 0.5099773242630385\n","iteration: 20, accuracy: 0.4988662131519274:  25% 20/80 [03:40<09:37,  9.63s/it]iteration: 20, accuracy: 0.4988662131519274\n","iteration: 21, accuracy: 0.49455782312925173:  26% 21/80 [03:50<09:11,  9.36s/it]iteration: 21, accuracy: 0.49455782312925173\n","iteration: 22, accuracy: 0.4875283446712018:  28% 22/80 [04:00<09:09,  9.48s/it] iteration: 22, accuracy: 0.4875283446712018\n","iteration: 23, accuracy: 0.4879818594104308:  29% 23/80 [04:09<09:06,  9.58s/it]iteration: 23, accuracy: 0.4879818594104308\n","iteration: 24, accuracy: 0.4800453514739229:  30% 24/80 [04:19<08:51,  9.48s/it]iteration: 24, accuracy: 0.4800453514739229\n","iteration: 25, accuracy: 0.4741496598639456:  31% 25/80 [04:27<08:41,  9.48s/it]iteration: 25, accuracy: 0.4741496598639456\n","iteration: 26, accuracy: 0.4748299319727891:  32% 26/80 [04:37<08:20,  9.27s/it]iteration: 26, accuracy: 0.4748299319727891\n","iteration: 27, accuracy: 0.4743764172335601:  34% 27/80 [04:46<08:14,  9.32s/it]iteration: 27, accuracy: 0.4743764172335601\n","iteration: 28, accuracy: 0.47346938775510206:  35% 28/80 [04:55<08:04,  9.32s/it]iteration: 28, accuracy: 0.47346938775510206\n","iteration: 29, accuracy: 0.4662131519274376:  36% 29/80 [05:05<07:45,  9.12s/it] iteration: 29, accuracy: 0.4662131519274376\n","iteration: 30, accuracy: 0.45691609977324266:  38% 30/80 [05:15<07:46,  9.32s/it]iteration: 30, accuracy: 0.45691609977324266\n","iteration: 31, accuracy: 0.445124716553288:  39% 31/80 [05:25<07:56,  9.72s/it]  iteration: 31, accuracy: 0.445124716553288\n","iteration: 32, accuracy: 0.4392290249433107:  40% 32/80 [05:35<07:45,  9.70s/it]iteration: 32, accuracy: 0.4392290249433107\n","iteration: 33, accuracy: 0.4365079365079365:  41% 33/80 [05:44<07:38,  9.75s/it]iteration: 33, accuracy: 0.4365079365079365\n","iteration: 34, accuracy: 0.4403628117913832:  42% 34/80 [05:54<07:21,  9.60s/it]iteration: 34, accuracy: 0.4403628117913832\n","iteration: 35, accuracy: 0.4385487528344671:  44% 35/80 [06:04<07:16,  9.69s/it]iteration: 35, accuracy: 0.4385487528344671\n","iteration: 36, accuracy: 0.4312925170068027:  45% 36/80 [06:14<07:10,  9.79s/it]iteration: 36, accuracy: 0.4312925170068027\n","iteration: 37, accuracy: 0.435827664399093:  46% 37/80 [06:23<07:01,  9.81s/it] iteration: 37, accuracy: 0.435827664399093\n","iteration: 38, accuracy: 0.4290249433106576:  48% 38/80 [06:33<06:45,  9.67s/it]iteration: 38, accuracy: 0.4290249433106576\n","iteration: 39, accuracy: 0.4217687074829932:  49% 39/80 [06:42<06:33,  9.59s/it]iteration: 39, accuracy: 0.4217687074829932\n","iteration: 40, accuracy: 0.41723356009070295:  50% 40/80 [06:51<06:20,  9.50s/it]iteration: 40, accuracy: 0.41723356009070295\n","iteration: 41, accuracy: 0.4:  51% 41/80 [06:59<06:02,  9.29s/it]                iteration: 41, accuracy: 0.4\n","iteration: 42, accuracy: 0.39591836734693875:  52% 42/80 [07:09<05:51,  9.25s/it]iteration: 42, accuracy: 0.39591836734693875\n","iteration: 43, accuracy: 0.3956916099773243:  54% 43/80 [07:19<05:40,  9.21s/it] iteration: 43, accuracy: 0.3956916099773243\n","iteration: 44, accuracy: 0.3968253968253968:  55% 44/80 [07:26<05:35,  9.32s/it]iteration: 44, accuracy: 0.3968253968253968\n","iteration: 45, accuracy: 0.38458049886621315:  56% 45/80 [07:35<05:12,  8.93s/it]iteration: 45, accuracy: 0.38458049886621315\n","iteration: 46, accuracy: 0.3816326530612245:  57% 46/80 [07:43<04:58,  8.78s/it] iteration: 46, accuracy: 0.3816326530612245\n","iteration: 47, accuracy: 0.3798185941043084:  59% 47/80 [07:52<04:48,  8.73s/it]iteration: 47, accuracy: 0.3798185941043084\n","iteration: 48, accuracy: 0.37732426303854877:  60% 48/80 [08:01<04:36,  8.64s/it]iteration: 48, accuracy: 0.37732426303854877\n","iteration: 49, accuracy: 0.373015873015873:  61% 49/80 [08:09<04:30,  8.73s/it]  iteration: 49, accuracy: 0.373015873015873\n","iteration: 50, accuracy: 0.37256235827664397:  62% 50/80 [08:18<04:16,  8.57s/it]iteration: 50, accuracy: 0.37256235827664397\n","iteration: 51, accuracy: 0.3648526077097506:  64% 51/80 [08:27<04:12,  8.70s/it] iteration: 51, accuracy: 0.3648526077097506\n","iteration: 52, accuracy: 0.36122448979591837:  65% 52/80 [08:35<04:04,  8.72s/it]iteration: 52, accuracy: 0.36122448979591837\n","iteration: 53, accuracy: 0.3505668934240363:  66% 53/80 [08:44<03:53,  8.66s/it] iteration: 53, accuracy: 0.3505668934240363\n","iteration: 54, accuracy: 0.3578231292517007:  68% 54/80 [08:53<03:46,  8.72s/it]iteration: 54, accuracy: 0.3578231292517007\n","iteration: 55, accuracy: 0.3476190476190476:  69% 55/80 [09:01<03:34,  8.57s/it]iteration: 55, accuracy: 0.3476190476190476\n","iteration: 56, accuracy: 0.345578231292517:  70% 56/80 [09:10<03:26,  8.62s/it] iteration: 56, accuracy: 0.345578231292517\n","iteration: 57, accuracy: 0.3349206349206349:  71% 57/80 [09:19<03:18,  8.62s/it]iteration: 57, accuracy: 0.3349206349206349\n","iteration: 58, accuracy: 0.33242630385487526:  72% 58/80 [09:27<03:09,  8.63s/it]iteration: 58, accuracy: 0.33242630385487526\n","iteration: 59, accuracy: 0.3276643990929705:  74% 59/80 [09:36<03:02,  8.70s/it] iteration: 59, accuracy: 0.3276643990929705\n","iteration: 60, accuracy: 0.3258503401360544:  75% 60/80 [09:44<02:52,  8.60s/it]iteration: 60, accuracy: 0.3258503401360544\n","iteration: 61, accuracy: 0.3195011337868481:  76% 61/80 [09:52<02:43,  8.61s/it]iteration: 61, accuracy: 0.3195011337868481\n","iteration: 62, accuracy: 0.31065759637188206:  78% 62/80 [10:01<02:31,  8.44s/it]iteration: 62, accuracy: 0.31065759637188206\n","iteration: 63, accuracy: 0.3111111111111111:  79% 63/80 [10:10<02:25,  8.58s/it] iteration: 63, accuracy: 0.3111111111111111\n","iteration: 64, accuracy: 0.3197278911564626:  80% 64/80 [10:19<02:17,  8.61s/it]iteration: 64, accuracy: 0.3197278911564626\n","iteration: 65, accuracy: 0.3167800453514739:  81% 65/80 [10:27<02:08,  8.59s/it]iteration: 65, accuracy: 0.3167800453514739\n","iteration: 66, accuracy: 0.31020408163265306:  82% 66/80 [10:36<02:01,  8.69s/it]iteration: 66, accuracy: 0.31020408163265306\n","iteration: 67, accuracy: 0.30952380952380953:  84% 67/80 [10:44<01:51,  8.55s/it]iteration: 67, accuracy: 0.30952380952380953\n","iteration: 68, accuracy: 0.3197278911564626:  85% 68/80 [10:52<01:43,  8.61s/it] iteration: 68, accuracy: 0.3197278911564626\n","iteration: 69, accuracy: 0.31179138321995464:  86% 69/80 [11:01<01:33,  8.51s/it]iteration: 69, accuracy: 0.31179138321995464\n","iteration: 70, accuracy: 0.3072562358276644:  88% 70/80 [11:10<01:26,  8.61s/it] iteration: 70, accuracy: 0.3072562358276644\n","iteration: 71, accuracy: 0.3167800453514739:  89% 71/80 [11:19<01:18,  8.67s/it]iteration: 71, accuracy: 0.3167800453514739\n","iteration: 72, accuracy: 0.3183673469387755:  90% 72/80 [11:28<01:08,  8.57s/it]iteration: 72, accuracy: 0.3183673469387755\n","iteration: 73, accuracy: 0.3340136054421769:  91% 73/80 [11:36<01:00,  8.67s/it]iteration: 73, accuracy: 0.3340136054421769\n","iteration: 74, accuracy: 0.3235827664399093:  92% 74/80 [11:44<00:51,  8.51s/it]iteration: 74, accuracy: 0.3235827664399093\n","iteration: 75, accuracy: 0.32063492063492066:  94% 75/80 [11:52<00:42,  8.56s/it]iteration: 75, accuracy: 0.32063492063492066\n","iteration: 76, accuracy: 0.319047619047619:  95% 76/80 [12:00<00:33,  8.33s/it]  iteration: 76, accuracy: 0.319047619047619\n","iteration: 77, accuracy: 0.3213151927437642:  96% 77/80 [12:08<00:24,  8.30s/it]iteration: 77, accuracy: 0.3213151927437642\n","iteration: 78, accuracy: 0.3247165532879819:  98% 78/80 [12:19<00:16,  8.33s/it]iteration: 78, accuracy: 0.3247165532879819\n","iteration: 79, accuracy: 0.3201814058956916:  99% 79/80 [12:28<00:09,  9.02s/it]iteration: 79, accuracy: 0.3201814058956916\n","iteration: 79, accuracy: 0.3201814058956916: 100% 80/80 [12:29<00:00,  9.37s/it]\n","Saving computed projection matrix to: /content/drive/My Drive/bias-bench-main/results/projection_matrix/projection_m-BertModel_c-bert-base-uncased_t-race_s-0.pt\n"]}],"source":["# bert inlp matrix --bias_type gender religion race 3000*3 80\n","\n","! python experiments/inlp_projection_matrix.py --model BertModel --model_name_or_path bert-base-uncased --bias_type race --n_classifiers 80"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":124922,"status":"ok","timestamp":1691337086926,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"1hP3iWuJ_air","outputId":"19d82ed6-e345-41dc-b104-6d8134253e4b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPBertForMaskedLM\n"," - model_name_or_path: bert-base-uncased\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-BertModel_c-bert-base-uncased_t-genderreligionrace_s-0.pt\n"," - load_path: None\n"," - bias_type: gender\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Evaluating gender examples.\n","100% 262/262 [01:57<00:00,  2.24it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 53.44\n","Stereotype score: 43.4\n","Anti-stereotype score: 68.93\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 53.44\n"]}],"source":["\n","! python experiments/crows_debias.py --model INLPBertForMaskedLM --model_name_or_path bert-base-uncased --projection_matrix ./results/projection_matrix/projection_m-BertModel_c-bert-base-uncased_t-genderreligionrace_s-0.pt --bias_type gender"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":190250,"status":"ok","timestamp":1691333186764,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"TtGq9hvGRnea","outputId":"e0afbece-a9ec-48c9-91b9-b77b43d19b51"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPBertForMaskedLM\n"," - model_name_or_path: bert-base-uncased\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-BertModel_c-bert-base-uncased_t-genderreligionrace_s-0.pt\n"," - load_path: None\n"," - bias_type: race\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Evaluating race-color examples.\n"," 70% 363/516 [02:09<00:37,  4.05it/s]Skipping example 363.\n","100% 515/516 [03:03<00:00,  2.80it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 55.15\n","Stereotype score: 56.14\n","Anti-stereotype score: 44.19\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 55.15\n"]}],"source":["\n","! python experiments/crows_debias.py --model INLPBertForMaskedLM --model_name_or_path bert-base-uncased --projection_matrix ./results/projection_matrix/projection_m-BertModel_c-bert-base-uncased_t-genderreligionrace_s-0.pt --bias_type race"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37819,"status":"ok","timestamp":1691333224569,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"1JaTvly4_any","outputId":"b1087da7-d3bd-48f1-94c7-49cd7d10bd4f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPBertForMaskedLM\n"," - model_name_or_path: bert-base-uncased\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-BertModel_c-bert-base-uncased_t-genderreligionrace_s-0.pt\n"," - load_path: None\n"," - bias_type: religion\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Evaluating religion examples.\n","100% 105/105 [00:31<00:00,  3.29it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 51.43\n","Stereotype score: 49.49\n","Anti-stereotype score: 83.33\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 51.43\n"]}],"source":["\n","! python experiments/crows_debias.py --model INLPBertForMaskedLM --model_name_or_path bert-base-uncased --projection_matrix ./results/projection_matrix/projection_m-BertModel_c-bert-base-uncased_t-genderreligionrace_s-0.pt --bias_type religion"]},{"cell_type":"markdown","metadata":{"id":"3o5iNccOU5Co"},"source":["### bert 321"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2841609,"status":"ok","timestamp":1691336360896,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"hRK9dm0s_aqb","outputId":"5d486d12-9a5a-4659-f8de-49c7653a1e66"},"outputs":[{"name":"stdout","output_type":"stream","text":["Computing projection matrix:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: BertModel\n"," - model_name_or_path: bert-base-uncased\n"," - bias_type: race\n"," - n_classifiers: 80\n"," - seed: 0\n","Loading INLP data:   2% 26349/1372632 [00:02<01:54, 11730.29it/s]INLP dataset collected:\n"," - Num. male sentences: 3000\n"," - Num. female sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   2% 27135/1372632 [00:02<01:59, 11243.21it/s]\n","Loading INLP data:   6% 84890/1372632 [00:04<01:14, 17342.93it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   6% 86562/1372632 [00:04<01:14, 17343.66it/s]\n","Loading INLP data:   8% 109230/1372632 [00:07<01:06, 18918.60it/s]INLP dataset collected:\n"," - Num. bias sentences: 3000\n"," - Num. neutral sentences: 3000\n","Loading INLP data:   8% 110011/1372632 [00:07<01:30, 13879.62it/s]\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Encoding male sentences: 100% 3000/3000 [05:03<00:00,  9.90it/s]\n","Encoding female sentences: 100% 3000/3000 [05:00<00:00,  9.99it/s]\n","Encoding neutral sentences: 100% 3000/3000 [04:26<00:00, 11.24it/s]\n","Dataset split sizes:\n","Train size: 4410; Dev size: 1890; Test size: 2700\n","Encoding bias sentences: 100% 3000/3000 [05:40<00:00,  8.80it/s]\n","Encoding neutral sentences: 100% 3000/3000 [04:30<00:00, 11.11it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Encoding bias sentences: 100% 3000/3000 [05:31<00:00,  9.05it/s]\n","Encoding neutral sentences: 100% 3000/3000 [04:29<00:00, 11.12it/s]\n","Dataset split sizes:\n","Train size: 2940; Dev size: 1260; Test size: 1800\n","Dataset split sizes:\n","Train size: 10290; Dev size: 4410; Test size: 6300\n","iteration: 0, accuracy: 0.6802721088435374:   0% 0/80 [00:11<?, ?it/s]iteration: 0, accuracy: 0.6802721088435374\n","iteration: 1, accuracy: 0.6852607709750567:   1% 1/80 [00:22<15:55, 12.10s/it]iteration: 1, accuracy: 0.6852607709750567\n","iteration: 2, accuracy: 0.6755102040816326:   2% 2/80 [00:32<14:38, 11.26s/it]iteration: 2, accuracy: 0.6755102040816326\n","iteration: 3, accuracy: 0.6668934240362812:   4% 3/80 [00:43<14:11, 11.05s/it]iteration: 3, accuracy: 0.6668934240362812\n","iteration: 4, accuracy: 0.6566893424036281:   5% 4/80 [00:54<13:45, 10.86s/it]iteration: 4, accuracy: 0.6566893424036281\n","iteration: 5, accuracy: 0.6351473922902494:   6% 5/80 [01:04<13:39, 10.93s/it]iteration: 5, accuracy: 0.6351473922902494\n","iteration: 6, accuracy: 0.6195011337868481:   8% 6/80 [01:14<13:16, 10.77s/it]iteration: 6, accuracy: 0.6195011337868481\n","iteration: 7, accuracy: 0.610204081632653:   9% 7/80 [01:26<12:52, 10.58s/it] iteration: 7, accuracy: 0.610204081632653\n","iteration: 8, accuracy: 0.6020408163265306:  10% 8/80 [01:36<12:52, 10.73s/it]iteration: 8, accuracy: 0.6020408163265306\n","iteration: 9, accuracy: 0.580952380952381:  11% 9/80 [01:47<12:39, 10.69s/it] iteration: 9, accuracy: 0.580952380952381\n","iteration: 10, accuracy: 0.5789115646258504:  12% 10/80 [01:58<12:37, 10.82s/it]iteration: 10, accuracy: 0.5789115646258504\n","iteration: 11, accuracy: 0.5619047619047619:  14% 11/80 [02:08<12:18, 10.70s/it]iteration: 11, accuracy: 0.5619047619047619\n","iteration: 12, accuracy: 0.5471655328798186:  15% 12/80 [02:20<12:03, 10.63s/it]iteration: 12, accuracy: 0.5471655328798186\n","iteration: 13, accuracy: 0.5430839002267573:  16% 13/80 [02:29<12:05, 10.83s/it]iteration: 13, accuracy: 0.5430839002267573\n","iteration: 14, accuracy: 0.5253968253968254:  18% 14/80 [02:39<11:37, 10.56s/it]iteration: 14, accuracy: 0.5253968253968254\n","iteration: 15, accuracy: 0.526984126984127:  19% 15/80 [02:49<11:06, 10.25s/it] iteration: 15, accuracy: 0.526984126984127\n","iteration: 16, accuracy: 0.5170068027210885:  20% 16/80 [02:58<10:47, 10.12s/it]iteration: 16, accuracy: 0.5170068027210885\n","iteration: 17, accuracy: 0.5206349206349207:  21% 17/80 [03:08<10:30, 10.01s/it]iteration: 17, accuracy: 0.5206349206349207\n","iteration: 18, accuracy: 0.5138321995464853:  22% 18/80 [03:17<10:02,  9.72s/it]iteration: 18, accuracy: 0.5138321995464853\n","iteration: 19, accuracy: 0.5099773242630385:  24% 19/80 [03:25<09:41,  9.53s/it]iteration: 19, accuracy: 0.5099773242630385\n","iteration: 20, accuracy: 0.4981859410430839:  25% 20/80 [03:35<09:21,  9.36s/it]iteration: 20, accuracy: 0.4981859410430839\n","iteration: 21, accuracy: 0.4950113378684807:  26% 21/80 [03:45<09:10,  9.34s/it]iteration: 21, accuracy: 0.4950113378684807\n","iteration: 22, accuracy: 0.4886621315192744:  28% 22/80 [03:54<09:06,  9.43s/it]iteration: 22, accuracy: 0.4886621315192744\n","iteration: 23, accuracy: 0.4873015873015873:  29% 23/80 [04:03<09:02,  9.51s/it]iteration: 23, accuracy: 0.4873015873015873\n","iteration: 24, accuracy: 0.4800453514739229:  30% 24/80 [04:12<08:41,  9.32s/it]iteration: 24, accuracy: 0.4800453514739229\n","iteration: 25, accuracy: 0.4743764172335601:  31% 25/80 [04:21<08:28,  9.24s/it]iteration: 25, accuracy: 0.4743764172335601\n","iteration: 26, accuracy: 0.47573696145124716:  32% 26/80 [04:30<08:10,  9.08s/it]iteration: 26, accuracy: 0.47573696145124716\n","iteration: 27, accuracy: 0.4746031746031746:  34% 27/80 [04:39<08:00,  9.06s/it] iteration: 27, accuracy: 0.4746031746031746\n","iteration: 28, accuracy: 0.4746031746031746:  35% 28/80 [04:49<07:50,  9.06s/it]iteration: 28, accuracy: 0.4746031746031746\n","iteration: 29, accuracy: 0.4657596371882086:  36% 29/80 [04:59<07:57,  9.37s/it]iteration: 29, accuracy: 0.4657596371882086\n","iteration: 30, accuracy: 0.45668934240362813:  38% 30/80 [05:08<07:49,  9.40s/it]iteration: 30, accuracy: 0.45668934240362813\n","iteration: 31, accuracy: 0.4453514739229025:  39% 31/80 [05:17<07:40,  9.39s/it] iteration: 31, accuracy: 0.4453514739229025\n","iteration: 32, accuracy: 0.4385487528344671:  40% 32/80 [05:27<07:31,  9.41s/it]iteration: 32, accuracy: 0.4385487528344671\n","iteration: 33, accuracy: 0.43673469387755104:  41% 33/80 [05:36<07:22,  9.43s/it]iteration: 33, accuracy: 0.43673469387755104\n","iteration: 34, accuracy: 0.4387755102040816:  42% 34/80 [05:45<07:09,  9.33s/it] iteration: 34, accuracy: 0.4387755102040816\n","iteration: 35, accuracy: 0.4392290249433107:  44% 35/80 [05:54<06:58,  9.30s/it]iteration: 35, accuracy: 0.4392290249433107\n","iteration: 36, accuracy: 0.4312925170068027:  45% 36/80 [06:04<06:47,  9.26s/it]iteration: 36, accuracy: 0.4312925170068027\n","iteration: 37, accuracy: 0.43537414965986393:  46% 37/80 [06:13<06:38,  9.26s/it]iteration: 37, accuracy: 0.43537414965986393\n","iteration: 38, accuracy: 0.43038548752834466:  48% 38/80 [06:22<06:29,  9.28s/it]iteration: 38, accuracy: 0.43038548752834466\n","iteration: 39, accuracy: 0.42086167800453517:  49% 39/80 [06:31<06:16,  9.17s/it]iteration: 39, accuracy: 0.42086167800453517\n","iteration: 40, accuracy: 0.41768707482993195:  50% 40/80 [06:39<06:05,  9.13s/it]iteration: 40, accuracy: 0.41768707482993195\n","iteration: 41, accuracy: 0.4002267573696145:  51% 41/80 [06:48<05:52,  9.03s/it] iteration: 41, accuracy: 0.4002267573696145\n","iteration: 42, accuracy: 0.39773242630385486:  52% 42/80 [06:57<05:32,  8.76s/it]iteration: 42, accuracy: 0.39773242630385486\n","iteration: 43, accuracy: 0.3961451247165533:  54% 43/80 [07:05<05:29,  8.90s/it] iteration: 43, accuracy: 0.3961451247165533\n","iteration: 44, accuracy: 0.3956916099773243:  55% 44/80 [07:13<05:14,  8.75s/it]iteration: 44, accuracy: 0.3956916099773243\n","iteration: 45, accuracy: 0.38412698412698415:  56% 45/80 [07:21<04:57,  8.50s/it]iteration: 45, accuracy: 0.38412698412698415\n","iteration: 46, accuracy: 0.38231292517006804:  57% 46/80 [07:30<04:45,  8.40s/it]iteration: 46, accuracy: 0.38231292517006804\n","iteration: 47, accuracy: 0.37936507936507935:  59% 47/80 [07:38<04:36,  8.37s/it]iteration: 47, accuracy: 0.37936507936507935\n","iteration: 48, accuracy: 0.3768707482993197:  60% 48/80 [07:47<04:32,  8.52s/it] iteration: 48, accuracy: 0.3768707482993197\n","iteration: 49, accuracy: 0.373469387755102:  61% 49/80 [07:55<04:19,  8.38s/it] iteration: 49, accuracy: 0.373469387755102\n","iteration: 50, accuracy: 0.373015873015873:  62% 50/80 [08:04<04:14,  8.47s/it]iteration: 50, accuracy: 0.373015873015873\n","iteration: 51, accuracy: 0.364172335600907:  64% 51/80 [08:12<04:02,  8.38s/it]iteration: 51, accuracy: 0.364172335600907\n","iteration: 52, accuracy: 0.36439909297052153:  65% 52/80 [08:20<03:56,  8.46s/it]iteration: 52, accuracy: 0.36439909297052153\n","iteration: 53, accuracy: 0.3505668934240363:  66% 53/80 [08:29<03:47,  8.42s/it] iteration: 53, accuracy: 0.3505668934240363\n","iteration: 54, accuracy: 0.3578231292517007:  68% 54/80 [08:37<03:38,  8.39s/it]iteration: 54, accuracy: 0.3578231292517007\n","iteration: 55, accuracy: 0.3494331065759637:  69% 55/80 [08:46<03:32,  8.51s/it]iteration: 55, accuracy: 0.3494331065759637\n","iteration: 56, accuracy: 0.34784580498866213:  70% 56/80 [08:54<03:20,  8.36s/it]iteration: 56, accuracy: 0.34784580498866213\n","iteration: 57, accuracy: 0.33582766439909295:  71% 57/80 [09:02<03:13,  8.41s/it]iteration: 57, accuracy: 0.33582766439909295\n","iteration: 58, accuracy: 0.33310657596371884:  72% 58/80 [09:10<03:00,  8.22s/it]iteration: 58, accuracy: 0.33310657596371884\n","iteration: 59, accuracy: 0.32902494331065757:  74% 59/80 [09:18<02:53,  8.26s/it]iteration: 59, accuracy: 0.32902494331065757\n","iteration: 60, accuracy: 0.32607709750566893:  75% 60/80 [09:27<02:45,  8.29s/it]iteration: 60, accuracy: 0.32607709750566893\n","iteration: 61, accuracy: 0.3201814058956916:  76% 61/80 [09:35<02:37,  8.31s/it] iteration: 61, accuracy: 0.3201814058956916\n","iteration: 62, accuracy: 0.30975056689342406:  78% 62/80 [09:44<02:30,  8.38s/it]iteration: 62, accuracy: 0.30975056689342406\n","iteration: 63, accuracy: 0.31020408163265306:  79% 63/80 [09:52<02:21,  8.34s/it]iteration: 63, accuracy: 0.31020408163265306\n","iteration: 64, accuracy: 0.32040816326530613:  80% 64/80 [10:00<02:14,  8.42s/it]iteration: 64, accuracy: 0.32040816326530613\n","iteration: 65, accuracy: 0.31541950113378686:  81% 65/80 [10:09<02:04,  8.29s/it]iteration: 65, accuracy: 0.31541950113378686\n","iteration: 66, accuracy: 0.3122448979591837:  82% 66/80 [10:17<01:56,  8.34s/it] iteration: 66, accuracy: 0.3122448979591837\n","iteration: 67, accuracy: 0.30952380952380953:  84% 67/80 [10:26<01:48,  8.31s/it]iteration: 67, accuracy: 0.30952380952380953\n","iteration: 68, accuracy: 0.31564625850340133:  85% 68/80 [10:34<01:39,  8.33s/it]iteration: 68, accuracy: 0.31564625850340133\n","iteration: 69, accuracy: 0.31473922902494333:  86% 69/80 [10:42<01:31,  8.36s/it]iteration: 69, accuracy: 0.31473922902494333\n","iteration: 70, accuracy: 0.3111111111111111:  88% 70/80 [10:51<01:23,  8.31s/it] iteration: 70, accuracy: 0.3111111111111111\n","iteration: 71, accuracy: 0.31746031746031744:  89% 71/80 [10:59<01:15,  8.41s/it]iteration: 71, accuracy: 0.31746031746031744\n","iteration: 72, accuracy: 0.3217687074829932:  90% 72/80 [11:07<01:05,  8.21s/it] iteration: 72, accuracy: 0.3217687074829932\n","iteration: 73, accuracy: 0.33537414965986395:  91% 73/80 [11:15<00:58,  8.29s/it]iteration: 73, accuracy: 0.33537414965986395\n","iteration: 74, accuracy: 0.32222222222222224:  92% 74/80 [11:23<00:48,  8.13s/it]iteration: 74, accuracy: 0.32222222222222224\n","iteration: 75, accuracy: 0.3199546485260771:  94% 75/80 [11:31<00:40,  8.20s/it] iteration: 75, accuracy: 0.3199546485260771\n","iteration: 76, accuracy: 0.31746031746031744:  95% 76/80 [11:39<00:32,  8.11s/it]iteration: 76, accuracy: 0.31746031746031744\n","iteration: 77, accuracy: 0.32290249433106577:  96% 77/80 [11:47<00:24,  8.12s/it]iteration: 77, accuracy: 0.32290249433106577\n","iteration: 78, accuracy: 0.3238095238095238:  98% 78/80 [11:55<00:16,  8.15s/it] iteration: 78, accuracy: 0.3238095238095238\n","iteration: 79, accuracy: 0.319047619047619:  99% 79/80 [12:03<00:08,  8.11s/it] iteration: 79, accuracy: 0.319047619047619\n","iteration: 79, accuracy: 0.319047619047619: 100% 80/80 [12:04<00:00,  9.06s/it]\n","Saving computed projection matrix to: /content/drive/MyDrive/bias-bench-main/results/projection_matrix/projection_m-BertModel_c-bert-base-uncased_t-race_s-0.pt\n"]}],"source":["# bert inlp matrix --bias_type religionracegender mix 3000*3 80\n","\n","! python experiments/inlp_projection_matrix.py --model BertModel --model_name_or_path bert-base-uncased --bias_type race --n_classifiers 80"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":124820,"status":"ok","timestamp":1691336679122,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"MXzZfJKURjh0","outputId":"cb239c42-d828-4380-e8db-0d5c233e91e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPBertForMaskedLM\n"," - model_name_or_path: bert-base-uncased\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-BertModel_c-bert-base-uncased_t-religionracegender_s-0.pt\n"," - load_path: None\n"," - bias_type: gender\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Evaluating gender examples.\n","100% 262/262 [01:58<00:00,  2.21it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 53.44\n","Stereotype score: 43.4\n","Anti-stereotype score: 68.93\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 53.44\n"]}],"source":["\n","! python experiments/crows_debias.py --model INLPBertForMaskedLM --model_name_or_path bert-base-uncased --projection_matrix ./results/projection_matrix/projection_m-BertModel_c-bert-base-uncased_t-religionracegender_s-0.pt --bias_type gender"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":185817,"status":"ok","timestamp":1691336864924,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"hZ6qjpNZRjko","outputId":"fb1a5575-6c92-47c5-861c-490ab97d86f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPBertForMaskedLM\n"," - model_name_or_path: bert-base-uncased\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-BertModel_c-bert-base-uncased_t-religionracegender_s-0.pt\n"," - load_path: None\n"," - bias_type: race\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Evaluating race-color examples.\n"," 70% 363/516 [02:05<00:37,  4.05it/s]Skipping example 363.\n","100% 515/516 [02:59<00:00,  2.87it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 55.15\n","Stereotype score: 56.14\n","Anti-stereotype score: 44.19\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 55.15\n"]}],"source":["\n","! python experiments/crows_debias.py --model INLPBertForMaskedLM --model_name_or_path bert-base-uncased --projection_matrix ./results/projection_matrix/projection_m-BertModel_c-bert-base-uncased_t-religionracegender_s-0.pt --bias_type race"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37549,"status":"ok","timestamp":1691336902469,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"I22iOXKyRjnI","outputId":"7a1675e6-d1eb-48cc-e0a9-51a120c5f4a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: INLPBertForMaskedLM\n"," - model_name_or_path: bert-base-uncased\n"," - bias_direction: None\n"," - projection_matrix: ./results/projection_matrix/projection_m-BertModel_c-bert-base-uncased_t-religionracegender_s-0.pt\n"," - load_path: None\n"," - bias_type: religion\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Evaluating religion examples.\n","100% 105/105 [00:31<00:00,  3.35it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 51.43\n","Stereotype score: 49.49\n","Anti-stereotype score: 83.33\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 51.43\n"]}],"source":["\n","! python experiments/crows_debias.py --model INLPBertForMaskedLM --model_name_or_path bert-base-uncased --projection_matrix ./results/projection_matrix/projection_m-BertModel_c-bert-base-uncased_t-religionracegender_s-0.pt --bias_type religion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LCqLzENjRjrS"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WKe6BH3Niqpc"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"DGqukRmWHrFb"},"source":["## CDA test for time"]},{"cell_type":"markdown","metadata":{"id":"Dy2tIRpBH0z-"},"source":["2.1 未去偏baseline不变，\n","得到单独去偏baseline"]},{"cell_type":"markdown","metadata":{"id":"JyNtxdtWItDD"},"source":["2.2 单独去偏 得到baseline"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":111417,"status":"ok","timestamp":1691993390740,"user":{"displayName":"Chase Wu","userId":"17394296691723216691"},"user_tz":-480},"id":"7F2fNV78tNag","outputId":"bc2c8926-2434-470d-8979-8ee0e90453ad"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/.shortcut-targets-by-id/1l1L_MqcvE3pS-GvOcJE1N8a7hng7c7rS/bias-bench-main\n"," - model: CDAAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: None\n"," - load_path: None\n"," - bias_type: gender\n","Downloading: 100% 684/684 [00:00<00:00, 3.60MB/s]\n","Downloading: 100% 45.2M/45.2M [00:00<00:00, 101MB/s]\n","Downloading: 100% 742k/742k [00:00<00:00, 30.0MB/s]\n","Downloading: 100% 1.25M/1.25M [00:00<00:00, 24.7MB/s]\n","Evaluating gender examples.\n","100% 262/262 [01:35<00:00,  2.75it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 48.09\n","Stereotype score: 47.8\n","Anti-stereotype score: 48.54\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 48.09\n"]}],"source":["# albert的CDA基线 --bias_type gender\n","\n","! python experiments/crows_debias.py --model CDAAlbertForMaskedLM --model_name_or_path albert-base-v2 --bias_type gender"]},{"cell_type":"code","source":["# albert在大规模数据集上进行500步CDA去偏之后 --bias_type gender\n","\n","! python experiments/crows_debias.py --model CDAAlbertForMaskedLM --load_path ./checkpoints/cda_c--albert-base-v2_t-gender-s-3/checkpoint-500 --model_name_or_path albert-base-v2 --bias_type gender"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4IpA7V2BqFNy","executionInfo":{"status":"ok","timestamp":1691993613948,"user_tz":-480,"elapsed":100939,"user":{"displayName":"Chase Wu","userId":"17394296691723216691"}},"outputId":"8e0c77b0-8765-41aa-f48a-26dea74d89bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/.shortcut-targets-by-id/1l1L_MqcvE3pS-GvOcJE1N8a7hng7c7rS/bias-bench-main\n"," - model: CDAAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: None\n"," - load_path: ./checkpoints/cda_c--albert-base-v2_t-gender-s-3/checkpoint-500\n"," - bias_type: gender\n","Evaluating gender examples.\n","100% 262/262 [01:34<00:00,  2.78it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 46.56\n","Stereotype score: 38.36\n","Anti-stereotype score: 59.22\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 46.56\n"]}]},{"cell_type":"code","source":["# albert在大规模数据集上进行1500步CDA去偏之后 --bias_type gender\n","\n","! python experiments/crows_debias.py --model CDAAlbertForMaskedLM --load_path ./checkpoints/cda_c--albert-base-v2_t-gender-s-3/checkpoint-1500 --model_name_or_path albert-base-v2 --bias_type gender"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GYz6QRZinfag","executionInfo":{"status":"ok","timestamp":1692009548728,"user_tz":-60,"elapsed":84137,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"a1c18be0-6ad4-403d-e06b-a74ccc6fc767"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: CDAAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: None\n"," - load_path: ./checkpoints/cda_c--albert-base-v2_t-gender-s-3/checkpoint-1500\n"," - bias_type: gender\n","Evaluating gender examples.\n","100% 262/262 [02:43<00:00,  1.61it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 50.0\n","Stereotype score: 46.54\n","Anti-stereotype score: 55.34\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 50.0\n"]}]},{"cell_type":"code","source":["# albert在大规模数据集上进行1500步CDA去偏之后 --bias_type race\n","\n","! python experiments/crows_debias.py --model CDAAlbertForMaskedLM --load_path ./checkpoints/cda_c--albert-base-v2_t-gender-s-3/checkpoint-1500 --model_name_or_path albert-base-v2 --bias_type race"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iBUTT9IgoeRV","executionInfo":{"status":"ok","timestamp":1692009927348,"user_tz":-60,"elapsed":289636,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"0412c3df-6401-4ec5-c0bb-9293da813132"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: CDAAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: None\n"," - load_path: ./checkpoints/cda_c--albert-base-v2_t-gender-s-3/checkpoint-1500\n"," - bias_type: race\n","Evaluating race-color examples.\n"," 70% 363/516 [03:06<00:49,  3.08it/s]Skipping example 363.\n","100% 515/516 [04:40<00:00,  1.83it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 52.82\n","Stereotype score: 54.24\n","Anti-stereotype score: 37.21\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 52.82\n"]}]},{"cell_type":"code","source":["# albert在大规模数据集上进行1500步CDA去偏之后 --bias_type religion\n","\n","! python experiments/crows_debias.py --model CDAAlbertForMaskedLM --load_path ./checkpoints/cda_c--albert-base-v2_t-gender-s-3/checkpoint-1500 --model_name_or_path albert-base-v2 --bias_type religion"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oWnxhSaaoisl","executionInfo":{"status":"ok","timestamp":1692010210710,"user_tz":-60,"elapsed":55767,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"8620c67c-39d6-4ec0-d4da-909a76b8a67b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: CDAAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: None\n"," - load_path: ./checkpoints/cda_c--albert-base-v2_t-gender-s-3/checkpoint-1500\n"," - bias_type: religion\n","Evaluating religion examples.\n","100% 105/105 [00:46<00:00,  2.27it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 55.24\n","Stereotype score: 56.57\n","Anti-stereotype score: 33.33\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 55.24\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"eGNq2p_XoxyL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# albert在小规模数据集上进行40步CDA去偏之后 --bias_type gender\n","\n","! python experiments/crows_debias.py --model CDAAlbertForMaskedLM --load_path ./checkpoints/cda_c--albert-base-v2_t-gender_s-2/checkpoint-40 --model_name_or_path albert-base-v2 --bias_type gender"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kvBznFXbAjGA","executionInfo":{"status":"ok","timestamp":1691982652873,"user_tz":-480,"elapsed":154152,"user":{"displayName":"Chase Wu","userId":"17394296691723216691"}},"outputId":"e87ba330-3b3a-43f0-88ba-c06a07047457"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/.shortcut-targets-by-id/1l1L_MqcvE3pS-GvOcJE1N8a7hng7c7rS/bias-bench-main\n"," - model: CDAAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: None\n"," - load_path: /content/drive/MyDrive/bias-bench-main/checkpoints/cda_c--albert-base-v2_t-gender_s-0/checkpoint-40\n"," - bias_type: gender\n","Downloading: 100% 684/684 [00:00<00:00, 3.79MB/s]\n","Downloading: 100% 742k/742k [00:00<00:00, 8.05MB/s]\n","Downloading: 100% 1.25M/1.25M [00:00<00:00, 10.2MB/s]\n","Evaluating gender examples.\n","100% 262/262 [02:23<00:00,  1.83it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 50.76\n","Stereotype score: 47.8\n","Anti-stereotype score: 55.34\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 50.76\n"]}]},{"cell_type":"code","source":["# albert在小规模数据集上对gender进行40步CDA去偏之后 --bias_type race\n","\n","! python experiments/crows_debias.py --model CDAAlbertForMaskedLM --load_path ./checkpoints/cda_c--albert-base-v2_t-gender_s-2/checkpoint-40 --model_name_or_path albert-base-v2 --bias_type race"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mi7mLfkkkcQm","executionInfo":{"status":"ok","timestamp":1692008870936,"user_tz":-60,"elapsed":57393,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"c021991b-bbbf-46f6-e060-a006083c7ac3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: CDAAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: None\n"," - load_path: ./checkpoints/cda_c--albert-base-v2_t-gender_s-2/checkpoint-40\n"," - bias_type: race\n","Downloading: 100% 684/684 [00:00<00:00, 2.89MB/s]\n","Downloading: 100% 742k/742k [00:00<00:00, 3.91MB/s]\n","Downloading: 100% 1.25M/1.25M [00:00<00:00, 4.20MB/s]\n","Evaluating race-color examples.\n"," 70% 363/516 [02:56<01:05,  2.33it/s]Skipping example 363.\n","100% 515/516 [04:15<00:00,  2.02it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 54.17\n","Stereotype score: 55.51\n","Anti-stereotype score: 39.53\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 54.17\n"]}]},{"cell_type":"code","source":["# albert在小规模数据集上对gender进行40步CDA去偏之后 --bias_type religion\n","\n","! python experiments/crows_debias.py --model CDAAlbertForMaskedLM --load_path ./checkpoints/cda_c--albert-base-v2_t-gender_s-2/checkpoint-40 --model_name_or_path albert-base-v2 --bias_type religion"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JDHzPGABkpIG","executionInfo":{"status":"ok","timestamp":1692009129393,"user_tz":-60,"elapsed":56183,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"0eb52f4d-2265-4613-f487-14db6831613f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/My Drive/bias-bench-main\n"," - model: CDAAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: None\n"," - load_path: ./checkpoints/cda_c--albert-base-v2_t-gender_s-2/checkpoint-40\n"," - bias_type: religion\n","Evaluating religion examples.\n","100% 105/105 [00:47<00:00,  2.20it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 54.29\n","Stereotype score: 54.55\n","Anti-stereotype score: 50.0\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 54.29\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"S2sTeKYDk2vz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## CDA 200"],"metadata":{"id":"CYNyi_Mc6mB7"}},{"cell_type":"markdown","source":["### cda 50"],"metadata":{"id":"vFsGRA4QZnvq"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1920242,"status":"ok","timestamp":1692051069468,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"NpEMGCmFtNdP","outputId":"04c673f1-6bcf-4808-81e8-330fbe3f8e40"},"outputs":[{"output_type":"stream","name":"stdout","text":["2023-08-14 21:39:12.496740: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-08-14 21:39:13.665237: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","08/14/2023 21:39:19 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","08/14/2023 21:39:19 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=False,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=8,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=./checkpoints/cda5_50/runs/Aug14_21-39-19_d20ab8b56479,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=50,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=./checkpoints/cda5_50,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=./checkpoints/cda5_50,\n","save_on_each_node=False,\n","save_steps=50,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=0,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","08/14/2023 21:39:21 - WARNING - datasets.builder - Using custom data configuration default-0de1c001dd32c133\n","08/14/2023 21:39:21 - INFO - datasets.builder - Generating dataset text (/root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)\n","Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4...\n","100% 1/1 [00:00<00:00, 2989.53it/s]\n","08/14/2023 21:39:21 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n","08/14/2023 21:39:23 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n","100% 1/1 [00:00<00:00, 42.25it/s]\n","08/14/2023 21:39:23 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n","08/14/2023 21:39:23 - INFO - datasets.builder - Generating split train\n","08/14/2023 21:39:25 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n","Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4. Subsequent calls will reuse this data.\n","100% 1/1 [00:00<00:00, 480.17it/s]\n","08/14/2023 21:39:27 - WARNING - datasets.builder - Using custom data configuration default-0de1c001dd32c133\n","08/14/2023 21:39:27 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","08/14/2023 21:39:27 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4\n","08/14/2023 21:39:27 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)\n","08/14/2023 21:39:27 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4\n","08/14/2023 21:39:28 - WARNING - datasets.builder - Using custom data configuration default-0de1c001dd32c133\n","08/14/2023 21:39:28 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","08/14/2023 21:39:28 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4\n","08/14/2023 21:39:28 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)\n","08/14/2023 21:39:28 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4\n","[INFO|file_utils.py:2140] 2023-08-14 21:39:28,724 >> https://huggingface.co/albert-base-v2/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp0078o24p\n","Downloading: 100% 684/684 [00:00<00:00, 2.79MB/s]\n","[INFO|file_utils.py:2144] 2023-08-14 21:39:29,081 >> storing https://huggingface.co/albert-base-v2/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|file_utils.py:2152] 2023-08-14 21:39:29,081 >> creating metadata file for /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:644] 2023-08-14 21:39:29,086 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:680] 2023-08-14 21:39:29,089 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|tokenization_auto.py:336] 2023-08-14 21:39:29,479 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:644] 2023-08-14 21:39:29,854 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:680] 2023-08-14 21:39:29,855 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|file_utils.py:2140] 2023-08-14 21:39:30,654 >> https://huggingface.co/albert-base-v2/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpq1oxw6d7\n","Downloading: 100% 742k/742k [00:00<00:00, 23.3MB/s]\n","[INFO|file_utils.py:2144] 2023-08-14 21:39:31,145 >> storing https://huggingface.co/albert-base-v2/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|file_utils.py:2152] 2023-08-14 21:39:31,145 >> creating metadata file for /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|file_utils.py:2140] 2023-08-14 21:39:31,519 >> https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp6e11fxet\n","Downloading: 100% 1.25M/1.25M [00:00<00:00, 6.30MB/s]\n","[INFO|file_utils.py:2144] 2023-08-14 21:39:32,085 >> storing https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|file_utils.py:2152] 2023-08-14 21:39:32,086 >> creating metadata file for /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|tokenization_utils_base.py:1771] 2023-08-14 21:39:33,275 >> loading file https://huggingface.co/albert-base-v2/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|tokenization_utils_base.py:1771] 2023-08-14 21:39:33,275 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|tokenization_utils_base.py:1771] 2023-08-14 21:39:33,275 >> loading file https://huggingface.co/albert-base-v2/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1771] 2023-08-14 21:39:33,275 >> loading file https://huggingface.co/albert-base-v2/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1771] 2023-08-14 21:39:33,276 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:644] 2023-08-14 21:39:33,659 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:680] 2023-08-14 21:39:33,661 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|file_utils.py:2140] 2023-08-14 21:39:34,172 >> https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp9owg6rwe\n","Downloading: 100% 45.2M/45.2M [00:03<00:00, 12.0MB/s]\n","[INFO|file_utils.py:2144] 2023-08-14 21:39:39,492 >> storing https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[INFO|file_utils.py:2152] 2023-08-14 21:39:39,493 >> creating metadata file for /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[INFO|modeling_utils.py:1427] 2023-08-14 21:39:39,494 >> loading weights file https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[INFO|modeling_utils.py:1694] 2023-08-14 21:39:39,819 >> All model checkpoint weights were used when initializing AlbertForMaskedLM.\n","\n","[INFO|modeling_utils.py:1702] 2023-08-14 21:39:39,819 >> All the weights of AlbertForMaskedLM were initialized from the model checkpoint at albert-base-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForMaskedLM for predictions without further training.\n","08/14/2023 21:39:39 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_function at 0x7c48079ad1b0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n","Running tokenizer on every text in dataset #0:   0% 0/29 [00:00<?, ?ba/s]\n","Running tokenizer on every text in dataset #1:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #2:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\u001b[A[WARNING|tokenization_utils_base.py:3377] 2023-08-14 21:39:41,422 >> Token indices sequence length is longer than the specified maximum sequence length for this model (589 > 512). Running this sequence through the model will result in indexing errors\n","Running tokenizer on every text in dataset #0:   3% 1/29 [00:01<00:40,  1.44s/ba][WARNING|tokenization_utils_base.py:3377] 2023-08-14 21:39:41,933 >> Token indices sequence length is longer than the specified maximum sequence length for this model (605 > 512). Running this sequence through the model will result in indexing errors\n","\n","\n","\n","Running tokenizer on every text in dataset #3:   3% 1/29 [00:01<00:45,  1.62s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:   3% 1/29 [00:01<00:47,  1.70s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #0:   7% 2/29 [00:04<00:57,  2.11s/ba][WARNING|tokenization_utils_base.py:3377] 2023-08-14 21:39:44,554 >> Token indices sequence length is longer than the specified maximum sequence length for this model (916 > 512). Running this sequence through the model will result in indexing errors\n","\n","\n","Running tokenizer on every text in dataset #2:   7% 2/29 [00:04<00:58,  2.15s/ba]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:   7% 2/29 [00:04<00:59,  2.22s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  10% 3/29 [00:05<00:43,  1.68s/ba]\n","\n","[WARNING|tokenization_utils_base.py:3377] 2023-08-14 21:39:45,486 >> Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n","Running tokenizer on every text in dataset #2:  10% 3/29 [00:05<00:41,  1.59s/ba]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:  10% 3/29 [00:05<00:43,  1.67s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  10% 3/29 [00:05<00:45,  1.76s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  14% 4/29 [00:06<00:36,  1.47s/ba]\n","Running tokenizer on every text in dataset #1:  14% 4/29 [00:06<00:35,  1.40s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  14% 4/29 [00:06<00:37,  1.49s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  17% 5/29 [00:07<00:31,  1.32s/ba]\n","Running tokenizer on every text in dataset #1:  17% 5/29 [00:07<00:31,  1.31s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  17% 5/29 [00:07<00:32,  1.34s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  21% 6/29 [00:08<00:28,  1.25s/ba]\n","Running tokenizer on every text in dataset #1:  21% 6/29 [00:08<00:28,  1.23s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  21% 6/29 [00:08<00:28,  1.25s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  24% 7/29 [00:09<00:26,  1.20s/ba]\n","Running tokenizer on every text in dataset #1:  24% 7/29 [00:09<00:26,  1.20s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  24% 7/29 [00:09<00:27,  1.23s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  28% 8/29 [00:10<00:23,  1.14s/ba]\n","Running tokenizer on every text in dataset #1:  28% 8/29 [00:10<00:24,  1.17s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  28% 8/29 [00:10<00:23,  1.12s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  31% 9/29 [00:11<00:22,  1.11s/ba]\n","Running tokenizer on every text in dataset #1:  31% 9/29 [00:11<00:23,  1.17s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  34% 10/29 [00:12<00:19,  1.04s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  34% 10/29 [00:12<00:19,  1.05s/ba]\n","Running tokenizer on every text in dataset #1:  34% 10/29 [00:12<00:21,  1.13s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  38% 11/29 [00:12<00:18,  1.01s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  38% 11/29 [00:13<00:19,  1.10s/ba]\n","\n","Running tokenizer on every text in dataset #2:  41% 12/29 [00:13<00:17,  1.01s/ba]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:  38% 11/29 [00:14<00:21,  1.17s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  38% 11/29 [00:14<00:23,  1.28s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  41% 12/29 [00:15<00:23,  1.40s/ba]\n","Running tokenizer on every text in dataset #1:  41% 12/29 [00:15<00:22,  1.31s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  41% 12/29 [00:16<00:23,  1.40s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:  45% 13/29 [00:17<00:22,  1.39s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  45% 13/29 [00:17<00:23,  1.48s/ba]\n","\n","\n","Running tokenizer on every text in dataset #0:  48% 14/29 [00:19<00:23,  1.54s/ba]\n","Running tokenizer on every text in dataset #1:  48% 14/29 [00:19<00:22,  1.48s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  52% 15/29 [00:19<00:20,  1.50s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  48% 14/29 [00:19<00:22,  1.52s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:  52% 15/29 [00:20<00:20,  1.46s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  52% 15/29 [00:21<00:23,  1.65s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  52% 15/29 [00:21<00:22,  1.57s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:  55% 16/29 [00:22<00:19,  1.48s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  55% 16/29 [00:22<00:21,  1.63s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  55% 16/29 [00:23<00:20,  1.58s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:  59% 17/29 [00:23<00:17,  1.42s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  59% 17/29 [00:24<00:19,  1.59s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  59% 17/29 [00:24<00:17,  1.44s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:  62% 18/29 [00:24<00:14,  1.35s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  62% 18/29 [00:25<00:15,  1.44s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  62% 18/29 [00:25<00:15,  1.36s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:  66% 19/29 [00:25<00:13,  1.30s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  66% 19/29 [00:26<00:12,  1.27s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  66% 19/29 [00:26<00:12,  1.27s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:  69% 20/29 [00:26<00:11,  1.23s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  69% 20/29 [00:27<00:11,  1.26s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  69% 20/29 [00:27<00:10,  1.22s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  76% 22/29 [00:27<00:07,  1.14s/ba]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  72% 21/29 [00:28<00:09,  1.19s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  72% 21/29 [00:28<00:09,  1.14s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:  76% 22/29 [00:28<00:07,  1.14s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  76% 22/29 [00:29<00:08,  1.14s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  76% 22/29 [00:29<00:08,  1.16s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  83% 24/29 [00:30<00:05,  1.15s/ba]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  79% 23/29 [00:31<00:07,  1.29s/ba]\n","\n","Running tokenizer on every text in dataset #2:  86% 25/29 [00:31<00:04,  1.13s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  79% 23/29 [00:31<00:07,  1.33s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  83% 24/29 [00:32<00:06,  1.23s/ba]\n","\n","Running tokenizer on every text in dataset #2:  90% 26/29 [00:32<00:03,  1.12s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  83% 24/29 [00:32<00:06,  1.23s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  86% 25/29 [00:33<00:04,  1.24s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  86% 25/29 [00:33<00:04,  1.24s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  93% 27/29 [00:33<00:02,  1.21s/ba]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  90% 26/29 [00:35<00:04,  1.43s/ba]\n","\n","Running tokenizer on every text in dataset #2:  97% 28/29 [00:36<00:01,  1.55s/ba]\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #2: 100% 29/29 [00:36<00:00,  1.25s/ba]\n","\n","\n","\n","Running tokenizer on every text in dataset #3:  90% 26/29 [00:36<00:05,  1.78s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  93% 27/29 [00:37<00:03,  1.68s/ba]\n","Running tokenizer on every text in dataset #1:  97% 28/29 [00:38<00:01,  1.71s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  93% 27/29 [00:38<00:03,  1.88s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1: 100% 29/29 [00:38<00:00,  1.34s/ba]\n","Running tokenizer on every text in dataset #0: 100% 29/29 [00:39<00:00,  1.38s/ba]\n","\n","\n","\n","Running tokenizer on every text in dataset #3:  97% 28/29 [00:40<00:01,  1.74s/ba]\u001b[A\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3: 100% 29/29 [00:40<00:00,  1.39s/ba]\n","Running tokenizer on every text in dataset #0:   0% 0/2 [00:00<?, ?ba/s]\n","Running tokenizer on every text in dataset #1:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:  50% 1/2 [00:01<00:01,  1.62s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  50% 1/2 [00:02<00:02,  2.01s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  50% 1/2 [00:01<00:01,  1.72s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #2: 100% 2/2 [00:01<00:00,  1.05ba/s]\n","\n","Running tokenizer on every text in dataset #1: 100% 2/2 [00:02<00:00,  1.21s/ba]\n","Running tokenizer on every text in dataset #0: 100% 2/2 [00:02<00:00,  1.29s/ba]\n","\n","\n","\n","Running tokenizer on every text in dataset #3: 100% 2/2 [00:02<00:00,  1.14s/ba]\n","08/14/2023 21:40:24 - INFO - __main__ - Applying gender CDA.\n","Applying counterfactual augmentation for gender #0:   0% 0/29 [00:00<?, ?ba/s]\n","Applying counterfactual augmentation for gender #1:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:   3% 1/29 [00:13<06:12, 13.32s/ba]\n","Applying counterfactual augmentation for gender #1:   3% 1/29 [00:15<07:23, 15.84s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:   3% 1/29 [00:16<07:39, 16.41s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:   7% 2/29 [00:22<04:59, 11.08s/ba]\n","Applying counterfactual augmentation for gender #1:   7% 2/29 [00:25<05:32, 12.33s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:   7% 2/29 [00:26<05:36, 12.45s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  10% 3/29 [00:33<04:42, 10.88s/ba]\n","\n","Applying counterfactual augmentation for gender #2:  14% 4/29 [00:36<03:29,  8.40s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  10% 3/29 [00:37<05:10, 11.95s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  14% 4/29 [00:41<03:59,  9.57s/ba]\n","\n","Applying counterfactual augmentation for gender #2:  17% 5/29 [00:43<03:14,  8.11s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  14% 4/29 [00:45<04:16, 10.24s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  17% 5/29 [00:49<03:38,  9.09s/ba]\n","\n","Applying counterfactual augmentation for gender #2:  21% 6/29 [00:55<03:33,  9.26s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  17% 5/29 [00:55<04:06, 10.27s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  21% 6/29 [00:58<03:33,  9.28s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  21% 6/29 [01:04<03:43,  9.74s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  24% 7/29 [01:12<03:52, 10.57s/ba]\n","Applying counterfactual augmentation for gender #1:  21% 6/29 [01:12<04:38, 12.11s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  28% 8/29 [01:16<03:31, 10.08s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #0:  28% 8/29 [01:21<03:31, 10.08s/ba]\n","Applying counterfactual augmentation for gender #1:  24% 7/29 [01:22<04:12, 11.46s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  28% 8/29 [01:23<03:18,  9.43s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  31% 9/29 [01:30<03:15,  9.79s/ba]\n","Applying counterfactual augmentation for gender #1:  28% 8/29 [01:32<03:48, 10.87s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  34% 10/29 [01:34<03:03,  9.68s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #0:  34% 10/29 [01:40<03:05,  9.77s/ba]\n","Applying counterfactual augmentation for gender #1:  31% 9/29 [01:40<03:23, 10.17s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  38% 11/29 [01:41<02:38,  8.79s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  34% 10/29 [01:44<03:04,  9.69s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  41% 12/29 [01:49<02:24,  8.49s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  38% 11/29 [01:51<03:06, 10.38s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  38% 11/29 [01:56<03:08, 10.46s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  45% 13/29 [01:58<02:17,  8.61s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  41% 12/29 [02:01<02:55, 10.31s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  41% 12/29 [02:03<02:39,  9.37s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  48% 14/29 [02:04<01:59,  8.00s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  45% 13/29 [02:10<02:37,  9.84s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  45% 13/29 [02:14<02:36,  9.76s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  45% 13/29 [02:15<02:23,  8.94s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  48% 14/29 [02:19<02:24,  9.63s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  48% 14/29 [02:21<02:15,  9.05s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  55% 16/29 [02:23<01:50,  8.47s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  52% 15/29 [02:30<02:19,  9.98s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  52% 15/29 [02:32<02:13,  9.53s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  52% 15/29 [02:34<02:10,  9.35s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  55% 16/29 [02:39<02:06,  9.73s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  55% 16/29 [02:40<01:58,  9.09s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  62% 18/29 [02:41<01:33,  8.47s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  55% 16/29 [02:41<01:50,  8.53s/ba]\u001b[A\n","Applying counterfactual augmentation for gender #1:  59% 17/29 [02:51<01:46,  8.85s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  66% 19/29 [02:51<01:28,  8.85s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #0:  59% 17/29 [02:53<02:10, 10.84s/ba]\n","\n","Applying counterfactual augmentation for gender #2:  69% 20/29 [02:59<01:19,  8.85s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  62% 18/29 [03:00<01:38,  8.96s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #0:  62% 18/29 [03:01<01:50, 10.09s/ba]\n","\n","Applying counterfactual augmentation for gender #2:  72% 21/29 [03:10<01:14,  9.28s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #0:  66% 19/29 [03:12<01:42, 10.20s/ba]\n","Applying counterfactual augmentation for gender #1:  66% 19/29 [03:12<01:38,  9.81s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  76% 22/29 [03:18<01:02,  8.99s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #0:  69% 20/29 [03:20<01:26,  9.63s/ba]\n","Applying counterfactual augmentation for gender #1:  69% 20/29 [03:20<01:24,  9.42s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  72% 21/29 [03:28<01:15,  9.38s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  72% 21/29 [03:31<01:19,  9.99s/ba]\n","Applying counterfactual augmentation for gender #1:  72% 21/29 [03:32<01:21, 10.22s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  83% 24/29 [03:37<00:45,  9.02s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #0:  76% 22/29 [03:38<01:04,  9.19s/ba]\n","Applying counterfactual augmentation for gender #1:  76% 22/29 [03:40<01:06,  9.53s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  86% 25/29 [03:45<00:35,  9.00s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #0:  79% 23/29 [03:51<01:01, 10.30s/ba]\n","Applying counterfactual augmentation for gender #1:  79% 23/29 [03:54<01:05, 10.92s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  90% 26/29 [03:58<00:30, 10.00s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #0:  83% 24/29 [04:01<00:51, 10.39s/ba]\n","Applying counterfactual augmentation for gender #1:  83% 24/29 [04:04<00:52, 10.59s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  93% 27/29 [04:08<00:20, 10.06s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #0:  86% 25/29 [04:13<00:43, 10.84s/ba]\n","Applying counterfactual augmentation for gender #1:  86% 25/29 [04:14<00:41, 10.37s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  97% 28/29 [04:16<00:09,  9.52s/ba]\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2: 100% 29/29 [04:17<00:00,  8.88s/ba]\n","Applying counterfactual augmentation for gender #0:  90% 26/29 [04:19<00:28,  9.41s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  90% 26/29 [04:19<00:30, 10.06s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  93% 27/29 [04:26<00:17,  8.65s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  93% 27/29 [04:28<00:19,  9.71s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  93% 27/29 [04:29<00:18,  9.02s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  97% 28/29 [04:35<00:08,  8.71s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  97% 28/29 [04:35<00:08,  8.14s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3: 100% 29/29 [04:35<00:00,  9.51s/ba]\n","Applying counterfactual augmentation for gender #0: 100% 29/29 [04:36<00:00,  9.53s/ba]\n","\n","Applying counterfactual augmentation for gender #1: 100% 29/29 [04:36<00:00,  9.53s/ba]\n","Applying counterfactual augmentation for gender #0:   0% 0/2 [00:00<?, ?ba/s]\n","Applying counterfactual augmentation for gender #1:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  50% 1/2 [00:04<00:04,  4.47s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  50% 1/2 [00:07<00:07,  7.31s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #0:  50% 1/2 [00:08<00:08,  8.85s/ba]\n","Applying counterfactual augmentation for gender #1: 100% 2/2 [00:09<00:00,  4.84s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #2: 100% 2/2 [00:09<00:00,  4.88s/ba]\n","Applying counterfactual augmentation for gender #0: 100% 2/2 [00:12<00:00,  6.05s/ba]\n","\n","\n","\n","Applying counterfactual augmentation for gender #3: 100% 2/2 [00:12<00:00,  6.22s/ba]\n","Applying counterfactual augmentation for race #0:   0% 0/29 [00:00<?, ?ba/s]\n","\n","Applying counterfactual augmentation for race #2:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #0:   3% 1/29 [00:02<00:58,  2.11s/ba]\n","\n","Applying counterfactual augmentation for race #2:   3% 1/29 [00:02<00:59,  2.14s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:   3% 1/29 [00:02<01:02,  2.25s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #0:   7% 2/29 [00:03<00:53,  1.98s/ba]\n","\n","Applying counterfactual augmentation for race #2:   7% 2/29 [00:03<00:52,  1.96s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:   7% 2/29 [00:04<01:01,  2.27s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:   7% 2/29 [00:04<01:03,  2.36s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  10% 3/29 [00:06<00:54,  2.10s/ba]\n","Applying counterfactual augmentation for race #1:  10% 3/29 [00:06<01:00,  2.31s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  10% 3/29 [00:07<01:02,  2.40s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  14% 4/29 [00:08<00:52,  2.10s/ba]\n","Applying counterfactual augmentation for race #1:  14% 4/29 [00:09<00:56,  2.26s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  14% 4/29 [00:09<00:56,  2.25s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  17% 5/29 [00:10<00:52,  2.19s/ba]\n","Applying counterfactual augmentation for race #1:  17% 5/29 [00:12<01:01,  2.58s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  17% 5/29 [00:12<01:03,  2.63s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  21% 6/29 [00:14<01:04,  2.79s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  21% 6/29 [00:16<01:07,  2.96s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  21% 6/29 [00:16<01:11,  3.12s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  24% 7/29 [00:18<01:05,  2.98s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  24% 7/29 [00:19<01:08,  3.12s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  24% 7/29 [00:19<01:09,  3.16s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  28% 8/29 [00:21<01:03,  3.02s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  28% 8/29 [00:22<01:01,  2.94s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  28% 8/29 [00:22<01:03,  3.02s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  31% 9/29 [00:23<00:56,  2.83s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  31% 9/29 [00:24<00:54,  2.71s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  34% 10/29 [00:24<00:48,  2.56s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  34% 10/29 [00:25<00:51,  2.69s/ba]\n","\n","Applying counterfactual augmentation for race #2:  38% 11/29 [00:26<00:41,  2.33s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  34% 10/29 [00:26<00:50,  2.64s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  34% 10/29 [00:27<00:50,  2.67s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  38% 11/29 [00:28<00:48,  2.70s/ba]\n","Applying counterfactual augmentation for race #1:  38% 11/29 [00:29<00:45,  2.53s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  38% 11/29 [00:30<00:51,  2.88s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  45% 13/29 [00:30<00:37,  2.32s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  41% 12/29 [00:32<00:53,  3.15s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  41% 12/29 [00:33<00:52,  3.09s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  48% 14/29 [00:34<00:40,  2.70s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  45% 13/29 [00:35<00:50,  3.15s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  45% 13/29 [00:37<00:50,  3.16s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  52% 15/29 [00:37<00:40,  2.86s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  48% 14/29 [00:39<00:48,  3.25s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  48% 14/29 [00:39<00:44,  2.97s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  55% 16/29 [00:40<00:35,  2.77s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  52% 15/29 [00:41<00:42,  3.01s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  52% 15/29 [00:41<00:38,  2.73s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  59% 17/29 [00:42<00:31,  2.65s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  55% 16/29 [00:43<00:35,  2.73s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  55% 16/29 [00:43<00:32,  2.53s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  62% 18/29 [00:44<00:26,  2.44s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  59% 17/29 [00:45<00:29,  2.46s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  59% 17/29 [00:46<00:29,  2.42s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  59% 17/29 [00:46<00:32,  2.74s/ba]\n","Applying counterfactual augmentation for race #1:  62% 18/29 [00:48<00:27,  2.49s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #0:  62% 18/29 [00:49<00:29,  2.68s/ba]\n","\n","Applying counterfactual augmentation for race #2:  69% 20/29 [00:49<00:21,  2.44s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  66% 19/29 [00:52<00:29,  2.91s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #0:  66% 19/29 [00:53<00:30,  3.05s/ba]\n","\n","Applying counterfactual augmentation for race #2:  72% 21/29 [00:53<00:23,  2.94s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  69% 20/29 [00:55<00:27,  3.04s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #0:  69% 20/29 [00:56<00:27,  3.08s/ba]\n","\n","Applying counterfactual augmentation for race #2:  76% 22/29 [00:56<00:20,  2.98s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  72% 21/29 [00:58<00:22,  2.87s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  72% 21/29 [00:58<00:23,  2.95s/ba]\n","\n","Applying counterfactual augmentation for race #2:  79% 23/29 [00:59<00:17,  2.88s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  76% 22/29 [01:00<00:18,  2.69s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  76% 22/29 [01:01<00:18,  2.71s/ba]\n","\n","Applying counterfactual augmentation for race #2:  83% 24/29 [01:01<00:13,  2.61s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  79% 23/29 [01:02<00:15,  2.56s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  86% 25/29 [01:03<00:09,  2.44s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  79% 23/29 [01:03<00:15,  2.65s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  83% 24/29 [01:04<00:11,  2.40s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  90% 26/29 [01:05<00:07,  2.37s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  83% 24/29 [01:05<00:12,  2.55s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  86% 25/29 [01:06<00:09,  2.35s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  93% 27/29 [01:07<00:04,  2.39s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  86% 25/29 [01:08<00:10,  2.57s/ba]\n","\n","Applying counterfactual augmentation for race #2:  97% 28/29 [01:11<00:02,  2.73s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  90% 26/29 [01:11<00:08,  2.98s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2: 100% 29/29 [01:11<00:00,  2.47s/ba]\n","Applying counterfactual augmentation for race #0:  90% 26/29 [01:11<00:08,  2.82s/ba]\n","Applying counterfactual augmentation for race #0:  93% 27/29 [01:14<00:05,  2.64s/ba]\n","Applying counterfactual augmentation for race #1:  93% 27/29 [01:14<00:05,  2.83s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  93% 27/29 [01:14<00:06,  3.03s/ba]\u001b[A\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  97% 28/29 [01:16<00:02,  2.78s/ba]\u001b[A\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3: 100% 29/29 [01:16<00:00,  2.65s/ba]\n","Applying counterfactual augmentation for race #0:  97% 28/29 [01:17<00:02,  2.70s/ba]\n","Applying counterfactual augmentation for race #0: 100% 29/29 [01:17<00:00,  2.66s/ba]\n","Applying counterfactual augmentation for race #1: 100% 29/29 [01:17<00:00,  2.66s/ba]\n","Applying counterfactual augmentation for race #0:   0% 0/2 [00:00<?, ?ba/s]\n","Applying counterfactual augmentation for race #1:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\n","\n","Applying counterfactual augmentation for race #2:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  50% 1/2 [00:01<00:01,  1.38s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  50% 1/2 [00:01<00:01,  1.87s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #0:  50% 1/2 [00:02<00:02,  2.25s/ba]\n","\n","Applying counterfactual augmentation for race #2: 100% 2/2 [00:02<00:00,  1.11s/ba]\n","\n","Applying counterfactual augmentation for race #1: 100% 2/2 [00:02<00:00,  1.21s/ba]\n","Applying counterfactual augmentation for race #0: 100% 2/2 [00:02<00:00,  1.43s/ba]\n","\n","\n","\n","Applying counterfactual augmentation for race #3: 100% 2/2 [00:02<00:00,  1.37s/ba]\n","Applying counterfactual augmentation for religion #0:   0% 0/29 [00:00<?, ?ba/s]\n","Applying counterfactual augmentation for religion #1:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #0:   3% 1/29 [00:02<00:57,  2.05s/ba]\n","Applying counterfactual augmentation for religion #1:   3% 1/29 [00:02<00:58,  2.10s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:   3% 1/29 [00:01<00:55,  1.99s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:   3% 1/29 [00:02<01:04,  2.31s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:   7% 2/29 [00:03<00:53,  1.98s/ba]\n","Applying counterfactual augmentation for religion #1:   7% 2/29 [00:04<00:55,  2.07s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:   7% 2/29 [00:04<00:59,  2.21s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  10% 3/29 [00:06<00:56,  2.17s/ba]\n","Applying counterfactual augmentation for religion #1:  10% 3/29 [00:06<01:02,  2.40s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  10% 3/29 [00:07<01:03,  2.46s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  14% 4/29 [00:10<01:14,  2.97s/ba]\n","Applying counterfactual augmentation for religion #1:  14% 4/29 [00:10<01:14,  2.98s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  14% 4/29 [00:10<01:13,  2.95s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  17% 5/29 [00:13<01:10,  2.94s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  17% 5/29 [00:13<01:10,  2.93s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #0:  21% 6/29 [00:16<01:06,  2.90s/ba]\n","\n","Applying counterfactual augmentation for religion #2:  21% 6/29 [00:16<01:14,  3.26s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  21% 6/29 [00:16<01:04,  2.81s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #0:  24% 7/29 [00:18<00:59,  2.69s/ba]\n","\n","Applying counterfactual augmentation for religion #2:  24% 7/29 [00:18<01:05,  2.97s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  24% 7/29 [00:18<00:58,  2.64s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  24% 7/29 [00:19<01:02,  2.86s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  28% 8/29 [00:20<00:55,  2.66s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #0:  28% 8/29 [00:21<00:56,  2.71s/ba]\n","Applying counterfactual augmentation for religion #1:  28% 8/29 [00:21<00:56,  2.67s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  31% 9/29 [00:24<00:57,  2.85s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  31% 9/29 [00:24<00:59,  2.97s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  31% 9/29 [00:25<01:00,  3.04s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  34% 10/29 [00:28<01:01,  3.22s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  34% 10/29 [00:29<01:06,  3.48s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  34% 10/29 [00:31<01:13,  3.87s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  38% 11/29 [00:33<01:06,  3.71s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  38% 11/29 [00:33<01:07,  3.76s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  41% 12/29 [00:34<00:56,  3.30s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #0:  41% 12/29 [00:36<01:01,  3.62s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  41% 12/29 [00:36<00:58,  3.44s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  41% 12/29 [00:37<00:58,  3.43s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  45% 13/29 [00:39<00:51,  3.22s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  45% 13/29 [00:38<00:49,  3.08s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  45% 13/29 [00:39<00:47,  2.97s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  48% 14/29 [00:41<00:43,  2.90s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  48% 14/29 [00:41<00:41,  2.79s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  48% 14/29 [00:41<00:40,  2.70s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  52% 15/29 [00:41<00:35,  2.55s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #0:  52% 15/29 [00:43<00:37,  2.71s/ba]\n","Applying counterfactual augmentation for religion #1:  52% 15/29 [00:43<00:34,  2.47s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  55% 16/29 [00:43<00:31,  2.43s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  55% 16/29 [00:45<00:29,  2.24s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #0:  55% 16/29 [00:45<00:33,  2.59s/ba]\n","\n","Applying counterfactual augmentation for religion #2:  59% 17/29 [00:46<00:29,  2.42s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  59% 17/29 [00:47<00:25,  2.10s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  59% 17/29 [00:47<00:28,  2.37s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  59% 17/29 [00:48<00:32,  2.72s/ba]\n","Applying counterfactual augmentation for religion #1:  62% 18/29 [00:51<00:29,  2.64s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  62% 18/29 [00:51<00:32,  2.92s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  62% 18/29 [00:52<00:33,  3.08s/ba]\n","Applying counterfactual augmentation for religion #1:  66% 19/29 [00:54<00:29,  2.95s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #0:  66% 19/29 [00:55<00:30,  3.02s/ba]\n","\n","Applying counterfactual augmentation for religion #2:  69% 20/29 [00:55<00:26,  3.00s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  69% 20/29 [00:57<00:25,  2.82s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  69% 20/29 [00:57<00:25,  2.86s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  69% 20/29 [00:58<00:26,  2.94s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  72% 21/29 [00:59<00:20,  2.56s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  72% 21/29 [00:59<00:21,  2.72s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  72% 21/29 [01:00<00:21,  2.64s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  76% 22/29 [01:01<00:17,  2.48s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #0:  76% 22/29 [01:02<00:16,  2.39s/ba]\n","\n","Applying counterfactual augmentation for religion #2:  79% 23/29 [01:02<00:14,  2.46s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  79% 23/29 [01:03<00:14,  2.40s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  79% 23/29 [01:04<00:14,  2.44s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  79% 23/29 [01:04<00:14,  2.34s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  83% 24/29 [01:05<00:11,  2.25s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  83% 24/29 [01:06<00:11,  2.38s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  83% 24/29 [01:06<00:11,  2.29s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  86% 25/29 [01:07<00:09,  2.30s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  86% 25/29 [01:09<00:10,  2.56s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  86% 25/29 [01:10<00:10,  2.71s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #0:  90% 26/29 [01:13<00:08,  2.80s/ba]\n","Applying counterfactual augmentation for religion #1:  90% 26/29 [01:13<00:09,  3.02s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  93% 27/29 [01:15<00:05,  2.72s/ba]\n","Applying counterfactual augmentation for religion #1:  93% 27/29 [01:16<00:05,  2.97s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  93% 27/29 [01:16<00:06,  3.16s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  97% 28/29 [01:16<00:02,  2.95s/ba]\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #2: 100% 29/29 [01:16<00:00,  2.64s/ba]\n","\n","\n","\n","Applying counterfactual augmentation for religion #3: 100% 29/29 [01:17<00:00,  2.69s/ba]\n","\n","Applying counterfactual augmentation for religion #0:  97% 28/29 [01:18<00:02,  2.65s/ba]\n","Applying counterfactual augmentation for religion #1: 100% 29/29 [01:18<00:00,  2.70s/ba]\n","Applying counterfactual augmentation for religion #0: 100% 29/29 [01:18<00:00,  2.70s/ba]\n","Applying counterfactual augmentation for religion #0:   0% 0/2 [00:00<?, ?ba/s]\n","Applying counterfactual augmentation for religion #1:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  50% 1/2 [00:01<00:01,  1.48s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  50% 1/2 [00:01<00:01,  1.70s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #0:  50% 1/2 [00:02<00:02,  2.10s/ba]\n","\n","Applying counterfactual augmentation for religion #2: 100% 2/2 [00:02<00:00,  1.09s/ba]\n","\n","Applying counterfactual augmentation for religion #1: 100% 2/2 [00:02<00:00,  1.17s/ba]\n","Applying counterfactual augmentation for religion #0: 100% 2/2 [00:02<00:00,  1.40s/ba]\n","\n","\n","\n","Applying counterfactual augmentation for religion #3: 100% 2/2 [00:02<00:00,  1.29s/ba]\n","<class 'datasets.dataset_dict.DatasetDict'>\n","<class 'datasets.dataset_dict.DatasetDict'>\n","<class 'datasets.dataset_dict.DatasetDict'>\n","> /content/drive/My Drive/bias-bench-main/experiments/run_mlm.py(805)main()\n","-> with training_args.main_process_first(desc=\"grouping texts together\"):\n","(Pdb) c\n","Grouping texts in chunks of 512 #0:   0% 0/19 [00:00<?, ?ba/s]\n","Grouping texts in chunks of 512 #1:   0% 0/19 [00:00<?, ?ba/s]\u001b[A\n","\n","Grouping texts in chunks of 512 #2:   0% 0/19 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #3:   0% 0/19 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0:   5% 1/19 [00:15<04:32, 15.14s/ba]\n","\n","Grouping texts in chunks of 512 #2:   5% 1/19 [00:15<04:34, 15.23s/ba]\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #0:  11% 2/19 [00:25<03:27, 12.21s/ba]\n","\n","\n","Grouping texts in chunks of 512 #3:  11% 2/19 [00:25<03:35, 12.65s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Grouping texts in chunks of 512 #2:  11% 2/19 [00:27<03:49, 13.50s/ba]\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #0:  16% 3/19 [00:37<03:17, 12.34s/ba]\n","\n","\n","Grouping texts in chunks of 512 #3:  16% 3/19 [00:39<03:33, 13.32s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Grouping texts in chunks of 512 #2:  16% 3/19 [00:41<03:37, 13.58s/ba]\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #0:  21% 4/19 [00:49<03:00, 12.06s/ba]\n","\n","\n","Grouping texts in chunks of 512 #3:  21% 4/19 [00:50<03:06, 12.42s/ba]\u001b[A\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #1:  21% 4/19 [00:54<03:20, 13.34s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #2:  21% 4/19 [00:54<03:23, 13.54s/ba]\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0:  26% 5/19 [01:02<02:53, 12.41s/ba]\n","Grouping texts in chunks of 512 #1:  26% 5/19 [01:05<02:54, 12.44s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #2:  26% 5/19 [01:08<03:08, 13.48s/ba]\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0:  32% 6/19 [01:17<02:55, 13.47s/ba]\n","\n","Grouping texts in chunks of 512 #2:  32% 6/19 [01:19<02:45, 12.73s/ba]\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #1:  32% 6/19 [01:19<02:51, 13.17s/ba]\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0:  37% 7/19 [01:29<02:35, 12.98s/ba]\n","Grouping texts in chunks of 512 #1:  37% 7/19 [01:32<02:37, 13.10s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #2:  37% 7/19 [01:34<02:41, 13.42s/ba]\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0:  42% 8/19 [01:39<02:12, 12.04s/ba]\n","\n","\n","Grouping texts in chunks of 512 #3:  47% 9/19 [01:46<01:53, 11.36s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Grouping texts in chunks of 512 #2:  42% 8/19 [01:49<02:35, 14.10s/ba]\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #0:  47% 9/19 [01:58<02:19, 14.00s/ba]\n","\n","\n","Grouping texts in chunks of 512 #3:  53% 10/19 [01:59<01:48, 12.01s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Grouping texts in chunks of 512 #2:  47% 9/19 [02:03<02:20, 14.00s/ba]\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #1:  47% 9/19 [02:04<02:23, 14.36s/ba]\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0:  53% 10/19 [02:12<02:05, 13.97s/ba]\n","\n","Grouping texts in chunks of 512 #2:  53% 10/19 [02:16<02:02, 13.56s/ba]\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #1:  53% 10/19 [02:19<02:11, 14.58s/ba]\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0:  58% 11/19 [02:22<01:43, 12.96s/ba]\n","\n","Grouping texts in chunks of 512 #2:  58% 11/19 [02:26<01:41, 12.66s/ba]\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #3:  68% 13/19 [02:30<01:04, 10.72s/ba]\u001b[A\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #1:  58% 11/19 [02:30<01:48, 13.54s/ba]\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0:  63% 12/19 [02:38<01:35, 13.71s/ba]\n","\n","Grouping texts in chunks of 512 #2:  63% 12/19 [02:39<01:28, 12.62s/ba]\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #1:  63% 12/19 [02:40<01:26, 12.35s/ba]\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0:  68% 13/19 [02:51<01:20, 13.46s/ba]\n","Grouping texts in chunks of 512 #1:  68% 13/19 [02:52<01:14, 12.45s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #2:  68% 13/19 [02:53<01:17, 12.98s/ba]\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0:  74% 14/19 [03:02<01:04, 12.82s/ba]\n","\n","Grouping texts in chunks of 512 #2:  74% 14/19 [03:02<01:00, 12.00s/ba]\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #1:  74% 14/19 [03:03<00:58, 11.76s/ba]\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #3:  89% 17/19 [03:03<00:17,  8.94s/ba]\u001b[A\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #1:  79% 15/19 [03:14<00:47, 11.80s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #2:  79% 15/19 [03:15<00:49, 12.35s/ba]\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0:  79% 15/19 [03:16<00:52, 13.21s/ba]\n","\n","\n","Grouping texts in chunks of 512 #3: 100% 19/19 [03:18<00:00, 10.46s/ba]\n","\n","\n","Grouping texts in chunks of 512 #2:  84% 16/19 [03:25<00:34, 11.45s/ba]\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #0:  84% 16/19 [03:26<00:36, 12.16s/ba]\n","\n","Grouping texts in chunks of 512 #0:  89% 17/19 [03:36<00:23, 11.51s/ba]\n","Grouping texts in chunks of 512 #0:  95% 18/19 [03:46<00:11, 11.08s/ba]\n","Grouping texts in chunks of 512 #1:  95% 18/19 [03:47<00:10, 10.98s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #0: 100% 19/19 [03:50<00:00, 12.13s/ba]\n","\n","\n","Grouping texts in chunks of 512 #2: 100% 19/19 [03:50<00:00, 12.15s/ba]\n","\n","Grouping texts in chunks of 512 #1: 100% 19/19 [03:51<00:00, 12.16s/ba]\n","Grouping texts in chunks of 512 #0:   0% 0/1 [00:00<?, ?ba/s]\n","Grouping texts in chunks of 512 #1:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\n","\n","Grouping texts in chunks of 512 #2:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #3:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #3: 100% 1/1 [00:06<00:00,  6.36s/ba]\n","\n","\n","Grouping texts in chunks of 512 #2: 100% 1/1 [00:06<00:00,  6.88s/ba]\n","\n","Grouping texts in chunks of 512 #1: 100% 1/1 [00:07<00:00,  7.18s/ba]\n","Grouping texts in chunks of 512 #0: 100% 1/1 [00:07<00:00,  7.29s/ba]\n","[INFO|trainer.py:420] 2023-08-14 22:00:21,833 >> max_steps is given, it will override any value given in num_train_epochs\n","[INFO|trainer.py:553] 2023-08-14 22:00:21,834 >> The following columns in the training set  don't have a corresponding argument in `AlbertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","[INFO|trainer.py:1244] 2023-08-14 22:00:21,845 >> ***** Running training *****\n","[INFO|trainer.py:1245] 2023-08-14 22:00:21,845 >>   Num examples = 61711\n","[INFO|trainer.py:1246] 2023-08-14 22:00:21,845 >>   Num Epochs = 1\n","[INFO|trainer.py:1247] 2023-08-14 22:00:21,845 >>   Instantaneous batch size per device = 16\n","[INFO|trainer.py:1248] 2023-08-14 22:00:21,845 >>   Total train batch size (w. parallel, distributed & accumulation) = 128\n","[INFO|trainer.py:1249] 2023-08-14 22:00:21,846 >>   Gradient Accumulation steps = 8\n","[INFO|trainer.py:1250] 2023-08-14 22:00:21,846 >>   Total optimization steps = 50\n","100% 50/50 [10:44<00:00, 12.96s/it][INFO|trainer.py:2090] 2023-08-14 22:11:06,396 >> Saving model checkpoint to ./checkpoints/cda5_50/checkpoint-50\n","[INFO|configuration_utils.py:430] 2023-08-14 22:11:06,403 >> Configuration saved in ./checkpoints/cda5_50/checkpoint-50/config.json\n","[INFO|modeling_utils.py:1074] 2023-08-14 22:11:06,512 >> Model weights saved in ./checkpoints/cda5_50/checkpoint-50/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2074] 2023-08-14 22:11:06,517 >> tokenizer config file saved in ./checkpoints/cda5_50/checkpoint-50/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2080] 2023-08-14 22:11:06,520 >> Special tokens file saved in ./checkpoints/cda5_50/checkpoint-50/special_tokens_map.json\n","[INFO|trainer.py:1473] 2023-08-14 22:11:06,810 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 644.965, 'train_samples_per_second': 9.923, 'train_steps_per_second': 0.078, 'train_loss': 2.586270751953125, 'epoch': 0.1}\n","100% 50/50 [10:44<00:00, 12.90s/it]\n","[INFO|trainer.py:2090] 2023-08-14 22:11:06,816 >> Saving model checkpoint to ./checkpoints/cda5_50\n","[INFO|configuration_utils.py:430] 2023-08-14 22:11:06,820 >> Configuration saved in ./checkpoints/cda5_50/config.json\n","[INFO|modeling_utils.py:1074] 2023-08-14 22:11:06,961 >> Model weights saved in ./checkpoints/cda5_50/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2074] 2023-08-14 22:11:06,966 >> tokenizer config file saved in ./checkpoints/cda5_50/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2080] 2023-08-14 22:11:06,970 >> Special tokens file saved in ./checkpoints/cda5_50/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =        0.1\n","  train_loss               =     2.5863\n","  train_runtime            = 0:10:44.96\n","  train_samples            =      61711\n","  train_samples_per_second =      9.923\n","  train_steps_per_second   =      0.078\n"]}],"source":["# ! bash ./batch_jobs/counterfactual_augmentation.sh\n","\n","# source \"batch_jobs/_experiment_configuration.sh\"\n","\n","experiment_id=\"cda_c-albert-base-v2\"\n","!python experiments/run_mlm.py \\\n","                    --model_name_or_path \"albert-base-v2\" \\\n","                    --do_train \\\n","                    --train_file \"./data/text/wikipedia-2.txt\" \\\n","                    --max_steps 50 \\\n","                    --per_device_train_batch_size 16 \\\n","                    --gradient_accumulation_steps 8 \\\n","                    --max_seq_length 512 \\\n","                    --save_steps 50 \\\n","                    --preprocessing_num_workers 4 \\\n","                    --counterfactual_augmentation \"gender\" \\\n","                    --seed 0 \\\n","                    --output_dir \"./checkpoints/cda5_50\"\n","\n","\n"]},{"cell_type":"code","source":["# albert在小规模数据集上进行gender_race_religion 50步CDA去偏之后 --bias_type gender\n","\n","! python experiments/crows_debias.py --model CDAAlbertForMaskedLM --load_path ./checkpoints/cda5_50 --model_name_or_path albert-base-v2 --bias_type gender"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pJoyycZWHEWH","executionInfo":{"status":"ok","timestamp":1692051241724,"user_tz":-60,"elapsed":26116,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"c472020b-4f1d-4fb5-90dd-005bc59a46c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: CDAAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: None\n"," - load_path: ./checkpoints/cda5_50\n"," - bias_type: gender\n","Evaluating gender examples.\n","100% 262/262 [00:15<00:00, 17.42it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 52.67\n","Stereotype score: 51.57\n","Anti-stereotype score: 54.37\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 52.67\n"]}]},{"cell_type":"code","source":["# albert在小规模数据集上进行gender_race_religion 50步CDA去偏之后 --bias_type\n","\n","! python experiments/crows_debias.py --model CDAAlbertForMaskedLM --load_path ./checkpoints/cda5_50 --model_name_or_path albert-base-v2 --bias_type race"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DPo1yWZ23DsT","executionInfo":{"status":"ok","timestamp":1692051180756,"user_tz":-60,"elapsed":43045,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"03c52189-d986-4999-f458-166ad1c078fc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: CDAAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: None\n"," - load_path: ./checkpoints/cda5_50\n"," - bias_type: race\n","Evaluating race-color examples.\n"," 70% 363/516 [00:15<00:04, 33.70it/s]Skipping example 363.\n","100% 515/516 [00:21<00:00, 24.52it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 58.06\n","Stereotype score: 59.53\n","Anti-stereotype score: 41.86\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 58.06\n"]}]},{"cell_type":"code","source":["# albert在小规模数据集上进行gender_race_religion 50步CDA去偏之后 --bias_type religion\n","\n","! python experiments/crows_debias.py --model CDAAlbertForMaskedLM --load_path ./checkpoints/cda5_50 --model_name_or_path albert-base-v2 --bias_type religion"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cIj6qLpS3Efg","executionInfo":{"status":"ok","timestamp":1692051258748,"user_tz":-60,"elapsed":13870,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"e65a6867-6072-4d23-804f-a36d1a8b99f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: CDAAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: None\n"," - load_path: ./checkpoints/cda5_50\n"," - bias_type: religion\n","Evaluating religion examples.\n","100% 105/105 [00:04<00:00, 25.32it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 54.29\n","Stereotype score: 54.55\n","Anti-stereotype score: 50.0\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 54.29\n"]}]},{"cell_type":"markdown","source":["### CDA tiny data 200"],"metadata":{"id":"vgCpVnONIm4J"}},{"cell_type":"code","source":["# ! bash ./batch_jobs/counterfactual_augmentation.sh\n","\n","# source \"batch_jobs/_experiment_configuration.sh\"\n","\n","experiment_id=\"cda_c-albert-base-v2\"\n","!python experiments/run_mlm.py \\\n","                    --model_name_or_path \"albert-base-v2\" \\\n","                    --do_train \\\n","                    --train_file \"./data/text/wikipedia-2.txt\" \\\n","                    --max_steps 200 \\\n","                    --per_device_train_batch_size 16 \\\n","                    --gradient_accumulation_steps 8 \\\n","                    --max_seq_length 512 \\\n","                    --save_steps 50 \\\n","                    --preprocessing_num_workers 4 \\\n","                    --counterfactual_augmentation \"gender\" \\\n","                    --seed 0 \\\n","                    --output_dir \"./checkpoints/cda123_200\"\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tcwkhhkuHRyt","executionInfo":{"status":"ok","timestamp":1692055097828,"user_tz":-60,"elapsed":3351371,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"5a6d0e2b-7a71-464d-defa-4078ea8e862a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-08-14 22:22:28.316012: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-08-14 22:22:29.324258: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","08/14/2023 22:22:31 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","08/14/2023 22:22:31 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=False,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=8,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=./checkpoints/cda123_200/runs/Aug14_22-22-31_d20ab8b56479,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=200,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=./checkpoints/cda123_200,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=./checkpoints/cda123_200,\n","save_on_each_node=False,\n","save_steps=50,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=0,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","08/14/2023 22:22:32 - WARNING - datasets.builder - Using custom data configuration default-0de1c001dd32c133\n","08/14/2023 22:22:32 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","08/14/2023 22:22:32 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4\n","08/14/2023 22:22:32 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)\n","08/14/2023 22:22:32 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4\n","100% 1/1 [00:00<00:00, 644.68it/s]\n","08/14/2023 22:22:33 - WARNING - datasets.builder - Using custom data configuration default-0de1c001dd32c133\n","08/14/2023 22:22:33 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","08/14/2023 22:22:33 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4\n","08/14/2023 22:22:33 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)\n","08/14/2023 22:22:33 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4\n","08/14/2023 22:22:34 - WARNING - datasets.builder - Using custom data configuration default-0de1c001dd32c133\n","08/14/2023 22:22:34 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","08/14/2023 22:22:34 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4\n","08/14/2023 22:22:34 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)\n","08/14/2023 22:22:34 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4\n","[INFO|configuration_utils.py:644] 2023-08-14 22:22:35,460 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:680] 2023-08-14 22:22:35,461 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|tokenization_auto.py:336] 2023-08-14 22:22:35,757 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:644] 2023-08-14 22:22:36,061 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:680] 2023-08-14 22:22:36,062 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|tokenization_utils_base.py:1771] 2023-08-14 22:22:38,171 >> loading file https://huggingface.co/albert-base-v2/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|tokenization_utils_base.py:1771] 2023-08-14 22:22:38,171 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|tokenization_utils_base.py:1771] 2023-08-14 22:22:38,171 >> loading file https://huggingface.co/albert-base-v2/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1771] 2023-08-14 22:22:38,171 >> loading file https://huggingface.co/albert-base-v2/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1771] 2023-08-14 22:22:38,171 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:644] 2023-08-14 22:22:38,534 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:680] 2023-08-14 22:22:38,535 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|modeling_utils.py:1427] 2023-08-14 22:22:38,974 >> loading weights file https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[INFO|modeling_utils.py:1694] 2023-08-14 22:22:39,220 >> All model checkpoint weights were used when initializing AlbertForMaskedLM.\n","\n","[INFO|modeling_utils.py:1702] 2023-08-14 22:22:39,221 >> All the weights of AlbertForMaskedLM were initialized from the model checkpoint at albert-base-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForMaskedLM for predictions without further training.\n","08/14/2023 22:22:39 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_function at 0x7da6310dc3a0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n","Running tokenizer on every text in dataset #0:   0% 0/29 [00:00<?, ?ba/s]\n","Running tokenizer on every text in dataset #1:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A[WARNING|tokenization_utils_base.py:3377] 2023-08-14 22:22:40,852 >> Token indices sequence length is longer than the specified maximum sequence length for this model (589 > 512). Running this sequence through the model will result in indexing errors\n","[WARNING|tokenization_utils_base.py:3377] 2023-08-14 22:22:41,019 >> Token indices sequence length is longer than the specified maximum sequence length for this model (605 > 512). Running this sequence through the model will result in indexing errors\n","Running tokenizer on every text in dataset #0:   3% 1/29 [00:01<00:42,  1.51s/ba]\n","\n","Running tokenizer on every text in dataset #2:   3% 1/29 [00:01<00:42,  1.53s/ba]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:   3% 1/29 [00:01<00:47,  1.69s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:   3% 1/29 [00:01<00:44,  1.59s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #0:   7% 2/29 [00:03<00:43,  1.61s/ba][WARNING|tokenization_utils_base.py:3377] 2023-08-14 22:22:42,791 >> Token indices sequence length is longer than the specified maximum sequence length for this model (916 > 512). Running this sequence through the model will result in indexing errors\n","\n","\n","\n","Running tokenizer on every text in dataset #3:   7% 2/29 [00:03<00:42,  1.58s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:   7% 2/29 [00:03<00:45,  1.68s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  10% 3/29 [00:04<00:43,  1.69s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  10% 3/29 [00:05<00:48,  1.87s/ba][WARNING|tokenization_utils_base.py:3377] 2023-08-14 22:22:45,035 >> Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n","\n","Running tokenizer on every text in dataset #1:  10% 3/29 [00:05<00:48,  1.87s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  14% 4/29 [00:05<00:33,  1.36s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  14% 4/29 [00:06<00:39,  1.58s/ba]\n","Running tokenizer on every text in dataset #1:  14% 4/29 [00:06<00:39,  1.57s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  17% 5/29 [00:06<00:30,  1.27s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  17% 5/29 [00:07<00:33,  1.38s/ba]\n","Running tokenizer on every text in dataset #1:  17% 5/29 [00:07<00:33,  1.41s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  21% 6/29 [00:08<00:28,  1.23s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  21% 6/29 [00:08<00:29,  1.30s/ba]\n","Running tokenizer on every text in dataset #1:  21% 6/29 [00:08<00:31,  1.37s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  24% 7/29 [00:09<00:25,  1.18s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  24% 7/29 [00:09<00:26,  1.20s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  28% 8/29 [00:09<00:22,  1.05s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  28% 8/29 [00:10<00:23,  1.12s/ba]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  28% 8/29 [00:10<00:24,  1.16s/ba]\n","\n","Running tokenizer on every text in dataset #2:  31% 9/29 [00:11<00:22,  1.11s/ba]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:  28% 8/29 [00:11<00:26,  1.24s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  31% 9/29 [00:11<00:23,  1.17s/ba]\n","\n","Running tokenizer on every text in dataset #2:  34% 10/29 [00:12<00:20,  1.07s/ba]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:  31% 9/29 [00:12<00:23,  1.18s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  34% 10/29 [00:12<00:21,  1.14s/ba]\n","\n","Running tokenizer on every text in dataset #2:  38% 11/29 [00:13<00:18,  1.04s/ba]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:  34% 10/29 [00:13<00:22,  1.17s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  38% 11/29 [00:13<00:21,  1.21s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  38% 11/29 [00:14<00:23,  1.31s/ba]\n","Running tokenizer on every text in dataset #1:  38% 11/29 [00:15<00:24,  1.37s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  41% 12/29 [00:15<00:24,  1.43s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  41% 12/29 [00:16<00:27,  1.61s/ba]\n","Running tokenizer on every text in dataset #1:  41% 12/29 [00:17<00:25,  1.52s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  45% 13/29 [00:17<00:26,  1.63s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  45% 13/29 [00:18<00:25,  1.62s/ba]\n","Running tokenizer on every text in dataset #1:  45% 13/29 [00:18<00:24,  1.54s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  48% 14/29 [00:19<00:24,  1.66s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  48% 14/29 [00:20<00:24,  1.63s/ba]\n","Running tokenizer on every text in dataset #1:  48% 14/29 [00:20<00:23,  1.54s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  55% 16/29 [00:21<00:19,  1.53s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  52% 15/29 [00:21<00:23,  1.65s/ba]\n","Running tokenizer on every text in dataset #1:  52% 15/29 [00:21<00:22,  1.57s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  59% 17/29 [00:22<00:18,  1.54s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  55% 16/29 [00:22<00:21,  1.64s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  55% 16/29 [00:23<00:20,  1.56s/ba]\n","\n","Running tokenizer on every text in dataset #2:  62% 18/29 [00:23<00:14,  1.35s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  59% 17/29 [00:23<00:17,  1.49s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  59% 17/29 [00:24<00:18,  1.53s/ba]\n","\n","Running tokenizer on every text in dataset #2:  66% 19/29 [00:24<00:12,  1.27s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  62% 18/29 [00:25<00:15,  1.39s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  62% 18/29 [00:25<00:15,  1.43s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  66% 19/29 [00:26<00:12,  1.27s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  66% 19/29 [00:26<00:12,  1.30s/ba]\n","Running tokenizer on every text in dataset #1:  66% 19/29 [00:26<00:13,  1.34s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  69% 20/29 [00:27<00:10,  1.19s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  69% 20/29 [00:27<00:10,  1.19s/ba]\n","Running tokenizer on every text in dataset #1:  69% 20/29 [00:28<00:11,  1.30s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  72% 21/29 [00:28<00:09,  1.19s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  72% 21/29 [00:29<00:09,  1.18s/ba]\n","Running tokenizer on every text in dataset #1:  72% 21/29 [00:29<00:10,  1.27s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  76% 22/29 [00:30<00:07,  1.14s/ba]\n","\n","Running tokenizer on every text in dataset #2:  79% 23/29 [00:29<00:07,  1.32s/ba]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:  76% 22/29 [00:30<00:08,  1.22s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  83% 24/29 [00:31<00:06,  1.23s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  79% 23/29 [00:31<00:07,  1.32s/ba]\n","Running tokenizer on every text in dataset #1:  79% 23/29 [00:32<00:07,  1.33s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  86% 25/29 [00:32<00:04,  1.18s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  83% 24/29 [00:33<00:06,  1.29s/ba]\n","\n","Running tokenizer on every text in dataset #2:  90% 26/29 [00:33<00:03,  1.25s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  86% 25/29 [00:33<00:05,  1.27s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  86% 25/29 [00:35<00:06,  1.57s/ba]\n","Running tokenizer on every text in dataset #1:  86% 25/29 [00:35<00:06,  1.56s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  93% 27/29 [00:35<00:03,  1.53s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  90% 26/29 [00:36<00:04,  1.57s/ba]\n","\n","Running tokenizer on every text in dataset #2:  97% 28/29 [00:37<00:01,  1.55s/ba]\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #2: 100% 29/29 [00:37<00:00,  1.29s/ba]\n","\n","Running tokenizer on every text in dataset #1:  90% 26/29 [00:37<00:05,  1.71s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  93% 27/29 [00:38<00:02,  1.48s/ba]\n","Running tokenizer on every text in dataset #1:  93% 27/29 [00:38<00:03,  1.58s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  97% 28/29 [00:38<00:01,  1.55s/ba]\u001b[A\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3: 100% 29/29 [00:39<00:00,  1.35s/ba]\n","Running tokenizer on every text in dataset #0: 100% 29/29 [00:39<00:00,  1.37s/ba]\n","\n","Running tokenizer on every text in dataset #1: 100% 29/29 [00:40<00:00,  1.38s/ba]\n","Running tokenizer on every text in dataset #0:   0% 0/2 [00:00<?, ?ba/s]\n","\n","Running tokenizer on every text in dataset #2:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  50% 1/2 [00:01<00:01,  1.11s/ba]\n","\n","Running tokenizer on every text in dataset #2:  50% 1/2 [00:01<00:01,  1.07s/ba]\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #2: 100% 2/2 [00:01<00:00,  1.57ba/s]\n","\n","\n","\n","Running tokenizer on every text in dataset #3:  50% 1/2 [00:01<00:01,  1.14s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1: 100% 2/2 [00:01<00:00,  1.48ba/s]\n","Running tokenizer on every text in dataset #0: 100% 2/2 [00:01<00:00,  1.28ba/s]\n","\n","\n","\n","Running tokenizer on every text in dataset #3: 100% 2/2 [00:01<00:00,  1.35ba/s]\n","08/14/2023 22:23:21 - INFO - __main__ - Applying gender CDA.\n","Applying counterfactual augmentation for gender #0:   0% 0/29 [00:00<?, ?ba/s]\n","Applying counterfactual augmentation for gender #1:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:   3% 1/29 [00:07<03:19,  7.11s/ba]\n","Applying counterfactual augmentation for gender #1:   3% 1/29 [00:07<03:37,  7.76s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:   3% 1/29 [00:08<04:04,  8.72s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:   7% 2/29 [00:18<04:19,  9.59s/ba]\n","Applying counterfactual augmentation for gender #1:   7% 2/29 [00:20<04:51, 10.80s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:   7% 2/29 [00:21<04:59, 11.09s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  10% 3/29 [00:26<03:45,  8.68s/ba]\n","\n","Applying counterfactual augmentation for gender #2:  14% 4/29 [00:30<03:05,  7.44s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  10% 3/29 [00:31<04:33, 10.52s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  14% 4/29 [00:36<03:57,  9.48s/ba]\n","\n","Applying counterfactual augmentation for gender #2:  17% 5/29 [00:40<03:14,  8.09s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  14% 4/29 [00:40<04:09,  9.99s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  17% 5/29 [00:43<03:23,  8.48s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  17% 5/29 [00:49<03:49,  9.55s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  21% 6/29 [00:49<03:17,  8.59s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  21% 6/29 [00:55<03:40,  9.58s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  21% 6/29 [01:01<03:58, 10.37s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  24% 7/29 [01:01<03:35,  9.79s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  24% 7/29 [01:06<03:44, 10.20s/ba]\n","\n","Applying counterfactual augmentation for gender #2:  28% 8/29 [01:12<03:32, 10.10s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #0:  28% 8/29 [01:20<04:00, 11.46s/ba]\n","Applying counterfactual augmentation for gender #1:  24% 7/29 [01:22<04:46, 13.03s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  28% 8/29 [01:24<03:43, 10.63s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  31% 9/29 [01:29<03:29, 10.45s/ba]\n","Applying counterfactual augmentation for gender #1:  28% 8/29 [01:29<03:55, 11.23s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  34% 10/29 [01:32<03:06,  9.83s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  31% 9/29 [01:34<03:29, 10.46s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  34% 10/29 [01:40<03:24, 10.78s/ba]\n","\n","Applying counterfactual augmentation for gender #2:  38% 11/29 [01:41<02:51,  9.54s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  34% 10/29 [01:44<03:18, 10.42s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  41% 12/29 [01:48<02:27,  8.68s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  38% 11/29 [01:49<03:05, 10.29s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  38% 11/29 [01:56<03:15, 10.87s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  45% 13/29 [01:58<02:25,  9.08s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  41% 12/29 [02:02<03:05, 10.90s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  41% 12/29 [02:03<02:47,  9.86s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  48% 14/29 [02:04<02:05,  8.38s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  45% 13/29 [02:08<02:32,  9.50s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  45% 13/29 [02:13<02:36,  9.81s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  52% 15/29 [02:15<02:05,  8.96s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  48% 14/29 [02:19<02:30, 10.00s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  48% 14/29 [02:22<02:23,  9.57s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  55% 16/29 [02:23<01:54,  8.78s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  52% 15/29 [02:27<02:11,  9.40s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  52% 15/29 [02:31<02:09,  9.27s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  52% 15/29 [02:33<02:07,  9.13s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  55% 16/29 [02:39<02:10, 10.04s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  55% 16/29 [02:40<02:02,  9.39s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  55% 16/29 [02:41<01:53,  8.73s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  62% 18/29 [02:41<01:37,  8.82s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  59% 17/29 [02:49<02:02, 10.23s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  59% 17/29 [02:49<01:50,  9.21s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  66% 19/29 [02:50<01:27,  8.77s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  62% 18/29 [03:00<01:54, 10.37s/ba]\n","\n","Applying counterfactual augmentation for gender #2:  69% 20/29 [03:01<01:23,  9.29s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #0:  66% 19/29 [03:08<01:35,  9.58s/ba]\n","Applying counterfactual augmentation for gender #1:  66% 19/29 [03:09<01:33,  9.31s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  72% 21/29 [03:09<01:12,  9.07s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #0:  69% 20/29 [03:19<01:31, 10.20s/ba]\n","\n","Applying counterfactual augmentation for gender #2:  76% 22/29 [03:19<01:05,  9.34s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  69% 20/29 [03:19<01:26,  9.60s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #0:  72% 21/29 [03:27<01:15,  9.38s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  72% 21/29 [03:27<01:11,  8.95s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  79% 23/29 [03:28<00:55,  9.25s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  76% 22/29 [03:37<01:06,  9.55s/ba]\n","\n","Applying counterfactual augmentation for gender #2:  83% 24/29 [03:38<00:46,  9.31s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  76% 22/29 [03:38<01:08,  9.72s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  76% 22/29 [03:40<01:09,  9.92s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  79% 23/29 [03:46<00:56,  9.35s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  79% 23/29 [03:49<00:59,  9.98s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  79% 23/29 [03:51<01:01, 10.23s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  83% 24/29 [03:57<00:50, 10.11s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  83% 24/29 [03:57<00:47,  9.57s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  83% 24/29 [04:02<00:52, 10.53s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  93% 27/29 [04:09<00:21, 10.57s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #0:  86% 25/29 [04:13<00:47, 11.84s/ba]\n","Applying counterfactual augmentation for gender #1:  86% 25/29 [04:15<00:45, 11.41s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  97% 28/29 [04:21<00:10, 10.81s/ba]\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2: 100% 29/29 [04:21<00:00,  9.03s/ba]\n","Applying counterfactual augmentation for gender #0:  90% 26/29 [04:22<00:32, 10.95s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  90% 26/29 [04:24<00:34, 11.51s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  93% 27/29 [04:27<00:18,  9.07s/ba]\n","Applying counterfactual augmentation for gender #1:  93% 27/29 [04:30<00:18,  9.06s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #0: 100% 29/29 [04:38<00:00,  9.60s/ba]\n","\n","Applying counterfactual augmentation for gender #1:  97% 28/29 [04:39<00:09,  9.10s/ba]\u001b[A\n","Applying counterfactual augmentation for gender #1: 100% 29/29 [04:39<00:00,  9.65s/ba]\n","\n","\n","\n","Applying counterfactual augmentation for gender #3:  97% 28/29 [04:39<00:09,  9.44s/ba]\u001b[A\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3: 100% 29/29 [04:39<00:00,  9.65s/ba]\n","Applying counterfactual augmentation for gender #0:   0% 0/2 [00:00<?, ?ba/s]\n","Applying counterfactual augmentation for gender #1:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  50% 1/2 [00:04<00:04,  4.88s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  50% 1/2 [00:07<00:07,  7.52s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  50% 1/2 [00:07<00:07,  7.49s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2: 100% 2/2 [00:07<00:00,  4.00s/ba]\n","\n","Applying counterfactual augmentation for gender #1: 100% 2/2 [00:08<00:00,  4.10s/ba]\n","Applying counterfactual augmentation for gender #0: 100% 2/2 [00:09<00:00,  4.94s/ba]\n","\n","\n","\n","Applying counterfactual augmentation for gender #3: 100% 2/2 [00:10<00:00,  5.23s/ba]\n","Applying counterfactual augmentation for race #0:   0% 0/29 [00:00<?, ?ba/s]\n","Applying counterfactual augmentation for race #1:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\n","\n","Applying counterfactual augmentation for race #2:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #0:   3% 1/29 [00:03<01:27,  3.11s/ba]\n","\n","Applying counterfactual augmentation for race #2:   3% 1/29 [00:03<01:36,  3.43s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:   3% 1/29 [00:03<01:37,  3.48s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:   7% 2/29 [00:05<01:18,  2.91s/ba]\n","\n","Applying counterfactual augmentation for race #2:   7% 2/29 [00:05<01:17,  2.88s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:   7% 2/29 [00:06<01:27,  3.26s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:   7% 2/29 [00:06<01:32,  3.42s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  10% 3/29 [00:08<01:11,  2.77s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  10% 3/29 [00:09<01:15,  2.89s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  10% 3/29 [00:09<01:15,  2.89s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  14% 4/29 [00:10<01:04,  2.57s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  14% 4/29 [00:11<01:03,  2.55s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  17% 5/29 [00:11<00:48,  2.04s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  17% 5/29 [00:12<00:57,  2.41s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  17% 5/29 [00:13<00:58,  2.44s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  17% 5/29 [00:13<00:59,  2.49s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  21% 6/29 [00:15<00:53,  2.33s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  21% 6/29 [00:15<00:54,  2.38s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  24% 7/29 [00:15<00:45,  2.08s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  24% 7/29 [00:17<00:52,  2.39s/ba]\n","\n","Applying counterfactual augmentation for race #2:  28% 8/29 [00:19<00:51,  2.45s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  24% 7/29 [00:19<01:02,  2.83s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  28% 8/29 [00:21<00:58,  2.80s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  28% 8/29 [00:22<01:00,  2.86s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  28% 8/29 [00:23<01:04,  3.08s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  31% 9/29 [00:24<01:00,  3.04s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  31% 9/29 [00:25<01:00,  3.01s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  34% 10/29 [00:26<00:55,  2.95s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  34% 10/29 [00:27<00:56,  2.96s/ba]\n","\n","Applying counterfactual augmentation for race #2:  38% 11/29 [00:28<00:47,  2.61s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  34% 10/29 [00:28<00:58,  3.08s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  34% 10/29 [00:29<00:59,  3.12s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  38% 11/29 [00:30<00:52,  2.93s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  38% 11/29 [00:31<00:53,  2.99s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  38% 11/29 [00:32<00:50,  2.81s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  41% 12/29 [00:33<00:51,  3.04s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  41% 12/29 [00:33<00:45,  2.69s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  41% 12/29 [00:34<00:43,  2.57s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  45% 13/29 [00:35<00:43,  2.75s/ba]\n","Applying counterfactual augmentation for race #1:  45% 13/29 [00:35<00:37,  2.36s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  45% 13/29 [00:35<00:41,  2.58s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  52% 15/29 [00:36<00:32,  2.32s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  48% 14/29 [00:40<00:45,  3.03s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  48% 14/29 [00:40<00:50,  3.34s/ba]\n","\n","Applying counterfactual augmentation for race #2:  55% 16/29 [00:40<00:35,  2.75s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  52% 15/29 [00:43<00:42,  3.02s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #0:  52% 15/29 [00:43<00:46,  3.34s/ba]\n","\n","Applying counterfactual augmentation for race #2:  59% 17/29 [00:44<00:36,  3.00s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  55% 16/29 [00:45<00:38,  2.93s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  55% 16/29 [00:46<00:38,  2.93s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  55% 16/29 [00:46<00:41,  3.20s/ba]\n","Applying counterfactual augmentation for race #1:  59% 17/29 [00:48<00:31,  2.66s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  66% 19/29 [00:48<00:25,  2.54s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #0:  59% 17/29 [00:49<00:36,  3.04s/ba]\n","\n","Applying counterfactual augmentation for race #2:  69% 20/29 [00:50<00:22,  2.54s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  62% 18/29 [00:50<00:29,  2.70s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #0:  62% 18/29 [00:51<00:31,  2.85s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  66% 19/29 [00:52<00:25,  2.54s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  72% 21/29 [00:53<00:19,  2.45s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  66% 19/29 [00:53<00:26,  2.63s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  69% 20/29 [00:55<00:21,  2.43s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  76% 22/29 [00:55<00:16,  2.36s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  69% 20/29 [00:57<00:25,  2.82s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  72% 21/29 [00:58<00:22,  2.86s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  79% 23/29 [00:59<00:17,  2.88s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  72% 21/29 [01:00<00:24,  3.07s/ba]\n","\n","Applying counterfactual augmentation for race #2:  83% 24/29 [01:02<00:14,  2.95s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  76% 22/29 [01:02<00:21,  3.10s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  76% 22/29 [01:03<00:21,  3.04s/ba]\n","\n","Applying counterfactual augmentation for race #2:  86% 25/29 [01:05<00:11,  3.00s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  79% 23/29 [01:05<00:18,  3.06s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  79% 23/29 [01:06<00:18,  3.01s/ba]\n","\n","Applying counterfactual augmentation for race #2:  90% 26/29 [01:07<00:08,  2.74s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  83% 24/29 [01:07<00:13,  2.77s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  83% 24/29 [01:08<00:13,  2.76s/ba]\n","\n","Applying counterfactual augmentation for race #2:  93% 27/29 [01:09<00:05,  2.62s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  86% 25/29 [01:10<00:10,  2.64s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  86% 25/29 [01:11<00:10,  2.70s/ba]\n","\n","Applying counterfactual augmentation for race #2:  97% 28/29 [01:12<00:02,  2.44s/ba]\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2: 100% 29/29 [01:12<00:00,  2.49s/ba]\n","\n","\n","\n","Applying counterfactual augmentation for race #3:  90% 26/29 [01:12<00:07,  2.64s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  93% 27/29 [01:14<00:04,  2.08s/ba]\n","Applying counterfactual augmentation for race #1:  93% 27/29 [01:14<00:04,  2.21s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  93% 27/29 [01:14<00:04,  2.41s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  97% 28/29 [01:16<00:02,  2.23s/ba]\u001b[A\n","Applying counterfactual augmentation for race #1: 100% 29/29 [01:17<00:00,  2.66s/ba]\n","\n","\n","\n","Applying counterfactual augmentation for race #3:  97% 28/29 [01:17<00:02,  2.45s/ba]\u001b[A\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3: 100% 29/29 [01:17<00:00,  2.66s/ba]\n","Applying counterfactual augmentation for race #0: 100% 29/29 [01:17<00:00,  2.68s/ba]\n","Applying counterfactual augmentation for race #0:   0% 0/2 [00:00<?, ?ba/s]\n","Applying counterfactual augmentation for race #1:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\n","\n","Applying counterfactual augmentation for race #2:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  50% 1/2 [00:02<00:02,  2.16s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  50% 1/2 [00:03<00:03,  3.30s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  50% 1/2 [00:03<00:03,  3.17s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2: 100% 2/2 [00:03<00:00,  1.70s/ba]\n","\n","Applying counterfactual augmentation for race #1: 100% 2/2 [00:03<00:00,  1.85s/ba]\n","Applying counterfactual augmentation for race #0: 100% 2/2 [00:04<00:00,  2.14s/ba]\n","\n","\n","\n","Applying counterfactual augmentation for race #3: 100% 2/2 [00:04<00:00,  2.09s/ba]\n","Applying counterfactual augmentation for religion #0:   0% 0/29 [00:00<?, ?ba/s]\n","Applying counterfactual augmentation for religion #1:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:   3% 1/29 [00:02<01:00,  2.18s/ba]\n","Applying counterfactual augmentation for religion #1:   3% 1/29 [00:02<01:06,  2.37s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:   3% 1/29 [00:02<01:06,  2.38s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:   7% 2/29 [00:04<00:53,  1.99s/ba]\n","Applying counterfactual augmentation for religion #1:   7% 2/29 [00:04<01:00,  2.25s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:   7% 2/29 [00:04<01:00,  2.24s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  10% 3/29 [00:06<00:56,  2.19s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  10% 3/29 [00:06<00:57,  2.22s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  10% 3/29 [00:07<01:02,  2.39s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  14% 4/29 [00:08<00:54,  2.17s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  14% 4/29 [00:08<00:53,  2.14s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  17% 5/29 [00:09<00:44,  1.84s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #0:  17% 5/29 [00:10<00:49,  2.04s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  17% 5/29 [00:10<00:51,  2.14s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  17% 5/29 [00:12<01:00,  2.50s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  21% 6/29 [00:15<01:09,  3.02s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  21% 6/29 [00:15<01:10,  3.05s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  24% 7/29 [00:16<01:01,  2.80s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #0:  24% 7/29 [00:18<01:09,  3.16s/ba]\n","\n","Applying counterfactual augmentation for religion #2:  28% 8/29 [00:18<00:58,  2.77s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  24% 7/29 [00:19<01:10,  3.21s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #0:  28% 8/29 [00:21<01:00,  2.90s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  28% 8/29 [00:21<00:58,  2.76s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  31% 9/29 [00:21<00:54,  2.72s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  28% 8/29 [00:22<01:05,  3.13s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #0:  31% 9/29 [00:23<00:54,  2.70s/ba]\n","\n","Applying counterfactual augmentation for religion #2:  34% 10/29 [00:24<00:52,  2.78s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #0:  34% 10/29 [00:25<00:48,  2.54s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  34% 10/29 [00:25<00:47,  2.48s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  38% 11/29 [00:26<00:44,  2.50s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #0:  38% 11/29 [00:28<00:45,  2.55s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  38% 11/29 [00:28<00:45,  2.53s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  41% 12/29 [00:28<00:40,  2.39s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  38% 11/29 [00:28<00:41,  2.32s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #0:  41% 12/29 [00:30<00:44,  2.62s/ba]\n","Applying counterfactual augmentation for religion #1:  41% 12/29 [00:30<00:39,  2.32s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  45% 13/29 [00:31<00:39,  2.49s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #0:  45% 13/29 [00:34<00:44,  2.80s/ba]\n","Applying counterfactual augmentation for religion #1:  45% 13/29 [00:34<00:42,  2.65s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  48% 14/29 [00:34<00:41,  2.75s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #0:  48% 14/29 [00:37<00:45,  3.01s/ba]\n","\n","Applying counterfactual augmentation for religion #2:  52% 15/29 [00:37<00:39,  2.82s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  48% 14/29 [00:37<00:42,  2.86s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  52% 15/29 [00:39<00:40,  2.89s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  52% 15/29 [00:40<00:41,  2.97s/ba]\n","Applying counterfactual augmentation for religion #1:  52% 15/29 [00:40<00:39,  2.85s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  55% 16/29 [00:41<00:34,  2.68s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  55% 16/29 [00:42<00:33,  2.57s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  55% 16/29 [00:42<00:36,  2.78s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  59% 17/29 [00:43<00:29,  2.47s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  62% 18/29 [00:44<00:26,  2.40s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #0:  59% 17/29 [00:45<00:32,  2.70s/ba]\n","\n","Applying counterfactual augmentation for religion #2:  66% 19/29 [00:46<00:22,  2.27s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  62% 18/29 [00:46<00:27,  2.50s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #0:  62% 18/29 [00:47<00:27,  2.52s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  66% 19/29 [00:48<00:22,  2.28s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  66% 19/29 [00:49<00:23,  2.34s/ba]\n","Applying counterfactual augmentation for religion #1:  66% 19/29 [00:49<00:24,  2.47s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  69% 20/29 [00:50<00:20,  2.23s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  69% 20/29 [00:52<00:24,  2.68s/ba]\n","Applying counterfactual augmentation for religion #1:  69% 20/29 [00:52<00:24,  2.76s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  72% 21/29 [00:54<00:21,  2.68s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  72% 21/29 [00:56<00:23,  2.90s/ba]\n","Applying counterfactual augmentation for religion #1:  72% 21/29 [00:56<00:24,  3.02s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  76% 22/29 [00:57<00:20,  2.94s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  76% 22/29 [00:59<00:21,  3.12s/ba]\n","Applying counterfactual augmentation for religion #1:  76% 22/29 [01:00<00:24,  3.43s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  79% 23/29 [01:01<00:19,  3.28s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  79% 23/29 [01:04<00:20,  3.49s/ba]\n","Applying counterfactual augmentation for religion #1:  79% 23/29 [01:05<00:21,  3.63s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  83% 24/29 [01:05<00:16,  3.36s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  83% 24/29 [01:08<00:18,  3.61s/ba]\n","Applying counterfactual augmentation for religion #1:  83% 24/29 [01:09<00:18,  3.79s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  90% 26/29 [01:09<00:10,  3.45s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #0:  86% 25/29 [01:11<00:14,  3.62s/ba]\n","Applying counterfactual augmentation for religion #1:  86% 25/29 [01:12<00:14,  3.59s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  90% 26/29 [01:13<00:11,  3.82s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  90% 26/29 [01:15<00:10,  3.60s/ba]\n","Applying counterfactual augmentation for religion #1:  90% 26/29 [01:17<00:11,  3.93s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  97% 28/29 [01:17<00:03,  3.84s/ba]\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #2: 100% 29/29 [01:18<00:00,  2.70s/ba]\n","\n","\n","\n","Applying counterfactual augmentation for religion #0:  93% 27/29 [01:18<00:06,  3.47s/ba]\n","Applying counterfactual augmentation for religion #1:  93% 27/29 [01:19<00:07,  3.58s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  97% 28/29 [01:20<00:03,  3.52s/ba]\u001b[A\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3: 100% 29/29 [01:20<00:00,  2.79s/ba]\n","Applying counterfactual augmentation for religion #0: 100% 29/29 [01:21<00:00,  2.81s/ba]\n","\n","Applying counterfactual augmentation for religion #1:  97% 28/29 [01:21<00:03,  3.09s/ba]\u001b[A\n","Applying counterfactual augmentation for religion #1: 100% 29/29 [01:21<00:00,  2.82s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #0:   0% 0/2 [00:00<?, ?ba/s]\n","Applying counterfactual augmentation for religion #1:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  50% 1/2 [00:01<00:01,  1.55s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  50% 1/2 [00:02<00:02,  2.07s/ba]\n","\n","Applying counterfactual augmentation for religion #2: 100% 2/2 [00:02<00:00,  1.11s/ba]\n","\n","\n","\n","Applying counterfactual augmentation for religion #3:  50% 1/2 [00:02<00:02,  2.09s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1: 100% 2/2 [00:02<00:00,  1.19s/ba]\n","Applying counterfactual augmentation for religion #0: 100% 2/2 [00:02<00:00,  1.36s/ba]\n","\n","\n","\n","Applying counterfactual augmentation for religion #3: 100% 2/2 [00:02<00:00,  1.32s/ba]\n","<class 'datasets.dataset_dict.DatasetDict'>\n","<class 'datasets.dataset_dict.DatasetDict'>\n","<class 'datasets.dataset_dict.DatasetDict'>\n","Grouping texts in chunks of 512 #0:   0% 0/19 [00:00<?, ?ba/s]\n","Grouping texts in chunks of 512 #1:   0% 0/19 [00:00<?, ?ba/s]\u001b[A\n","\n","Grouping texts in chunks of 512 #2:   0% 0/19 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #3:   0% 0/19 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0:   5% 1/19 [00:15<04:30, 15.01s/ba]\n","\n","Grouping texts in chunks of 512 #2:   5% 1/19 [00:15<04:39, 15.53s/ba]\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #0:  11% 2/19 [00:25<03:33, 12.56s/ba]\n","\n","\n","Grouping texts in chunks of 512 #3:  11% 2/19 [00:26<03:39, 12.91s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Grouping texts in chunks of 512 #2:  11% 2/19 [00:26<03:39, 12.90s/ba]\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #0:  16% 3/19 [00:38<03:19, 12.48s/ba]\n","\n","\n","Grouping texts in chunks of 512 #3:  16% 3/19 [00:41<03:42, 13.88s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Grouping texts in chunks of 512 #2:  16% 3/19 [00:41<03:40, 13.77s/ba]\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #0:  21% 4/19 [00:48<02:55, 11.71s/ba]\n","\n","\n","Grouping texts in chunks of 512 #3:  21% 4/19 [00:52<03:10, 12.70s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Grouping texts in chunks of 512 #2:  21% 4/19 [00:54<03:20, 13.38s/ba]\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #0:  26% 5/19 [01:02<02:56, 12.60s/ba]\n","\n","\n","Grouping texts in chunks of 512 #3:  26% 5/19 [01:03<02:49, 12.09s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Grouping texts in chunks of 512 #2:  26% 5/19 [01:06<03:02, 13.03s/ba]\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #1:  26% 5/19 [01:07<03:01, 13.00s/ba]\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0:  32% 6/19 [01:18<02:57, 13.64s/ba]\n","\n","Grouping texts in chunks of 512 #2:  32% 6/19 [01:19<02:48, 12.98s/ba]\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #1:  32% 6/19 [01:21<02:54, 13.39s/ba]\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0:  37% 7/19 [01:30<02:35, 12.98s/ba]\n","\n","Grouping texts in chunks of 512 #2:  37% 7/19 [01:33<02:38, 13.23s/ba]\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #1:  37% 7/19 [01:34<02:38, 13.22s/ba]\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0:  42% 8/19 [01:40<02:12, 12.01s/ba]\n","\n","Grouping texts in chunks of 512 #2:  42% 8/19 [01:43<02:16, 12.37s/ba]\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #3:  47% 9/19 [01:45<01:48, 10.88s/ba]\u001b[A\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #0:  47% 9/19 [01:55<02:10, 13.06s/ba]\n","\n","\n","Grouping texts in chunks of 512 #3:  53% 10/19 [01:58<01:43, 11.46s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Grouping texts in chunks of 512 #2:  47% 9/19 [01:59<02:13, 13.38s/ba]\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #0:  53% 10/19 [02:09<01:59, 13.24s/ba]\n","\n","\n","Grouping texts in chunks of 512 #3:  58% 11/19 [02:10<01:32, 11.61s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Grouping texts in chunks of 512 #2:  53% 10/19 [02:11<01:57, 13.07s/ba]\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #1:  53% 10/19 [02:19<02:10, 14.51s/ba]\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0:  58% 11/19 [02:23<01:47, 13.47s/ba]\n","\n","Grouping texts in chunks of 512 #2:  58% 11/19 [02:24<01:44, 13.12s/ba]\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #1:  58% 11/19 [02:35<02:01, 15.16s/ba]\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #3:  68% 13/19 [02:36<01:14, 12.34s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Grouping texts in chunks of 512 #0:  63% 12/19 [02:42<01:47, 15.34s/ba]\n","\n","\n","Grouping texts in chunks of 512 #3:  74% 14/19 [02:44<00:55, 11.13s/ba]\u001b[A\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #1:  63% 12/19 [02:46<01:37, 13.87s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #2:  68% 13/19 [02:55<01:24, 14.12s/ba]\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0:  68% 13/19 [02:56<01:29, 14.84s/ba]\n","Grouping texts in chunks of 512 #1:  68% 13/19 [02:59<01:20, 13.44s/ba]\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #3:  84% 16/19 [03:02<00:29,  9.98s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Grouping texts in chunks of 512 #0:  74% 14/19 [03:08<01:09, 13.87s/ba]\n","Grouping texts in chunks of 512 #1:  74% 14/19 [03:11<01:05, 13.16s/ba]\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #3:  89% 17/19 [03:14<00:20, 10.40s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Grouping texts in chunks of 512 #2:  79% 15/19 [03:18<00:51, 12.76s/ba]\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #0:  79% 15/19 [03:21<00:54, 13.74s/ba]\n","\n","\n","Grouping texts in chunks of 512 #3:  95% 18/19 [03:23<00:09,  9.96s/ba]\u001b[A\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #3: 100% 19/19 [03:27<00:00, 10.92s/ba]\n","\n","\n","Grouping texts in chunks of 512 #0:  84% 16/19 [03:33<00:39, 13.27s/ba]\n","Grouping texts in chunks of 512 #1:  84% 16/19 [03:33<00:36, 12.31s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #2:  89% 17/19 [03:39<00:22, 11.49s/ba]\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #0:  89% 17/19 [03:42<00:23, 11.99s/ba]\n","\n","Grouping texts in chunks of 512 #2:  95% 18/19 [03:50<00:11, 11.48s/ba]\u001b[A\u001b[A\n","\n","Grouping texts in chunks of 512 #2: 100% 19/19 [03:53<00:00, 12.27s/ba]\n","\n","Grouping texts in chunks of 512 #0:  95% 18/19 [03:54<00:11, 11.91s/ba]\n","Grouping texts in chunks of 512 #1: 100% 19/19 [03:55<00:00, 12.39s/ba]\n","Grouping texts in chunks of 512 #0: 100% 19/19 [03:56<00:00, 12.42s/ba]\n","Grouping texts in chunks of 512 #0:   0% 0/1 [00:00<?, ?ba/s]\n","Grouping texts in chunks of 512 #1:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\n","\n","Grouping texts in chunks of 512 #2:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #3:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #3: 100% 1/1 [00:07<00:00,  7.42s/ba]\n","\n","\n","Grouping texts in chunks of 512 #2: 100% 1/1 [00:07<00:00,  7.63s/ba]\n","\n","Grouping texts in chunks of 512 #1: 100% 1/1 [00:08<00:00,  8.74s/ba]\n","Grouping texts in chunks of 512 #0: 100% 1/1 [00:08<00:00,  8.79s/ba]\n","[INFO|trainer.py:420] 2023-08-14 22:35:09,997 >> max_steps is given, it will override any value given in num_train_epochs\n","[INFO|trainer.py:553] 2023-08-14 22:35:09,997 >> The following columns in the training set  don't have a corresponding argument in `AlbertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","[INFO|trainer.py:1244] 2023-08-14 22:35:10,007 >> ***** Running training *****\n","[INFO|trainer.py:1245] 2023-08-14 22:35:10,007 >>   Num examples = 61711\n","[INFO|trainer.py:1246] 2023-08-14 22:35:10,007 >>   Num Epochs = 1\n","[INFO|trainer.py:1247] 2023-08-14 22:35:10,007 >>   Instantaneous batch size per device = 16\n","[INFO|trainer.py:1248] 2023-08-14 22:35:10,007 >>   Total train batch size (w. parallel, distributed & accumulation) = 128\n","[INFO|trainer.py:1249] 2023-08-14 22:35:10,007 >>   Gradient Accumulation steps = 8\n","[INFO|trainer.py:1250] 2023-08-14 22:35:10,007 >>   Total optimization steps = 200\n"," 25% 50/200 [10:42<32:21, 12.95s/it][INFO|trainer.py:2090] 2023-08-14 22:45:52,553 >> Saving model checkpoint to ./checkpoints/cda123_200/checkpoint-50\n","[INFO|configuration_utils.py:430] 2023-08-14 22:45:52,558 >> Configuration saved in ./checkpoints/cda123_200/checkpoint-50/config.json\n","[INFO|modeling_utils.py:1074] 2023-08-14 22:45:52,663 >> Model weights saved in ./checkpoints/cda123_200/checkpoint-50/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2074] 2023-08-14 22:45:52,668 >> tokenizer config file saved in ./checkpoints/cda123_200/checkpoint-50/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2080] 2023-08-14 22:45:52,671 >> Special tokens file saved in ./checkpoints/cda123_200/checkpoint-50/special_tokens_map.json\n"," 50% 100/200 [21:30<21:33, 12.93s/it][INFO|trainer.py:2090] 2023-08-14 22:56:40,610 >> Saving model checkpoint to ./checkpoints/cda123_200/checkpoint-100\n","[INFO|configuration_utils.py:430] 2023-08-14 22:56:40,615 >> Configuration saved in ./checkpoints/cda123_200/checkpoint-100/config.json\n","[INFO|modeling_utils.py:1074] 2023-08-14 22:56:40,769 >> Model weights saved in ./checkpoints/cda123_200/checkpoint-100/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2074] 2023-08-14 22:56:40,774 >> tokenizer config file saved in ./checkpoints/cda123_200/checkpoint-100/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2080] 2023-08-14 22:56:40,777 >> Special tokens file saved in ./checkpoints/cda123_200/checkpoint-100/special_tokens_map.json\n"," 75% 150/200 [32:18<10:46, 12.93s/it][INFO|trainer.py:2090] 2023-08-14 23:07:28,425 >> Saving model checkpoint to ./checkpoints/cda123_200/checkpoint-150\n","[INFO|configuration_utils.py:430] 2023-08-14 23:07:28,431 >> Configuration saved in ./checkpoints/cda123_200/checkpoint-150/config.json\n","[INFO|modeling_utils.py:1074] 2023-08-14 23:07:28,557 >> Model weights saved in ./checkpoints/cda123_200/checkpoint-150/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2074] 2023-08-14 23:07:28,562 >> tokenizer config file saved in ./checkpoints/cda123_200/checkpoint-150/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2080] 2023-08-14 23:07:28,566 >> Special tokens file saved in ./checkpoints/cda123_200/checkpoint-150/special_tokens_map.json\n","100% 200/200 [43:05<00:00, 12.91s/it][INFO|trainer.py:2090] 2023-08-14 23:18:15,740 >> Saving model checkpoint to ./checkpoints/cda123_200/checkpoint-200\n","[INFO|configuration_utils.py:430] 2023-08-14 23:18:15,745 >> Configuration saved in ./checkpoints/cda123_200/checkpoint-200/config.json\n","[INFO|modeling_utils.py:1074] 2023-08-14 23:18:15,848 >> Model weights saved in ./checkpoints/cda123_200/checkpoint-200/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2074] 2023-08-14 23:18:15,852 >> tokenizer config file saved in ./checkpoints/cda123_200/checkpoint-200/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2080] 2023-08-14 23:18:15,869 >> Special tokens file saved in ./checkpoints/cda123_200/checkpoint-200/special_tokens_map.json\n","[INFO|trainer.py:1473] 2023-08-14 23:18:16,177 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 2586.1702, 'train_samples_per_second': 9.899, 'train_steps_per_second': 0.077, 'train_loss': 2.1160003662109377, 'epoch': 0.41}\n","100% 200/200 [43:06<00:00, 12.93s/it]\n","[INFO|trainer.py:2090] 2023-08-14 23:18:16,183 >> Saving model checkpoint to ./checkpoints/cda123_200\n","[INFO|configuration_utils.py:430] 2023-08-14 23:18:16,189 >> Configuration saved in ./checkpoints/cda123_200/config.json\n","[INFO|modeling_utils.py:1074] 2023-08-14 23:18:16,368 >> Model weights saved in ./checkpoints/cda123_200/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2074] 2023-08-14 23:18:16,373 >> tokenizer config file saved in ./checkpoints/cda123_200/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2080] 2023-08-14 23:18:16,379 >> Special tokens file saved in ./checkpoints/cda123_200/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =       0.41\n","  train_loss               =      2.116\n","  train_runtime            = 0:43:06.17\n","  train_samples            =      61711\n","  train_samples_per_second =      9.899\n","  train_steps_per_second   =      0.077\n"]}]},{"cell_type":"code","source":["# albert在小规模数据集上进行gender_race_religion 200步CDA去偏之后 --bias_type gender\n","\n","! python experiments/crows_debias.py --model CDAAlbertForMaskedLM --load_path ./checkpoints/cda123_200 --model_name_or_path albert-base-v2 --bias_type gender"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sjocJza9WMEx","executionInfo":{"status":"ok","timestamp":1692055255339,"user_tz":-60,"elapsed":23058,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"55703ff2-4bea-472f-e50b-2aa3c0e4bf49"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: CDAAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: None\n"," - load_path: ./checkpoints/cda123_200\n"," - bias_type: gender\n","Evaluating gender examples.\n","100% 262/262 [00:12<00:00, 20.66it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 51.91\n","Stereotype score: 49.69\n","Anti-stereotype score: 55.34\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 51.91\n"]}]},{"cell_type":"code","source":["# albert在小规模数据集上进行gender_race_religion 200步CDA去偏之后 --bias_type race\n","\n","! python experiments/crows_debias.py --model CDAAlbertForMaskedLM --load_path ./checkpoints/cda123_200 --model_name_or_path albert-base-v2 --bias_type race"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"slP_VPajWMPw","executionInfo":{"status":"ok","timestamp":1692055207285,"user_tz":-60,"elapsed":32884,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"132da822-9d40-4ff8-f814-b52e2082c19d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: CDAAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: None\n"," - load_path: ./checkpoints/cda123_200\n"," - bias_type: race\n","Evaluating race-color examples.\n"," 70% 360/516 [00:15<00:04, 34.87it/s]Skipping example 363.\n","100% 515/516 [00:20<00:00, 24.65it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 57.86\n","Stereotype score: 59.32\n","Anti-stereotype score: 41.86\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 57.86\n"]}]},{"cell_type":"code","source":["# albert在小规模数据集上进行gender_race_religion 200步CDA去偏之后 --bias_type religion\n","\n","! python experiments/crows_debias.py --model CDAAlbertForMaskedLM --load_path ./checkpoints/cda123_200 --model_name_or_path albert-base-v2 --bias_type religion"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YhDD9KRvIlc_","executionInfo":{"status":"ok","timestamp":1692055166955,"user_tz":-60,"elapsed":13710,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"52c60e6a-bc53-40f4-bdda-f797bd8b1101"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: CDAAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: None\n"," - load_path: ./checkpoints/cda123_200\n"," - bias_type: religion\n","Evaluating religion examples.\n","100% 105/105 [00:04<00:00, 26.10it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 52.38\n","Stereotype score: 52.53\n","Anti-stereotype score: 50.0\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 52.38\n"]}]},{"cell_type":"markdown","source":["### tiny data 200 mix  "],"metadata":{"id":"7bLA6keE-etD"}},{"cell_type":"code","source":["\n","experiment_id=\"cda_c-albert-base-v2\"\n","!python experiments/run_mlm.py \\\n","                    --model_name_or_path \"albert-base-v2\" \\\n","                    --do_train \\\n","                    --train_file \"./data/text/wikipedia-2.txt\" \\\n","                    --max_steps 200 \\\n","                    --per_device_train_batch_size 16 \\\n","                    --gradient_accumulation_steps 8 \\\n","                    --max_seq_length 512 \\\n","                    --save_steps 50 \\\n","                    --preprocessing_num_workers 4 \\\n","                    --counterfactual_augmentation \"gender\" \\\n","                    --seed 0 \\\n","                    --output_dir \"./checkpoints/cda_random_200\"\n","\n","\n"],"metadata":{"id":"pgz3gp5YHR2G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692641269259,"user_tz":-60,"elapsed":3254399,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"129ed843-97bd-46c2-fbb9-2307b04bf65c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-08-21 17:13:30.624971: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-08-21 17:13:31.625527: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","08/21/2023 17:13:34 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","08/21/2023 17:13:34 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=False,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=8,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=./checkpoints/cda_random_200/runs/Aug21_17-13-34_33ff6e54ac6c,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=200,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=./checkpoints/cda_random_200,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=./checkpoints/cda_random_200,\n","save_on_each_node=False,\n","save_steps=50,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=0,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","08/21/2023 17:13:34 - WARNING - datasets.builder - Using custom data configuration default-0de1c001dd32c133\n","08/21/2023 17:13:34 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","08/21/2023 17:13:34 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4\n","08/21/2023 17:13:34 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)\n","08/21/2023 17:13:34 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4\n","100% 1/1 [00:00<00:00, 640.84it/s]\n","08/21/2023 17:13:34 - WARNING - datasets.builder - Using custom data configuration default-0de1c001dd32c133\n","08/21/2023 17:13:34 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","08/21/2023 17:13:34 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4\n","08/21/2023 17:13:34 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)\n","08/21/2023 17:13:34 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4\n","08/21/2023 17:13:34 - WARNING - datasets.builder - Using custom data configuration default-0de1c001dd32c133\n","08/21/2023 17:13:34 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","08/21/2023 17:13:34 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4\n","08/21/2023 17:13:34 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)\n","08/21/2023 17:13:34 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4\n","[INFO|configuration_utils.py:644] 2023-08-21 17:13:35,024 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:680] 2023-08-21 17:13:35,024 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|tokenization_auto.py:336] 2023-08-21 17:13:35,443 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:644] 2023-08-21 17:13:35,579 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:680] 2023-08-21 17:13:35,579 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|tokenization_utils_base.py:1771] 2023-08-21 17:13:36,899 >> loading file https://huggingface.co/albert-base-v2/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|tokenization_utils_base.py:1771] 2023-08-21 17:13:36,899 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|tokenization_utils_base.py:1771] 2023-08-21 17:13:36,899 >> loading file https://huggingface.co/albert-base-v2/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1771] 2023-08-21 17:13:36,900 >> loading file https://huggingface.co/albert-base-v2/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1771] 2023-08-21 17:13:36,900 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:644] 2023-08-21 17:13:37,035 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:680] 2023-08-21 17:13:37,036 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|modeling_utils.py:1427] 2023-08-21 17:13:37,246 >> loading weights file https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[INFO|modeling_utils.py:1694] 2023-08-21 17:13:37,396 >> All model checkpoint weights were used when initializing AlbertForMaskedLM.\n","\n","[INFO|modeling_utils.py:1702] 2023-08-21 17:13:37,396 >> All the weights of AlbertForMaskedLM were initialized from the model checkpoint at albert-base-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForMaskedLM for predictions without further training.\n","08/21/2023 17:13:37 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_function at 0x7eebb9aca560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n","Running tokenizer on every text in dataset #0:   0% 0/29 [00:00<?, ?ba/s]\n","Running tokenizer on every text in dataset #1:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A[WARNING|tokenization_utils_base.py:3377] 2023-08-21 17:13:38,387 >> Token indices sequence length is longer than the specified maximum sequence length for this model (589 > 512). Running this sequence through the model will result in indexing errors\n","Running tokenizer on every text in dataset #0:   3% 1/29 [00:00<00:27,  1.03ba/s][WARNING|tokenization_utils_base.py:3377] 2023-08-21 17:13:38,632 >> Token indices sequence length is longer than the specified maximum sequence length for this model (605 > 512). Running this sequence through the model will result in indexing errors\n","\n","\n","\n","Running tokenizer on every text in dataset #3:   3% 1/29 [00:00<00:26,  1.04ba/s]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #2:   3% 1/29 [00:01<00:31,  1.13s/ba]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:   7% 2/29 [00:02<00:38,  1.44s/ba][WARNING|tokenization_utils_base.py:3377] 2023-08-21 17:13:40,366 >> Token indices sequence length is longer than the specified maximum sequence length for this model (916 > 512). Running this sequence through the model will result in indexing errors\n","\n","\n","Running tokenizer on every text in dataset #2:   7% 2/29 [00:02<00:37,  1.39s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:   7% 2/29 [00:02<00:38,  1.44s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:   7% 2/29 [00:03<00:46,  1.72s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  10% 3/29 [00:04<00:44,  1.71s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  10% 3/29 [00:04<00:43,  1.68s/ba]\u001b[A\u001b[A\u001b[A[WARNING|tokenization_utils_base.py:3377] 2023-08-21 17:13:42,902 >> Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n","\n","Running tokenizer on every text in dataset #1:  10% 3/29 [00:05<00:48,  1.87s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  14% 4/29 [00:05<00:36,  1.47s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  14% 4/29 [00:06<00:44,  1.77s/ba]\n","Running tokenizer on every text in dataset #1:  14% 4/29 [00:07<00:45,  1.81s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  17% 5/29 [00:07<00:35,  1.48s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  17% 5/29 [00:08<00:39,  1.64s/ba]\n","Running tokenizer on every text in dataset #1:  17% 5/29 [00:08<00:41,  1.73s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  21% 6/29 [00:09<00:37,  1.64s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  21% 6/29 [00:09<00:37,  1.64s/ba]\n","Running tokenizer on every text in dataset #1:  21% 6/29 [00:10<00:39,  1.73s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  24% 7/29 [00:10<00:34,  1.55s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  24% 7/29 [00:11<00:33,  1.54s/ba]\n","Running tokenizer on every text in dataset #1:  24% 7/29 [00:11<00:33,  1.51s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  28% 8/29 [00:11<00:28,  1.36s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  28% 8/29 [00:12<00:28,  1.37s/ba]\n","Running tokenizer on every text in dataset #1:  28% 8/29 [00:12<00:27,  1.33s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  31% 9/29 [00:12<00:24,  1.21s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  31% 9/29 [00:13<00:26,  1.30s/ba]\n","Running tokenizer on every text in dataset #1:  31% 9/29 [00:13<00:24,  1.23s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  34% 10/29 [00:13<00:23,  1.22s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  34% 10/29 [00:14<00:22,  1.21s/ba]\n","Running tokenizer on every text in dataset #1:  34% 10/29 [00:14<00:21,  1.14s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  38% 11/29 [00:14<00:20,  1.13s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  38% 11/29 [00:15<00:21,  1.20s/ba]\n","Running tokenizer on every text in dataset #1:  38% 11/29 [00:15<00:20,  1.13s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  41% 12/29 [00:15<00:18,  1.09s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  41% 12/29 [00:16<00:20,  1.20s/ba]\n","Running tokenizer on every text in dataset #1:  41% 12/29 [00:16<00:18,  1.09s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  45% 13/29 [00:16<00:17,  1.06s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  45% 13/29 [00:16<00:17,  1.07s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  45% 13/29 [00:17<00:18,  1.13s/ba]\n","\n","Running tokenizer on every text in dataset #2:  48% 14/29 [00:17<00:15,  1.05s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  48% 14/29 [00:17<00:15,  1.03s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  48% 14/29 [00:18<00:16,  1.10s/ba]\n","\n","Running tokenizer on every text in dataset #2:  52% 15/29 [00:18<00:14,  1.04s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  52% 15/29 [00:18<00:14,  1.04s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  52% 15/29 [00:19<00:15,  1.12s/ba]\n","\n","Running tokenizer on every text in dataset #2:  55% 16/29 [00:19<00:13,  1.05s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  55% 16/29 [00:19<00:13,  1.04s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  55% 16/29 [00:21<00:15,  1.20s/ba]\n","\n","Running tokenizer on every text in dataset #2:  59% 17/29 [00:22<00:19,  1.60s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  59% 17/29 [00:22<00:19,  1.60s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:  59% 17/29 [00:23<00:19,  1.63s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  59% 17/29 [00:25<00:24,  2.07s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  62% 18/29 [00:25<00:20,  1.83s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:  62% 18/29 [00:25<00:20,  1.86s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  62% 18/29 [00:26<00:20,  1.89s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  66% 19/29 [00:26<00:17,  1.75s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:  66% 19/29 [00:27<00:18,  1.82s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  66% 19/29 [00:28<00:17,  1.73s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  69% 20/29 [00:28<00:15,  1.68s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  72% 21/29 [00:29<00:12,  1.56s/ba]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  69% 20/29 [00:29<00:15,  1.67s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  72% 21/29 [00:29<00:12,  1.57s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  76% 22/29 [00:30<00:09,  1.41s/ba]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  72% 21/29 [00:30<00:11,  1.48s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  76% 22/29 [00:30<00:10,  1.47s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  76% 22/29 [00:31<00:09,  1.30s/ba]\n","Running tokenizer on every text in dataset #1:  76% 22/29 [00:31<00:10,  1.44s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  79% 23/29 [00:31<00:08,  1.35s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  83% 24/29 [00:31<00:05,  1.15s/ba]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  79% 23/29 [00:32<00:07,  1.25s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  83% 24/29 [00:32<00:06,  1.24s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  83% 24/29 [00:33<00:05,  1.16s/ba]\n","Running tokenizer on every text in dataset #1:  83% 24/29 [00:33<00:06,  1.26s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  86% 25/29 [00:33<00:04,  1.16s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  90% 26/29 [00:34<00:03,  1.10s/ba]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  86% 25/29 [00:34<00:04,  1.18s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  90% 26/29 [00:34<00:03,  1.14s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  90% 26/29 [00:35<00:03,  1.11s/ba]\n","Running tokenizer on every text in dataset #1:  90% 26/29 [00:35<00:03,  1.23s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  97% 28/29 [00:35<00:01,  1.03s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  93% 27/29 [00:35<00:02,  1.15s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #2: 100% 29/29 [00:36<00:00,  1.24s/ba]\n","Running tokenizer on every text in dataset #0:  93% 27/29 [00:36<00:01,  1.04ba/s]\n","Running tokenizer on every text in dataset #1:  93% 27/29 [00:36<00:02,  1.07s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3: 100% 29/29 [00:37<00:00,  1.28s/ba]\n","Running tokenizer on every text in dataset #0: 100% 29/29 [00:37<00:00,  1.29s/ba]\n","\n","Running tokenizer on every text in dataset #1: 100% 29/29 [00:37<00:00,  1.29s/ba]\n","Running tokenizer on every text in dataset #0:   0% 0/2 [00:00<?, ?ba/s]\n","Running tokenizer on every text in dataset #1:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  50% 1/2 [00:00<00:00,  1.36ba/s]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  50% 1/2 [00:00<00:00,  1.02ba/s]\n","\n","Running tokenizer on every text in dataset #2: 100% 2/2 [00:00<00:00,  2.18ba/s]\n","\n","\n","\n","Running tokenizer on every text in dataset #3:  50% 1/2 [00:00<00:00,  1.08ba/s]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1: 100% 2/2 [00:01<00:00,  1.63ba/s]\n","Running tokenizer on every text in dataset #0: 100% 2/2 [00:01<00:00,  1.36ba/s]\n","\n","\n","\n","Running tokenizer on every text in dataset #3: 100% 2/2 [00:01<00:00,  1.52ba/s]\n","08/21/2023 17:14:16 - INFO - __main__ - Applying gender CDA.\n","Applying counterfactual augmentation for gender #0:   0% 0/29 [00:00<?, ?ba/s]\n","Applying counterfactual augmentation for gender #1:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #0:   3% 1/29 [00:12<05:51, 12.54s/ba]\n","\n","Applying counterfactual augmentation for gender #2:   3% 1/29 [00:12<05:52, 12.60s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:   3% 1/29 [00:13<06:29, 13.90s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:   3% 1/29 [00:14<06:57, 14.91s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:   7% 2/29 [00:23<05:16, 11.73s/ba]\n","Applying counterfactual augmentation for gender #1:   7% 2/29 [00:26<06:01, 13.38s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:   7% 2/29 [00:27<06:10, 13.73s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  10% 3/29 [00:32<04:31, 10.43s/ba]\n","Applying counterfactual augmentation for gender #1:  10% 3/29 [00:35<04:53, 11.27s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  10% 3/29 [00:35<04:51, 11.19s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  14% 4/29 [00:41<04:05,  9.82s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  14% 4/29 [00:46<04:34, 10.98s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  14% 4/29 [00:47<04:44, 11.37s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  17% 5/29 [00:50<03:44,  9.37s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  17% 5/29 [00:54<03:52,  9.68s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  17% 5/29 [00:55<04:04, 10.19s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  21% 6/29 [00:58<03:27,  9.01s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  21% 6/29 [01:03<03:44,  9.75s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  24% 7/29 [01:06<03:27,  9.41s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  24% 7/29 [01:09<03:32,  9.67s/ba]\n","\n","Applying counterfactual augmentation for gender #2:  28% 8/29 [01:12<02:58,  8.49s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  24% 7/29 [01:12<03:29,  9.52s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  28% 8/29 [01:16<03:05,  8.85s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  28% 8/29 [01:20<03:09,  9.04s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  31% 9/29 [01:23<03:04,  9.24s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  31% 9/29 [01:27<03:11,  9.60s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  31% 9/29 [01:30<03:03,  9.20s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  34% 10/29 [01:31<02:44,  8.66s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  34% 10/29 [01:35<02:52,  9.10s/ba]\n","\n","Applying counterfactual augmentation for gender #2:  38% 11/29 [01:38<02:30,  8.34s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  34% 10/29 [01:41<03:05,  9.78s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  34% 10/29 [01:45<03:09,  9.98s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  38% 11/29 [01:48<03:03, 10.18s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  38% 11/29 [01:51<02:55,  9.77s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  38% 11/29 [01:53<02:46,  9.27s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  41% 12/29 [01:58<02:53, 10.20s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  41% 12/29 [02:00<02:43,  9.63s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  41% 12/29 [02:02<02:38,  9.30s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  45% 13/29 [02:07<02:34,  9.66s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  45% 13/29 [02:08<02:26,  9.16s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  45% 13/29 [02:09<02:16,  8.51s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  48% 14/29 [02:14<02:16,  9.12s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  48% 14/29 [02:17<02:17,  9.15s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  48% 14/29 [02:19<02:15,  9.04s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  52% 15/29 [02:26<02:18,  9.86s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  52% 15/29 [02:26<02:07,  9.12s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  52% 15/29 [02:27<02:03,  8.81s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  55% 16/29 [02:34<02:02,  9.40s/ba]\n","Applying counterfactual augmentation for gender #1:  55% 16/29 [02:35<01:49,  8.40s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  55% 16/29 [02:35<01:55,  8.85s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  62% 18/29 [02:40<01:39,  9.05s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  59% 17/29 [02:47<01:53,  9.48s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #0:  59% 17/29 [02:50<02:17, 11.44s/ba]\n","\n","Applying counterfactual augmentation for gender #2:  66% 19/29 [02:51<01:35,  9.57s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  62% 18/29 [02:59<01:52, 10.24s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #0:  62% 18/29 [03:01<02:03, 11.19s/ba]\n","\n","Applying counterfactual augmentation for gender #2:  69% 20/29 [03:01<01:29,  9.89s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  66% 19/29 [03:09<01:41, 10.11s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  66% 19/29 [03:09<01:42, 10.27s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  66% 19/29 [03:10<01:45, 10.59s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  69% 20/29 [03:17<01:27,  9.77s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  69% 20/29 [03:18<01:29,  9.99s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  69% 20/29 [03:22<01:37, 10.82s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  72% 21/29 [03:26<01:15,  9.45s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  72% 21/29 [03:28<01:20, 10.00s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  72% 21/29 [03:29<01:18,  9.87s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  76% 22/29 [03:35<01:05,  9.36s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  76% 22/29 [03:38<01:07,  9.64s/ba]\n","Applying counterfactual augmentation for gender #1:  76% 22/29 [03:39<01:11, 10.14s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  79% 23/29 [03:48<01:01, 10.23s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  79% 23/29 [03:51<01:03, 10.59s/ba]\n","Applying counterfactual augmentation for gender #1:  79% 23/29 [03:51<01:04, 10.82s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  83% 24/29 [03:58<00:52, 10.42s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  83% 24/29 [04:03<00:54, 10.88s/ba]\n","Applying counterfactual augmentation for gender #1:  83% 24/29 [04:03<00:56, 11.23s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  86% 25/29 [04:07<00:39,  9.82s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  93% 27/29 [04:09<00:19,  9.71s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  86% 25/29 [04:11<00:40, 10.13s/ba]\n","\n","Applying counterfactual augmentation for gender #2:  97% 28/29 [04:18<00:09,  9.55s/ba]\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2: 100% 29/29 [04:19<00:00,  8.94s/ba]\n","\n","\n","\n","Applying counterfactual augmentation for gender #0:  90% 26/29 [04:20<00:29,  9.90s/ba]\n","Applying counterfactual augmentation for gender #0:  93% 27/29 [04:26<00:16,  8.44s/ba]\n","Applying counterfactual augmentation for gender #1:  93% 27/29 [04:26<00:17,  8.72s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  93% 27/29 [04:26<00:19,  9.51s/ba]\u001b[A\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  97% 28/29 [04:32<00:08,  8.31s/ba]\u001b[A\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3: 100% 29/29 [04:33<00:00,  9.42s/ba]\n","\n","Applying counterfactual augmentation for gender #1:  97% 28/29 [04:33<00:08,  8.11s/ba]\u001b[A\n","Applying counterfactual augmentation for gender #1: 100% 29/29 [04:34<00:00,  9.46s/ba]\n","Applying counterfactual augmentation for gender #0: 100% 29/29 [04:34<00:00,  9.48s/ba]\n","Applying counterfactual augmentation for gender #0:   0% 0/2 [00:00<?, ?ba/s]\n","Applying counterfactual augmentation for gender #1:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  50% 1/2 [00:06<00:06,  6.62s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  50% 1/2 [00:07<00:07,  7.80s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #0:  50% 1/2 [00:09<00:09,  9.18s/ba]\n","\n","Applying counterfactual augmentation for gender #2: 100% 2/2 [00:09<00:00,  4.51s/ba]\n","\n","Applying counterfactual augmentation for gender #1: 100% 2/2 [00:09<00:00,  4.96s/ba]\n","Applying counterfactual augmentation for gender #0: 100% 2/2 [00:10<00:00,  5.49s/ba]\n","\n","\n","\n","Applying counterfactual augmentation for gender #3: 100% 2/2 [00:10<00:00,  5.45s/ba]\n","Applying counterfactual augmentation for race #0:   0% 0/29 [00:00<?, ?ba/s]\n","Applying counterfactual augmentation for race #1:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\n","\n","Applying counterfactual augmentation for race #2:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #0:   3% 1/29 [00:01<00:52,  1.87s/ba]\n","\n","Applying counterfactual augmentation for race #2:   3% 1/29 [00:02<00:59,  2.14s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:   3% 1/29 [00:02<01:04,  2.29s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:   7% 2/29 [00:03<00:50,  1.88s/ba]\n","\n","Applying counterfactual augmentation for race #2:   7% 2/29 [00:04<00:56,  2.08s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:   7% 2/29 [00:05<01:12,  2.67s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #0:  10% 3/29 [00:07<01:10,  2.71s/ba]\n","\n","Applying counterfactual augmentation for race #2:  10% 3/29 [00:07<01:09,  2.66s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  10% 3/29 [00:09<01:26,  3.33s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  10% 3/29 [00:09<01:25,  3.30s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  14% 4/29 [00:11<01:16,  3.06s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  14% 4/29 [00:12<01:18,  3.14s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  14% 4/29 [00:12<01:23,  3.34s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  17% 5/29 [00:13<01:11,  2.99s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  17% 5/29 [00:14<01:09,  2.91s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  17% 5/29 [00:15<01:12,  3.03s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  21% 6/29 [00:16<01:03,  2.74s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  21% 6/29 [00:16<01:01,  2.68s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  24% 7/29 [00:17<00:53,  2.44s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  24% 7/29 [00:18<00:56,  2.58s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  24% 7/29 [00:19<00:57,  2.60s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  28% 8/29 [00:19<00:48,  2.29s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  28% 8/29 [00:20<00:50,  2.40s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  28% 8/29 [00:21<00:49,  2.35s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  31% 9/29 [00:21<00:45,  2.28s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  31% 9/29 [00:22<00:47,  2.39s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  31% 9/29 [00:23<00:46,  2.34s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  34% 10/29 [00:23<00:41,  2.20s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  34% 10/29 [00:26<00:51,  2.70s/ba]\n","\n","Applying counterfactual augmentation for race #2:  38% 11/29 [00:27<00:45,  2.52s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  34% 10/29 [00:27<00:53,  2.82s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  34% 10/29 [00:29<00:59,  3.11s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  38% 11/29 [00:30<00:57,  3.19s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  38% 11/29 [00:31<01:00,  3.34s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  38% 11/29 [00:32<00:55,  3.08s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  41% 12/29 [00:33<00:54,  3.19s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  41% 12/29 [00:34<00:51,  3.06s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  41% 12/29 [00:34<00:46,  2.76s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  45% 13/29 [00:35<00:45,  2.84s/ba]\n","Applying counterfactual augmentation for race #1:  45% 13/29 [00:36<00:39,  2.47s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  45% 13/29 [00:36<00:44,  2.75s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  48% 14/29 [00:38<00:41,  2.73s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  48% 14/29 [00:38<00:38,  2.55s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  48% 14/29 [00:38<00:36,  2.47s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  52% 15/29 [00:40<00:36,  2.63s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  52% 15/29 [00:40<00:33,  2.42s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  52% 15/29 [00:40<00:33,  2.38s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  59% 17/29 [00:41<00:27,  2.30s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #0:  55% 16/29 [00:42<00:33,  2.54s/ba]\n","Applying counterfactual augmentation for race #1:  55% 16/29 [00:43<00:30,  2.34s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  62% 18/29 [00:44<00:25,  2.35s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  59% 17/29 [00:46<00:31,  2.65s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  59% 17/29 [00:47<00:38,  3.24s/ba]\n","\n","Applying counterfactual augmentation for race #2:  66% 19/29 [00:47<00:27,  2.74s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  62% 18/29 [00:50<00:34,  3.10s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  62% 18/29 [00:51<00:35,  3.26s/ba]\n","\n","Applying counterfactual augmentation for race #2:  69% 20/29 [00:51<00:27,  3.02s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #0:  66% 19/29 [00:53<00:29,  2.98s/ba]\n","Applying counterfactual augmentation for race #1:  66% 19/29 [00:53<00:30,  3.05s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  72% 21/29 [00:53<00:22,  2.84s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #0:  69% 20/29 [00:55<00:24,  2.77s/ba]\n","Applying counterfactual augmentation for race #1:  69% 20/29 [00:55<00:24,  2.75s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  76% 22/29 [00:56<00:18,  2.64s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #0:  72% 21/29 [00:57<00:20,  2.55s/ba]\n","\n","Applying counterfactual augmentation for race #2:  79% 23/29 [00:58<00:15,  2.53s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  72% 21/29 [00:58<00:21,  2.72s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #0:  76% 22/29 [00:59<00:16,  2.39s/ba]\n","\n","Applying counterfactual augmentation for race #2:  83% 24/29 [01:00<00:11,  2.39s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  76% 22/29 [01:00<00:18,  2.63s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #0:  79% 23/29 [01:02<00:14,  2.35s/ba]\n","\n","Applying counterfactual augmentation for race #2:  86% 25/29 [01:02<00:09,  2.39s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  79% 23/29 [01:03<00:16,  2.79s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #0:  83% 24/29 [01:05<00:13,  2.74s/ba]\n","\n","Applying counterfactual augmentation for race #2:  90% 26/29 [01:06<00:08,  2.83s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  83% 24/29 [01:07<00:15,  3.15s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #0:  86% 25/29 [01:09<00:12,  3.07s/ba]\n","\n","Applying counterfactual augmentation for race #2:  93% 27/29 [01:09<00:05,  3.00s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  90% 26/29 [01:12<00:08,  2.97s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  90% 26/29 [01:12<00:09,  3.23s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  97% 28/29 [01:12<00:02,  2.88s/ba]\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2: 100% 29/29 [01:12<00:00,  2.51s/ba]\n","\n","Applying counterfactual augmentation for race #0:  93% 27/29 [01:13<00:05,  2.52s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  93% 27/29 [01:14<00:05,  2.84s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  93% 27/29 [01:15<00:05,  2.54s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  97% 28/29 [01:15<00:02,  2.41s/ba]\u001b[A\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3: 100% 29/29 [01:15<00:00,  2.61s/ba]\n","Applying counterfactual augmentation for race #0: 100% 29/29 [01:16<00:00,  2.63s/ba]\n","\n","Applying counterfactual augmentation for race #1: 100% 29/29 [01:16<00:00,  2.63s/ba]\n","Applying counterfactual augmentation for race #0:   0% 0/2 [00:00<?, ?ba/s]\n","Applying counterfactual augmentation for race #1:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\n","\n","Applying counterfactual augmentation for race #2:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  50% 1/2 [00:01<00:01,  1.56s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  50% 1/2 [00:02<00:02,  2.14s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  50% 1/2 [00:01<00:01,  1.99s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2: 100% 2/2 [00:02<00:00,  1.15s/ba]\n","\n","Applying counterfactual augmentation for race #1: 100% 2/2 [00:02<00:00,  1.29s/ba]\n","Applying counterfactual augmentation for race #0: 100% 2/2 [00:02<00:00,  1.43s/ba]\n","\n","\n","\n","Applying counterfactual augmentation for race #3: 100% 2/2 [00:02<00:00,  1.40s/ba]\n","Applying counterfactual augmentation for religion #0:   0% 0/29 [00:00<?, ?ba/s]\n","Applying counterfactual augmentation for religion #1:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #0:   3% 1/29 [00:02<00:57,  2.06s/ba]\n","\n","Applying counterfactual augmentation for religion #2:   3% 1/29 [00:01<00:55,  1.97s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:   3% 1/29 [00:02<01:05,  2.33s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #0:   7% 2/29 [00:04<01:09,  2.57s/ba]\n","\n","Applying counterfactual augmentation for religion #2:   7% 2/29 [00:04<01:08,  2.52s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:   7% 2/29 [00:06<01:26,  3.22s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:   7% 2/29 [00:06<01:30,  3.36s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  10% 3/29 [00:08<01:19,  3.06s/ba]\n","Applying counterfactual augmentation for religion #1:  10% 3/29 [00:09<01:27,  3.36s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  10% 3/29 [00:09<01:27,  3.37s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  14% 4/29 [00:11<01:17,  3.11s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  14% 4/29 [00:12<01:15,  3.04s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  14% 4/29 [00:12<01:18,  3.14s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  17% 5/29 [00:13<01:05,  2.72s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  17% 5/29 [00:14<01:04,  2.67s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  17% 5/29 [00:14<01:08,  2.86s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  21% 6/29 [00:16<00:58,  2.53s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  21% 6/29 [00:16<00:57,  2.48s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  21% 6/29 [00:17<01:02,  2.74s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  24% 7/29 [00:18<00:52,  2.39s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  24% 7/29 [00:18<00:52,  2.39s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  28% 8/29 [00:19<00:47,  2.28s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #0:  28% 8/29 [00:20<00:47,  2.28s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  28% 8/29 [00:20<00:44,  2.14s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  31% 9/29 [00:21<00:45,  2.28s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #0:  31% 9/29 [00:22<00:47,  2.37s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  31% 9/29 [00:23<00:48,  2.42s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  34% 10/29 [00:25<00:51,  2.70s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #0:  34% 10/29 [00:26<00:53,  2.82s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  34% 10/29 [00:27<00:54,  2.88s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  38% 11/29 [00:28<00:50,  2.79s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #0:  38% 11/29 [00:30<00:55,  3.10s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  38% 11/29 [00:30<00:55,  3.07s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  41% 12/29 [00:31<00:47,  2.77s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  38% 11/29 [00:31<00:53,  2.95s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #0:  41% 12/29 [00:33<00:52,  3.10s/ba]\n","\n","Applying counterfactual augmentation for religion #2:  45% 13/29 [00:33<00:42,  2.66s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  41% 12/29 [00:33<00:43,  2.58s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #0:  45% 13/29 [00:35<00:43,  2.73s/ba]\n","Applying counterfactual augmentation for religion #1:  45% 13/29 [00:35<00:37,  2.35s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  48% 14/29 [00:35<00:36,  2.43s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #0:  48% 14/29 [00:37<00:38,  2.56s/ba]\n","\n","Applying counterfactual augmentation for religion #2:  52% 15/29 [00:37<00:32,  2.30s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  48% 14/29 [00:37<00:34,  2.29s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  52% 15/29 [00:39<00:32,  2.32s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  52% 15/29 [00:39<00:34,  2.47s/ba]\n","Applying counterfactual augmentation for religion #1:  52% 15/29 [00:39<00:31,  2.23s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  55% 16/29 [00:41<00:29,  2.24s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #0:  55% 16/29 [00:41<00:30,  2.38s/ba]\n","\n","Applying counterfactual augmentation for religion #2:  59% 17/29 [00:41<00:27,  2.25s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  59% 17/29 [00:44<00:30,  2.58s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  59% 17/29 [00:44<00:29,  2.45s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  59% 17/29 [00:46<00:35,  2.95s/ba]\n","\n","Applying counterfactual augmentation for religion #2:  66% 19/29 [00:47<00:26,  2.66s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  62% 18/29 [00:48<00:31,  2.87s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #0:  62% 18/29 [00:49<00:32,  2.96s/ba]\n","\n","Applying counterfactual augmentation for religion #2:  69% 20/29 [00:51<00:26,  2.93s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  66% 19/29 [00:51<00:29,  2.98s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #0:  66% 19/29 [00:52<00:29,  2.95s/ba]\n","\n","Applying counterfactual augmentation for religion #2:  72% 21/29 [00:53<00:21,  2.67s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  69% 20/29 [00:53<00:24,  2.68s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #0:  69% 20/29 [00:54<00:24,  2.76s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  72% 21/29 [00:55<00:19,  2.43s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  72% 21/29 [00:56<00:20,  2.55s/ba]\n","Applying counterfactual augmentation for religion #1:  72% 21/29 [00:56<00:21,  2.64s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  76% 22/29 [00:57<00:16,  2.31s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  76% 22/29 [00:58<00:16,  2.39s/ba]\n","Applying counterfactual augmentation for religion #1:  76% 22/29 [00:58<00:17,  2.53s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  83% 24/29 [00:59<00:10,  2.16s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #0:  79% 23/29 [01:00<00:14,  2.36s/ba]\n","Applying counterfactual augmentation for religion #1:  79% 23/29 [01:00<00:14,  2.42s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  86% 25/29 [01:01<00:08,  2.11s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #0:  83% 24/29 [01:03<00:12,  2.57s/ba]\n","Applying counterfactual augmentation for religion #1:  83% 24/29 [01:04<00:13,  2.66s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  90% 26/29 [01:04<00:07,  2.54s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  86% 25/29 [01:05<00:10,  2.61s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #0:  86% 25/29 [01:07<00:11,  2.93s/ba]\n","\n","Applying counterfactual augmentation for religion #2:  93% 27/29 [01:08<00:05,  2.90s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #0:  90% 26/29 [01:10<00:08,  2.82s/ba]\n","Applying counterfactual augmentation for religion #1:  90% 26/29 [01:10<00:09,  3.08s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  97% 28/29 [01:11<00:02,  2.86s/ba]\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #2: 100% 29/29 [01:11<00:00,  2.46s/ba]\n","Applying counterfactual augmentation for religion #0:  93% 27/29 [01:12<00:05,  2.56s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  93% 27/29 [01:11<00:05,  2.99s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  93% 27/29 [01:12<00:05,  2.62s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  97% 28/29 [01:13<00:02,  2.51s/ba]\u001b[A\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3: 100% 29/29 [01:13<00:00,  2.53s/ba]\n","Applying counterfactual augmentation for religion #0: 100% 29/29 [01:14<00:00,  2.55s/ba]\n","\n","Applying counterfactual augmentation for religion #1: 100% 29/29 [01:14<00:00,  2.56s/ba]\n","Applying counterfactual augmentation for religion #0:   0% 0/2 [00:00<?, ?ba/s]\n","Applying counterfactual augmentation for religion #1:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #0:  50% 1/2 [00:01<00:01,  1.96s/ba]\n","\n","Applying counterfactual augmentation for religion #2:  50% 1/2 [00:01<00:01,  1.93s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  50% 1/2 [00:02<00:02,  2.16s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #2: 100% 2/2 [00:02<00:00,  1.19s/ba]\n","\n","Applying counterfactual augmentation for religion #1: 100% 2/2 [00:02<00:00,  1.24s/ba]\n","Applying counterfactual augmentation for religion #0: 100% 2/2 [00:02<00:00,  1.43s/ba]\n","\n","\n","\n","Applying counterfactual augmentation for religion #3: 100% 2/2 [00:02<00:00,  1.48s/ba]\n","<class 'datasets.dataset_dict.DatasetDict'>\n","<class 'datasets.dataset_dict.DatasetDict'>\n","<class 'datasets.dataset_dict.DatasetDict'>\n","Grouping texts in chunks of 512 #0:   0% 0/19 [00:00<?, ?ba/s]\n","Grouping texts in chunks of 512 #1:   0% 0/19 [00:00<?, ?ba/s]\u001b[A\n","\n","Grouping texts in chunks of 512 #0:   5% 1/19 [00:12<03:43, 12.40s/ba]\n","\n","\n","Grouping texts in chunks of 512 #3:   0% 0/19 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #1:   5% 1/19 [00:12<03:38, 12.12s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #0:  11% 2/19 [00:23<03:17, 11.64s/ba]\n","\n","\n","Grouping texts in chunks of 512 #3:   5% 1/19 [00:11<03:22, 11.26s/ba]\u001b[A\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #1:  11% 2/19 [00:25<03:38, 12.83s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #0:  16% 3/19 [00:35<03:09, 11.82s/ba]\n","\n","\n","Grouping texts in chunks of 512 #3:  11% 2/19 [00:22<03:15, 11.52s/ba]\u001b[A\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #1:  16% 3/19 [00:35<03:05, 11.62s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #0:  21% 4/19 [00:48<03:04, 12.28s/ba]\n","\n","\n","Grouping texts in chunks of 512 #3:  16% 3/19 [00:36<03:17, 12.33s/ba]\u001b[A\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #1:  21% 4/19 [00:48<03:02, 12.17s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #0:  26% 5/19 [00:58<02:42, 11.59s/ba]\n","\n","\n","Grouping texts in chunks of 512 #3:  21% 4/19 [00:46<02:51, 11.44s/ba]\u001b[A\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #1:  26% 5/19 [01:00<02:50, 12.19s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #0:  32% 6/19 [01:15<02:52, 13.29s/ba]\n","\n","\n","Grouping texts in chunks of 512 #3:  26% 5/19 [01:03<03:06, 13.34s/ba]\u001b[A\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #1:  32% 6/19 [01:17<02:59, 13.82s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #0:  37% 7/19 [01:32<02:55, 14.66s/ba]\n","\n","\n","Grouping texts in chunks of 512 #3:  32% 6/19 [01:19<03:08, 14.53s/ba]\u001b[A\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #1:  37% 7/19 [01:33<02:53, 14.44s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #2:  37% 7/19 [01:33<02:53, 14.49s/ba]\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0:  42% 8/19 [01:48<02:44, 14.95s/ba]\n","Grouping texts in chunks of 512 #1:  42% 8/19 [01:47<02:36, 14.22s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #0:  47% 9/19 [01:59<02:17, 13.79s/ba]\n","\n","\n","Grouping texts in chunks of 512 #3:  42% 8/19 [01:47<02:33, 13.95s/ba]\u001b[A\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #1:  47% 9/19 [02:00<02:17, 13.77s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #0:  53% 10/19 [02:11<01:59, 13.23s/ba]\n","\n","\n","Grouping texts in chunks of 512 #3:  47% 9/19 [01:58<02:12, 13.21s/ba]\u001b[A\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #1:  53% 10/19 [02:10<01:54, 12.75s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #2:  53% 10/19 [02:10<01:56, 12.91s/ba]\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0:  58% 11/19 [02:24<01:45, 13.18s/ba]\n","Grouping texts in chunks of 512 #1:  58% 11/19 [02:23<01:43, 12.95s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #2:  58% 11/19 [02:22<01:40, 12.55s/ba]\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0:  63% 12/19 [02:35<01:27, 12.47s/ba]\n","Grouping texts in chunks of 512 #1:  63% 12/19 [02:36<01:29, 12.84s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #2:  63% 12/19 [02:36<01:31, 13.06s/ba]\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0:  68% 13/19 [02:52<01:22, 13.77s/ba]\n","Grouping texts in chunks of 512 #1:  68% 13/19 [02:51<01:19, 13.33s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #2:  68% 13/19 [02:51<01:22, 13.81s/ba]\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0:  74% 14/19 [03:06<01:09, 13.88s/ba]\n","Grouping texts in chunks of 512 #1:  74% 14/19 [03:05<01:08, 13.60s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #0:  79% 15/19 [03:18<00:53, 13.44s/ba]\n","\n","\n","Grouping texts in chunks of 512 #3:  74% 14/19 [03:05<01:07, 13.52s/ba]\u001b[A\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #1:  79% 15/19 [03:18<00:53, 13.40s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #0:  84% 16/19 [03:30<00:38, 12.81s/ba]\n","\n","\n","Grouping texts in chunks of 512 #3:  79% 15/19 [03:17<00:52, 13.02s/ba]\u001b[A\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #1:  84% 16/19 [03:29<00:38, 12.74s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #0:  89% 17/19 [03:47<00:28, 14.03s/ba]\n","\n","\n","Grouping texts in chunks of 512 #3:  84% 16/19 [03:34<00:42, 14.25s/ba]\u001b[A\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #1:  89% 17/19 [03:45<00:27, 13.65s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #0:  95% 18/19 [03:57<00:12, 12.93s/ba]\n","\n","\n","Grouping texts in chunks of 512 #3:  89% 17/19 [03:45<00:26, 13.21s/ba]\u001b[A\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #0: 100% 19/19 [04:03<00:00, 12.81s/ba]\n","\n","\n","Grouping texts in chunks of 512 #2:  95% 18/19 [03:55<00:13, 13.12s/ba]\u001b[A\u001b[A\n","Grouping texts in chunks of 512 #1: 100% 19/19 [04:01<00:00, 12.73s/ba]\n","\n","\n","Grouping texts in chunks of 512 #2: 100% 19/19 [03:58<00:00, 12.54s/ba]\n","\n","\n","\n","Grouping texts in chunks of 512 #3:  95% 18/19 [03:55<00:12, 12.30s/ba]\u001b[A\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #3: 100% 19/19 [03:57<00:00, 12.48s/ba]\n","Grouping texts in chunks of 512 #0:   0% 0/1 [00:00<?, ?ba/s]\n","Grouping texts in chunks of 512 #1:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\n","\n","Grouping texts in chunks of 512 #2:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0: 100% 1/1 [00:07<00:00,  7.85s/ba]\n","\n","Grouping texts in chunks of 512 #1: 100% 1/1 [00:08<00:00,  8.10s/ba]\n","\n","\n","Grouping texts in chunks of 512 #2: 100% 1/1 [00:08<00:00,  8.04s/ba]\n","\n","\n","\n","Grouping texts in chunks of 512 #3: 100% 1/1 [00:08<00:00,  8.07s/ba]\n","[INFO|trainer.py:420] 2023-08-21 17:26:08,031 >> max_steps is given, it will override any value given in num_train_epochs\n","[INFO|trainer.py:553] 2023-08-21 17:26:08,032 >> The following columns in the training set  don't have a corresponding argument in `AlbertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","[INFO|trainer.py:1244] 2023-08-21 17:26:08,042 >> ***** Running training *****\n","[INFO|trainer.py:1245] 2023-08-21 17:26:08,042 >>   Num examples = 61704\n","[INFO|trainer.py:1246] 2023-08-21 17:26:08,042 >>   Num Epochs = 1\n","[INFO|trainer.py:1247] 2023-08-21 17:26:08,042 >>   Instantaneous batch size per device = 16\n","[INFO|trainer.py:1248] 2023-08-21 17:26:08,042 >>   Total train batch size (w. parallel, distributed & accumulation) = 128\n","[INFO|trainer.py:1249] 2023-08-21 17:26:08,042 >>   Gradient Accumulation steps = 8\n","[INFO|trainer.py:1250] 2023-08-21 17:26:08,042 >>   Total optimization steps = 200\n"," 25% 50/200 [10:19<31:09, 12.46s/it][INFO|trainer.py:2090] 2023-08-21 17:36:27,962 >> Saving model checkpoint to ./checkpoints/cda_random_200/checkpoint-50\n","[INFO|configuration_utils.py:430] 2023-08-21 17:36:27,969 >> Configuration saved in ./checkpoints/cda_random_200/checkpoint-50/config.json\n","[INFO|modeling_utils.py:1074] 2023-08-21 17:36:28,107 >> Model weights saved in ./checkpoints/cda_random_200/checkpoint-50/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2074] 2023-08-21 17:36:28,112 >> tokenizer config file saved in ./checkpoints/cda_random_200/checkpoint-50/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2080] 2023-08-21 17:36:28,116 >> Special tokens file saved in ./checkpoints/cda_random_200/checkpoint-50/special_tokens_map.json\n"," 50% 100/200 [20:43<20:47, 12.48s/it][INFO|trainer.py:2090] 2023-08-21 17:46:51,813 >> Saving model checkpoint to ./checkpoints/cda_random_200/checkpoint-100\n","[INFO|configuration_utils.py:430] 2023-08-21 17:46:51,819 >> Configuration saved in ./checkpoints/cda_random_200/checkpoint-100/config.json\n","[INFO|modeling_utils.py:1074] 2023-08-21 17:46:51,980 >> Model weights saved in ./checkpoints/cda_random_200/checkpoint-100/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2074] 2023-08-21 17:46:51,987 >> tokenizer config file saved in ./checkpoints/cda_random_200/checkpoint-100/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2080] 2023-08-21 17:46:51,991 >> Special tokens file saved in ./checkpoints/cda_random_200/checkpoint-100/special_tokens_map.json\n"," 75% 150/200 [31:07<10:23, 12.47s/it][INFO|trainer.py:2090] 2023-08-21 17:57:15,553 >> Saving model checkpoint to ./checkpoints/cda_random_200/checkpoint-150\n","[INFO|configuration_utils.py:430] 2023-08-21 17:57:15,559 >> Configuration saved in ./checkpoints/cda_random_200/checkpoint-150/config.json\n","[INFO|modeling_utils.py:1074] 2023-08-21 17:57:15,662 >> Model weights saved in ./checkpoints/cda_random_200/checkpoint-150/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2074] 2023-08-21 17:57:15,667 >> tokenizer config file saved in ./checkpoints/cda_random_200/checkpoint-150/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2080] 2023-08-21 17:57:15,670 >> Special tokens file saved in ./checkpoints/cda_random_200/checkpoint-150/special_tokens_map.json\n","100% 200/200 [41:31<00:00, 12.48s/it][INFO|trainer.py:2090] 2023-08-21 18:07:39,540 >> Saving model checkpoint to ./checkpoints/cda_random_200/checkpoint-200\n","[INFO|configuration_utils.py:430] 2023-08-21 18:07:39,559 >> Configuration saved in ./checkpoints/cda_random_200/checkpoint-200/config.json\n","[INFO|modeling_utils.py:1074] 2023-08-21 18:07:39,660 >> Model weights saved in ./checkpoints/cda_random_200/checkpoint-200/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2074] 2023-08-21 18:07:39,665 >> tokenizer config file saved in ./checkpoints/cda_random_200/checkpoint-200/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2080] 2023-08-21 18:07:39,668 >> Special tokens file saved in ./checkpoints/cda_random_200/checkpoint-200/special_tokens_map.json\n","[INFO|trainer.py:1473] 2023-08-21 18:07:39,984 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 2491.9423, 'train_samples_per_second': 10.273, 'train_steps_per_second': 0.08, 'train_loss': 2.513113708496094, 'epoch': 0.41}\n","100% 200/200 [41:31<00:00, 12.46s/it]\n","[INFO|trainer.py:2090] 2023-08-21 18:07:39,989 >> Saving model checkpoint to ./checkpoints/cda_random_200\n","[INFO|configuration_utils.py:430] 2023-08-21 18:07:39,993 >> Configuration saved in ./checkpoints/cda_random_200/config.json\n","[INFO|modeling_utils.py:1074] 2023-08-21 18:07:40,159 >> Model weights saved in ./checkpoints/cda_random_200/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2074] 2023-08-21 18:07:40,166 >> tokenizer config file saved in ./checkpoints/cda_random_200/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2080] 2023-08-21 18:07:40,172 >> Special tokens file saved in ./checkpoints/cda_random_200/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =       0.41\n","  train_loss               =     2.5131\n","  train_runtime            = 0:41:31.94\n","  train_samples            =      61704\n","  train_samples_per_second =     10.273\n","  train_steps_per_second   =       0.08\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rUo1Hi-rtNio","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692642422208,"user_tz":-60,"elapsed":22648,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"f95e94c1-71ef-4782-ac38-a2f96407bad1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: CDAAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: None\n"," - load_path: ./checkpoints/cda_random_200\n"," - bias_type: gender\n","Evaluating gender examples.\n","100% 262/262 [00:14<00:00, 17.76it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 49.62\n","Stereotype score: 47.17\n","Anti-stereotype score: 53.4\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 49.62\n"]}],"source":["! python experiments/crows_debias.py --model CDAAlbertForMaskedLM --load_path ./checkpoints/cda_random_200 --model_name_or_path albert-base-v2 --bias_type gender"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BXq0oB5wtNn-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692642452343,"user_tz":-60,"elapsed":30139,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"add5601f-d80f-4d16-d25b-2bb5ca1e4d47"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: CDAAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: None\n"," - load_path: ./checkpoints/cda_random_200\n"," - bias_type: race\n","Evaluating race-color examples.\n"," 70% 362/516 [00:14<00:06, 22.16it/s]Skipping example 363.\n","100% 515/516 [00:21<00:00, 24.00it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 52.23\n","Stereotype score: 53.39\n","Anti-stereotype score: 39.53\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 52.23\n"]}],"source":["! python experiments/crows_debias.py --model CDAAlbertForMaskedLM --load_path ./checkpoints/cda_random_200 --model_name_or_path albert-base-v2 --bias_type race"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YzIbtsqeRjts","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692642471325,"user_tz":-60,"elapsed":11994,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"37b54db3-d99d-4c1b-d1ff-42668d225309"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: CDAAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: None\n"," - load_path: ./checkpoints/cda_random_200\n"," - bias_type: religion\n","Evaluating religion examples.\n","100% 105/105 [00:03<00:00, 27.21it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 57.14\n","Stereotype score: 57.58\n","Anti-stereotype score: 50.0\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 57.14\n"]}],"source":["! python experiments/crows_debias.py --model CDAAlbertForMaskedLM --load_path ./checkpoints/cda_random_200 --model_name_or_path albert-base-v2 --bias_type religion"]},{"cell_type":"markdown","source":["### race religion mix 200"],"metadata":{"id":"ZgAAneyaXhln"}},{"cell_type":"code","source":["\n","experiment_id=\"cda_c-albert-base-v2\"\n","!python experiments/run_mlm.py \\\n","                    --model_name_or_path \"albert-base-v2\" \\\n","                    --do_train \\\n","                    --train_file \"./data/text/wikipedia-2.txt\" \\\n","                    --max_steps 200 \\\n","                    --per_device_train_batch_size 16 \\\n","                    --gradient_accumulation_steps 8 \\\n","                    --max_seq_length 512 \\\n","                    --save_steps 50 \\\n","                    --preprocessing_num_workers 4 \\\n","                    --counterfactual_augmentation \"gender\" \\\n","                    --seed 0 \\\n","                    --output_dir \"./checkpoints/cda_23random_200\"\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KELdtlJxXiIG","executionInfo":{"status":"ok","timestamp":1692643471630,"user_tz":-60,"elapsed":533357,"user":{"displayName":"lii l","userId":"18111132785562331664"}},"outputId":"e2596f95-02fb-4f19-a707-6283f0957bd5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2023-08-21 18:35:32.182207: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-08-21 18:35:33.281008: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","08/21/2023 18:35:37 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","08/21/2023 18:35:37 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","bf16=False,\n","bf16_full_eval=False,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=False,\n","do_predict=False,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_steps=None,\n","evaluation_strategy=IntervalStrategy.NO,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","gradient_accumulation_steps=8,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_strategy=HubStrategy.EVERY_SAVE,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=-1,\n","log_level_replica=-1,\n","log_on_each_node=True,\n","logging_dir=./checkpoints/cda_23random_200/runs/Aug21_18-35-37_33ff6e54ac6c,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=IntervalStrategy.STEPS,\n","lr_scheduler_type=SchedulerType.LINEAR,\n","max_grad_norm=1.0,\n","max_steps=200,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=OptimizerNames.ADAMW_HF,\n","output_dir=./checkpoints/cda_23random_200,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=16,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=./checkpoints/cda_23random_200,\n","save_on_each_node=False,\n","save_steps=50,\n","save_strategy=IntervalStrategy.STEPS,\n","save_total_limit=None,\n","seed=0,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","tf32=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_legacy_prediction_loop=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","08/21/2023 18:35:37 - WARNING - datasets.builder - Using custom data configuration default-0de1c001dd32c133\n","08/21/2023 18:35:37 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","08/21/2023 18:35:37 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4\n","08/21/2023 18:35:37 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)\n","08/21/2023 18:35:37 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4\n","100% 1/1 [00:00<00:00, 487.82it/s]\n","08/21/2023 18:35:37 - WARNING - datasets.builder - Using custom data configuration default-0de1c001dd32c133\n","08/21/2023 18:35:37 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","08/21/2023 18:35:37 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4\n","08/21/2023 18:35:37 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)\n","08/21/2023 18:35:37 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4\n","08/21/2023 18:35:38 - WARNING - datasets.builder - Using custom data configuration default-0de1c001dd32c133\n","08/21/2023 18:35:38 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n","08/21/2023 18:35:38 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4\n","08/21/2023 18:35:38 - WARNING - datasets.builder - Reusing dataset text (/root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4)\n","08/21/2023 18:35:38 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/text/default-0de1c001dd32c133/0.0.0/08f6fb1dd2dab0a18ea441c359e1d63794ea8cb53e7863e6edf8fc5655e47ec4\n","[INFO|configuration_utils.py:644] 2023-08-21 18:35:38,262 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:680] 2023-08-21 18:35:38,263 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|tokenization_auto.py:336] 2023-08-21 18:35:38,448 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:644] 2023-08-21 18:35:38,648 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:680] 2023-08-21 18:35:38,648 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|tokenization_utils_base.py:1771] 2023-08-21 18:35:39,660 >> loading file https://huggingface.co/albert-base-v2/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/10be6ce6d3508f1fdce98a57a574283b47c055228c1235f8686f039287ff8174.d6110e25022b713452eb83d5bfa8ae64530995a93d8e694fe52e05aa85dd3a7d\n","[INFO|tokenization_utils_base.py:1771] 2023-08-21 18:35:39,660 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/828a43aa4b9d07e2b7d3be7c6bc10a3ae6e16e8d9c3a0c557783639de9eaeb1b.670e237d152dd53ef77575d4f4a6cd34158db03128fe4f63437ce0d5992bac74\n","[INFO|tokenization_utils_base.py:1771] 2023-08-21 18:35:39,661 >> loading file https://huggingface.co/albert-base-v2/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1771] 2023-08-21 18:35:39,661 >> loading file https://huggingface.co/albert-base-v2/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1771] 2023-08-21 18:35:39,661 >> loading file https://huggingface.co/albert-base-v2/resolve/main/tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:644] 2023-08-21 18:35:39,798 >> loading configuration file https://huggingface.co/albert-base-v2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e48be00f755a5f765e36a32885e8d6a573081df3321c9e19428d12abadf7dba2.b8f28145885741cf994c0e8a97b724f6c974460c297002145e48e511d2496e88\n","[INFO|configuration_utils.py:680] 2023-08-21 18:35:39,799 >> Model config AlbertConfig {\n","  \"_name_or_path\": \"albert-base-v2\",\n","  \"architectures\": [\n","    \"AlbertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0,\n","  \"bos_token_id\": 2,\n","  \"classifier_dropout_prob\": 0.1,\n","  \"down_scale_factor\": 1,\n","  \"embedding_size\": 128,\n","  \"eos_token_id\": 3,\n","  \"gap_size\": 0,\n","  \"hidden_act\": \"gelu_new\",\n","  \"hidden_dropout_prob\": 0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"inner_group_num\": 1,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"albert\",\n","  \"net_structure_type\": 0,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_groups\": 1,\n","  \"num_hidden_layers\": 12,\n","  \"num_memory_blocks\": 0,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30000\n","}\n","\n","[INFO|modeling_utils.py:1427] 2023-08-21 18:35:39,999 >> loading weights file https://huggingface.co/albert-base-v2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/bf1986d976e9a8320cbd3a0597e610bf299d639ce31b7ca581cbf54be3aaa6d3.d6d54047dfe6ae844e3bf6e7a7d0aff71cb598d3df019361e076ba7639b1da9b\n","[INFO|modeling_utils.py:1694] 2023-08-21 18:35:40,155 >> All model checkpoint weights were used when initializing AlbertForMaskedLM.\n","\n","[INFO|modeling_utils.py:1702] 2023-08-21 18:35:40,156 >> All the weights of AlbertForMaskedLM were initialized from the model checkpoint at albert-base-v2.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use AlbertForMaskedLM for predictions without further training.\n","08/21/2023 18:35:40 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_function at 0x7d209570a560> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n","Running tokenizer on every text in dataset #0:   0% 0/29 [00:00<?, ?ba/s]\n","Running tokenizer on every text in dataset #1:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A[WARNING|tokenization_utils_base.py:3377] 2023-08-21 18:35:41,190 >> Token indices sequence length is longer than the specified maximum sequence length for this model (589 > 512). Running this sequence through the model will result in indexing errors\n","[WARNING|tokenization_utils_base.py:3377] 2023-08-21 18:35:41,235 >> Token indices sequence length is longer than the specified maximum sequence length for this model (605 > 512). Running this sequence through the model will result in indexing errors\n","Running tokenizer on every text in dataset #0:   3% 1/29 [00:00<00:27,  1.01ba/s]\n","\n","Running tokenizer on every text in dataset #2:   3% 1/29 [00:00<00:23,  1.20ba/s]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:   3% 1/29 [00:01<00:30,  1.09s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:   3% 1/29 [00:01<00:31,  1.12s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #0:   7% 2/29 [00:01<00:26,  1.02ba/s][WARNING|tokenization_utils_base.py:3377] 2023-08-21 18:35:42,543 >> Token indices sequence length is longer than the specified maximum sequence length for this model (916 > 512). Running this sequence through the model will result in indexing errors\n","\n","\n","\n","Running tokenizer on every text in dataset #3:   7% 2/29 [00:02<00:28,  1.04s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:   7% 2/29 [00:02<00:30,  1.14s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  10% 3/29 [00:03<00:27,  1.05s/ba][WARNING|tokenization_utils_base.py:3377] 2023-08-21 18:35:43,825 >> Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n","\n","\n","\n","Running tokenizer on every text in dataset #3:  10% 3/29 [00:03<00:28,  1.10s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  14% 4/29 [00:03<00:20,  1.22ba/s]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  14% 4/29 [00:04<00:26,  1.07s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  14% 4/29 [00:04<00:26,  1.04s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  17% 5/29 [00:04<00:22,  1.09ba/s]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  17% 5/29 [00:05<00:26,  1.09s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  17% 5/29 [00:05<00:24,  1.04s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  21% 6/29 [00:05<00:23,  1.01s/ba]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  21% 6/29 [00:06<00:24,  1.07s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  21% 6/29 [00:06<00:24,  1.06s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  24% 7/29 [00:06<00:22,  1.04s/ba]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  24% 7/29 [00:07<00:23,  1.06s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  24% 7/29 [00:07<00:23,  1.09s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  28% 8/29 [00:07<00:21,  1.01s/ba]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  28% 8/29 [00:08<00:21,  1.04s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  28% 8/29 [00:08<00:21,  1.01s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  31% 9/29 [00:08<00:21,  1.05s/ba]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  31% 9/29 [00:09<00:21,  1.09s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  31% 9/29 [00:10<00:24,  1.22s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  34% 10/29 [00:10<00:24,  1.30s/ba]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  34% 10/29 [00:11<00:25,  1.36s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  34% 10/29 [00:12<00:28,  1.50s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  38% 11/29 [00:12<00:25,  1.42s/ba]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  38% 11/29 [00:13<00:29,  1.62s/ba]\n","\n","Running tokenizer on every text in dataset #2:  41% 12/29 [00:13<00:24,  1.43s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  38% 11/29 [00:14<00:29,  1.65s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:  38% 11/29 [00:14<00:28,  1.58s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  41% 12/29 [00:15<00:28,  1.69s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  41% 12/29 [00:15<00:27,  1.62s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:  41% 12/29 [00:16<00:26,  1.57s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  45% 13/29 [00:16<00:25,  1.57s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  45% 13/29 [00:17<00:24,  1.56s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:  45% 13/29 [00:17<00:25,  1.57s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  48% 14/29 [00:18<00:24,  1.60s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  48% 14/29 [00:18<00:22,  1.50s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:  48% 14/29 [00:19<00:21,  1.43s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  55% 16/29 [00:19<00:17,  1.34s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  52% 15/29 [00:19<00:20,  1.49s/ba]\n","Running tokenizer on every text in dataset #1:  52% 15/29 [00:20<00:18,  1.32s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  55% 16/29 [00:20<00:18,  1.40s/ba]\n","Running tokenizer on every text in dataset #1:  55% 16/29 [00:21<00:15,  1.21s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  59% 17/29 [00:21<00:18,  1.55s/ba]\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  62% 18/29 [00:22<00:14,  1.36s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  59% 17/29 [00:22<00:18,  1.51s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  59% 17/29 [00:23<00:19,  1.66s/ba]\n","\n","Running tokenizer on every text in dataset #2:  66% 19/29 [00:23<00:12,  1.27s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  62% 18/29 [00:23<00:15,  1.42s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  62% 18/29 [00:24<00:16,  1.48s/ba]\n","\n","Running tokenizer on every text in dataset #2:  69% 20/29 [00:24<00:11,  1.22s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  66% 19/29 [00:24<00:13,  1.31s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  66% 19/29 [00:25<00:13,  1.35s/ba]\n","\n","Running tokenizer on every text in dataset #2:  72% 21/29 [00:25<00:08,  1.12s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  69% 20/29 [00:25<00:11,  1.24s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  76% 22/29 [00:26<00:07,  1.05s/ba]\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  69% 20/29 [00:26<00:11,  1.30s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  72% 21/29 [00:27<00:09,  1.18s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #0:  72% 21/29 [00:27<00:09,  1.22s/ba]\n","Running tokenizer on every text in dataset #1:  72% 21/29 [00:27<00:09,  1.23s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  83% 24/29 [00:27<00:04,  1.01ba/s]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  76% 22/29 [00:28<00:08,  1.26s/ba]\n","Running tokenizer on every text in dataset #1:  76% 22/29 [00:29<00:09,  1.37s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  86% 25/29 [00:29<00:04,  1.22s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  79% 23/29 [00:30<00:08,  1.50s/ba]\n","Running tokenizer on every text in dataset #1:  79% 23/29 [00:31<00:09,  1.57s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  90% 26/29 [00:31<00:04,  1.39s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  83% 24/29 [00:32<00:08,  1.61s/ba]\n","Running tokenizer on every text in dataset #1:  83% 24/29 [00:33<00:08,  1.66s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  93% 27/29 [00:33<00:03,  1.52s/ba]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #0:  86% 25/29 [00:34<00:06,  1.66s/ba]\n","Running tokenizer on every text in dataset #1:  86% 25/29 [00:34<00:06,  1.63s/ba]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:  97% 28/29 [00:34<00:01,  1.51s/ba]\u001b[A\u001b[A\n","\n","Running tokenizer on every text in dataset #2: 100% 29/29 [00:35<00:00,  1.21s/ba]\n","\n","\n","\n","Running tokenizer on every text in dataset #0:  90% 26/29 [00:35<00:04,  1.53s/ba]\n","Running tokenizer on every text in dataset #0:  93% 27/29 [00:36<00:02,  1.39s/ba]\n","\n","\n","Running tokenizer on every text in dataset #3:  93% 27/29 [00:36<00:03,  1.55s/ba]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1:  93% 27/29 [00:37<00:02,  1.43s/ba]\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3: 100% 29/29 [00:37<00:00,  1.30s/ba]\n","Running tokenizer on every text in dataset #0:  97% 28/29 [00:38<00:01,  1.30s/ba]\n","Running tokenizer on every text in dataset #0: 100% 29/29 [00:38<00:00,  1.31s/ba]\n","Running tokenizer on every text in dataset #1: 100% 29/29 [00:37<00:00,  1.31s/ba]\n","Running tokenizer on every text in dataset #0:   0% 0/2 [00:00<?, ?ba/s]\n","Running tokenizer on every text in dataset #1:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\n","\n","Running tokenizer on every text in dataset #2:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #0:  50% 1/2 [00:00<00:00,  1.07ba/s]\n","\n","Running tokenizer on every text in dataset #2:  50% 1/2 [00:00<00:00,  1.08ba/s]\u001b[A\u001b[A\n","\n","\n","Running tokenizer on every text in dataset #3:  50% 1/2 [00:00<00:00,  1.06ba/s]\u001b[A\u001b[A\u001b[A\n","Running tokenizer on every text in dataset #1: 100% 2/2 [00:01<00:00,  1.68ba/s]\n","\n","\n","Running tokenizer on every text in dataset #2: 100% 2/2 [00:01<00:00,  1.75ba/s]\n","Running tokenizer on every text in dataset #0: 100% 2/2 [00:01<00:00,  1.41ba/s]\n","\n","\n","\n","Running tokenizer on every text in dataset #3: 100% 2/2 [00:01<00:00,  1.51ba/s]\n","08/21/2023 18:36:20 - INFO - __main__ - Applying gender CDA.\n","Applying counterfactual augmentation for gender #0:   0% 0/29 [00:00<?, ?ba/s]\n","Applying counterfactual augmentation for gender #1:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #0:   3% 1/29 [00:06<03:14,  6.96s/ba]\n","\n","Applying counterfactual augmentation for gender #2:   3% 1/29 [00:06<03:11,  6.84s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:   3% 1/29 [00:08<03:56,  8.43s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #0:   7% 2/29 [00:17<04:01,  8.95s/ba]\n","\n","Applying counterfactual augmentation for gender #2:   7% 2/29 [00:17<03:59,  8.86s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:   7% 2/29 [00:19<04:32, 10.10s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:   7% 2/29 [00:20<04:36, 10.23s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  10% 3/29 [00:25<03:40,  8.47s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  10% 3/29 [00:29<04:12,  9.70s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  14% 4/29 [00:29<03:02,  7.29s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  14% 4/29 [00:35<03:54,  9.38s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  14% 4/29 [00:38<03:58,  9.53s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  17% 5/29 [00:39<03:13,  8.08s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  17% 5/29 [00:42<03:19,  8.32s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  17% 5/29 [00:45<03:29,  8.72s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  21% 6/29 [00:49<03:17,  8.60s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  21% 6/29 [00:53<03:34,  9.33s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  21% 6/29 [00:56<03:35,  9.37s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  24% 7/29 [00:58<03:14,  8.83s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  24% 7/29 [01:02<03:18,  9.03s/ba]\n","\n","Applying counterfactual augmentation for gender #2:  28% 8/29 [01:05<02:51,  8.15s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #0:  28% 8/29 [01:12<03:17,  9.42s/ba]\n","Applying counterfactual augmentation for gender #1:  24% 7/29 [01:13<04:02, 11.01s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  28% 8/29 [01:15<03:21,  9.60s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  31% 9/29 [01:24<03:23, 10.19s/ba]\n","Applying counterfactual augmentation for gender #1:  28% 8/29 [01:24<03:48, 10.90s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  31% 9/29 [01:26<03:18,  9.91s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  34% 10/29 [01:26<02:55,  9.24s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  34% 10/29 [01:35<03:17, 10.39s/ba]\n","\n","Applying counterfactual augmentation for gender #2:  38% 11/29 [01:36<02:48,  9.35s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  34% 10/29 [01:38<03:19, 10.51s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  41% 12/29 [01:42<02:23,  8.46s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  38% 11/29 [01:44<03:03, 10.22s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  38% 11/29 [01:47<03:04, 10.27s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  45% 13/29 [01:51<02:18,  8.66s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  41% 12/29 [01:59<03:16, 11.56s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  41% 12/29 [01:59<03:03, 10.80s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  48% 14/29 [02:02<02:18,  9.22s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  45% 13/29 [02:07<02:45, 10.32s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  45% 13/29 [02:08<02:42, 10.16s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  45% 13/29 [02:10<02:27,  9.24s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  48% 14/29 [02:17<02:37, 10.47s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  48% 14/29 [02:18<02:33, 10.20s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  48% 14/29 [02:20<02:21,  9.43s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  52% 15/29 [02:26<02:16,  9.78s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  52% 15/29 [02:26<02:10,  9.35s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  52% 15/29 [02:28<02:05,  8.97s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  59% 17/29 [02:32<01:59,  9.95s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #0:  55% 16/29 [02:37<02:13, 10.28s/ba]\n","Applying counterfactual augmentation for gender #1:  55% 16/29 [02:37<01:58,  9.11s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  62% 18/29 [02:39<01:40,  9.15s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  59% 17/29 [02:44<01:41,  8.43s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  59% 17/29 [02:44<01:50,  9.19s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  59% 17/29 [02:47<02:04, 10.35s/ba]\n","Applying counterfactual augmentation for gender #1:  62% 18/29 [02:57<01:45,  9.63s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  62% 18/29 [02:57<01:52, 10.22s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  62% 18/29 [02:58<01:55, 10.49s/ba]\n","Applying counterfactual augmentation for gender #1:  66% 19/29 [03:05<01:32,  9.26s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  66% 19/29 [03:05<01:35,  9.55s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  66% 19/29 [03:07<01:40, 10.08s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  69% 20/29 [03:15<01:27,  9.76s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  69% 20/29 [03:16<01:28,  9.86s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  69% 20/29 [03:18<01:33, 10.38s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  72% 21/29 [03:22<01:11,  8.91s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  79% 23/29 [03:24<00:53,  8.98s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  72% 21/29 [03:26<01:15,  9.49s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  76% 22/29 [03:34<01:07,  9.69s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  76% 22/29 [03:36<01:07,  9.63s/ba]\n","Applying counterfactual augmentation for gender #1:  76% 22/29 [03:37<01:10, 10.14s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  79% 23/29 [03:42<00:55,  9.29s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  79% 23/29 [03:45<00:56,  9.37s/ba]\n","Applying counterfactual augmentation for gender #1:  79% 23/29 [03:45<00:58,  9.76s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  83% 24/29 [03:51<00:46,  9.25s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #0:  83% 24/29 [03:56<00:49,  9.90s/ba]\n","Applying counterfactual augmentation for gender #1:  83% 24/29 [03:57<00:51, 10.21s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  86% 25/29 [03:59<00:35,  8.92s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  93% 27/29 [04:01<00:18,  9.13s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #0:  86% 25/29 [04:04<00:37,  9.39s/ba]\n","\n","Applying counterfactual augmentation for gender #2:  97% 28/29 [04:11<00:09,  9.13s/ba]\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for gender #2: 100% 29/29 [04:11<00:00,  8.69s/ba]\n","\n","\n","\n","Applying counterfactual augmentation for gender #0:  90% 26/29 [04:14<00:28,  9.47s/ba]\n","Applying counterfactual augmentation for gender #0:  93% 27/29 [04:19<00:16,  8.13s/ba]\n","\n","\n","Applying counterfactual augmentation for gender #3:  93% 27/29 [04:19<00:18,  9.01s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  93% 27/29 [04:20<00:16,  8.47s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:  97% 28/29 [04:24<00:07,  7.97s/ba]\u001b[A\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3: 100% 29/29 [04:25<00:00,  9.15s/ba]\n","Applying counterfactual augmentation for gender #0:  97% 28/29 [04:26<00:08,  8.03s/ba]\n","Applying counterfactual augmentation for gender #0: 100% 29/29 [04:27<00:00,  9.22s/ba]\n","\n","Applying counterfactual augmentation for gender #1: 100% 29/29 [04:27<00:00,  9.23s/ba]\n","Applying counterfactual augmentation for gender #0:   0% 0/2 [00:00<?, ?ba/s]\n","Applying counterfactual augmentation for gender #1:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #3:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for gender #1:  50% 1/2 [00:06<00:06,  6.46s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for gender #2:  50% 1/2 [00:07<00:07,  7.79s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for gender #0:  50% 1/2 [00:08<00:08,  8.99s/ba]\n","\n","Applying counterfactual augmentation for gender #2: 100% 2/2 [00:09<00:00,  4.55s/ba]\n","\n","Applying counterfactual augmentation for gender #1: 100% 2/2 [00:09<00:00,  4.84s/ba]\n","Applying counterfactual augmentation for gender #0: 100% 2/2 [00:10<00:00,  5.40s/ba]\n","\n","\n","\n","Applying counterfactual augmentation for gender #3: 100% 2/2 [00:11<00:00,  5.51s/ba]\n","Applying counterfactual augmentation for race #0:   0% 0/29 [00:00<?, ?ba/s]\n","Applying counterfactual augmentation for race #1:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\n","\n","Applying counterfactual augmentation for race #2:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #0:   3% 1/29 [00:01<00:54,  1.94s/ba]\n","\n","Applying counterfactual augmentation for race #2:   3% 1/29 [00:01<00:55,  1.99s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:   3% 1/29 [00:02<01:01,  2.21s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:   7% 2/29 [00:03<00:50,  1.87s/ba]\n","\n","Applying counterfactual augmentation for race #2:   7% 2/29 [00:03<00:52,  1.95s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:   7% 2/29 [00:05<01:09,  2.59s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:   7% 2/29 [00:05<01:14,  2.75s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  10% 3/29 [00:07<01:08,  2.65s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  10% 3/29 [00:09<01:26,  3.32s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  10% 3/29 [00:09<01:28,  3.40s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  14% 4/29 [00:10<01:14,  2.98s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  14% 4/29 [00:11<01:16,  3.06s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  14% 4/29 [00:12<01:18,  3.16s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  17% 5/29 [00:14<01:13,  3.06s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  17% 5/29 [00:14<01:10,  2.96s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  17% 5/29 [00:15<01:11,  2.99s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  21% 6/29 [00:16<01:03,  2.77s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  21% 6/29 [00:16<01:01,  2.68s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  21% 6/29 [00:17<01:03,  2.74s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  24% 7/29 [00:18<00:57,  2.63s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  24% 7/29 [00:19<00:56,  2.58s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  28% 8/29 [00:19<00:49,  2.33s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  28% 8/29 [00:20<00:49,  2.37s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  28% 8/29 [00:21<00:49,  2.36s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  31% 9/29 [00:21<00:46,  2.33s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  31% 9/29 [00:22<00:46,  2.33s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  31% 9/29 [00:23<00:47,  2.36s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  34% 10/29 [00:23<00:42,  2.26s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  34% 10/29 [00:25<00:47,  2.52s/ba]\n","\n","Applying counterfactual augmentation for race #2:  38% 11/29 [00:26<00:45,  2.55s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  34% 10/29 [00:27<00:52,  2.78s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  38% 11/29 [00:29<00:54,  3.03s/ba]\n","\n","Applying counterfactual augmentation for race #2:  41% 12/29 [00:29<00:45,  2.65s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  38% 11/29 [00:31<01:01,  3.40s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  38% 11/29 [00:32<00:58,  3.24s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  41% 12/29 [00:34<00:59,  3.48s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  41% 12/29 [00:35<01:00,  3.57s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  41% 12/29 [00:36<00:57,  3.41s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  45% 13/29 [00:37<00:54,  3.39s/ba]\n","Applying counterfactual augmentation for race #1:  45% 13/29 [00:38<00:51,  3.20s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  45% 13/29 [00:39<00:56,  3.55s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  48% 14/29 [00:40<00:51,  3.40s/ba]\n","Applying counterfactual augmentation for race #1:  48% 14/29 [00:42<00:48,  3.22s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  48% 14/29 [00:42<00:49,  3.28s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  52% 15/29 [00:43<00:44,  3.16s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  52% 15/29 [00:44<00:40,  2.91s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  52% 15/29 [00:44<00:40,  2.93s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  55% 16/29 [00:46<00:38,  2.98s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  55% 16/29 [00:47<00:38,  2.96s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  55% 16/29 [00:47<00:38,  2.97s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  59% 17/29 [00:50<00:42,  3.56s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  59% 17/29 [00:50<00:37,  3.14s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  59% 17/29 [00:51<00:37,  3.13s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  62% 18/29 [00:54<00:37,  3.44s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  62% 18/29 [00:54<00:35,  3.27s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  62% 18/29 [00:54<00:36,  3.35s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  66% 19/29 [00:56<00:30,  3.05s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  66% 19/29 [00:56<00:29,  2.92s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  66% 19/29 [00:57<00:30,  3.05s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  69% 20/29 [00:58<00:25,  2.81s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  69% 20/29 [00:58<00:23,  2.64s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  69% 20/29 [00:59<00:24,  2.76s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  72% 21/29 [01:00<00:20,  2.58s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  72% 21/29 [01:00<00:19,  2.50s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  72% 21/29 [01:01<00:21,  2.63s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  76% 22/29 [01:02<00:17,  2.46s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  76% 22/29 [01:03<00:17,  2.50s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  76% 22/29 [01:03<00:17,  2.47s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  79% 23/29 [01:05<00:15,  2.59s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  79% 23/29 [01:06<00:15,  2.64s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  79% 23/29 [01:07<00:17,  2.86s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #0:  83% 24/29 [01:09<00:15,  3.05s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  83% 24/29 [01:09<00:14,  2.89s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  90% 26/29 [01:10<00:08,  2.89s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  83% 24/29 [01:11<00:15,  3.14s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #0:  86% 25/29 [01:13<00:12,  3.20s/ba]\n","\n","Applying counterfactual augmentation for race #2:  93% 27/29 [01:13<00:05,  2.97s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0:  90% 26/29 [01:15<00:08,  2.87s/ba]\n","\n","Applying counterfactual augmentation for race #2:  97% 28/29 [01:15<00:02,  2.66s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:  90% 26/29 [01:15<00:08,  2.99s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2: 100% 29/29 [01:16<00:00,  2.62s/ba]\n","\n","Applying counterfactual augmentation for race #0:  93% 27/29 [01:16<00:04,  2.48s/ba]\n","\n","\n","Applying counterfactual augmentation for race #3:  93% 27/29 [01:17<00:05,  2.68s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #0: 100% 29/29 [01:19<00:00,  2.73s/ba]\n","\n","\n","\n","Applying counterfactual augmentation for race #3: 100% 29/29 [01:19<00:00,  2.73s/ba]\n","\n","Applying counterfactual augmentation for race #1: 100% 29/29 [01:19<00:00,  2.74s/ba]\n","Applying counterfactual augmentation for race #0:   0% 0/2 [00:00<?, ?ba/s]\n","Applying counterfactual augmentation for race #1:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\n","\n","Applying counterfactual augmentation for race #2:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for race #3:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for race #1:  50% 1/2 [00:01<00:01,  1.49s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for race #2:  50% 1/2 [00:01<00:01,  1.72s/ba]\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for race #2: 100% 2/2 [00:02<00:00,  1.00s/ba]\n","\n","\n","\n","Applying counterfactual augmentation for race #0:  50% 1/2 [00:02<00:02,  2.36s/ba]\n","Applying counterfactual augmentation for race #1: 100% 2/2 [00:02<00:00,  1.22s/ba]\n","Applying counterfactual augmentation for race #0: 100% 2/2 [00:02<00:00,  1.45s/ba]\n","\n","\n","\n","Applying counterfactual augmentation for race #3: 100% 2/2 [00:02<00:00,  1.38s/ba]\n","Applying counterfactual augmentation for religion #0:   0% 0/29 [00:00<?, ?ba/s]\n","Applying counterfactual augmentation for religion #1:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:   0% 0/29 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #0:   3% 1/29 [00:02<01:11,  2.54s/ba]\n","\n","Applying counterfactual augmentation for religion #2:   3% 1/29 [00:03<01:25,  3.06s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:   3% 1/29 [00:03<01:35,  3.40s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #0:   7% 2/29 [00:05<01:16,  2.84s/ba]\n","\n","Applying counterfactual augmentation for religion #2:   7% 2/29 [00:06<01:22,  3.04s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:   7% 2/29 [00:06<01:31,  3.38s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:   7% 2/29 [00:06<01:34,  3.50s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  10% 3/29 [00:08<01:19,  3.06s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  10% 3/29 [00:10<01:26,  3.35s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  10% 3/29 [00:10<01:29,  3.45s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  14% 4/29 [00:11<01:12,  2.91s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  14% 4/29 [00:12<01:11,  2.87s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  14% 4/29 [00:12<01:13,  2.94s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  17% 5/29 [00:13<01:01,  2.57s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  17% 5/29 [00:14<01:01,  2.57s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  17% 5/29 [00:14<01:04,  2.67s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  21% 6/29 [00:15<00:55,  2.41s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  21% 6/29 [00:16<00:55,  2.41s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  21% 6/29 [00:17<01:00,  2.61s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  24% 7/29 [00:17<00:51,  2.32s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  24% 7/29 [00:18<00:52,  2.37s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  28% 8/29 [00:19<00:47,  2.25s/ba]\n","Applying counterfactual augmentation for religion #1:  24% 7/29 [00:19<00:57,  2.61s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  28% 8/29 [00:20<00:44,  2.10s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  31% 9/29 [00:23<00:53,  2.69s/ba]\n","Applying counterfactual augmentation for religion #1:  28% 8/29 [00:23<01:01,  2.93s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  31% 9/29 [00:24<00:54,  2.73s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  34% 10/29 [00:25<00:53,  2.83s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #0:  34% 10/29 [00:26<00:55,  2.92s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  34% 10/29 [00:27<00:56,  2.95s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  38% 11/29 [00:28<00:50,  2.80s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #0:  38% 11/29 [00:30<00:54,  3.05s/ba]\n","\n","Applying counterfactual augmentation for religion #2:  41% 12/29 [00:30<00:42,  2.52s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  38% 11/29 [00:30<00:53,  2.96s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  38% 11/29 [00:31<00:49,  2.74s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  41% 12/29 [00:32<00:48,  2.88s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  41% 12/29 [00:32<00:45,  2.66s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  41% 12/29 [00:33<00:42,  2.49s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  45% 13/29 [00:34<00:42,  2.63s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  45% 13/29 [00:34<00:39,  2.49s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  45% 13/29 [00:35<00:38,  2.39s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  48% 14/29 [00:37<00:37,  2.53s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  48% 14/29 [00:36<00:36,  2.41s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  48% 14/29 [00:38<00:35,  2.38s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  52% 15/29 [00:38<00:31,  2.28s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  52% 15/29 [00:40<00:37,  2.66s/ba]\n","Applying counterfactual augmentation for religion #1:  52% 15/29 [00:41<00:37,  2.69s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  55% 16/29 [00:43<00:36,  2.84s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  55% 16/29 [00:44<00:40,  3.14s/ba]\n","Applying counterfactual augmentation for religion #1:  55% 16/29 [00:44<00:37,  2.86s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  62% 18/29 [00:46<00:31,  2.87s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  59% 17/29 [00:46<00:35,  2.96s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #0:  59% 17/29 [00:47<00:39,  3.26s/ba]\n","\n","Applying counterfactual augmentation for religion #2:  66% 19/29 [00:49<00:28,  2.86s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  62% 18/29 [00:49<00:33,  3.06s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #0:  62% 18/29 [00:50<00:34,  3.09s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  66% 19/29 [00:51<00:27,  2.73s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  66% 19/29 [00:52<00:27,  2.76s/ba]\n","Applying counterfactual augmentation for religion #1:  66% 19/29 [00:52<00:27,  2.71s/ba]\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:  69% 20/29 [00:53<00:21,  2.39s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  72% 21/29 [00:53<00:20,  2.55s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #0:  69% 20/29 [00:55<00:24,  2.69s/ba]\n","Applying counterfactual augmentation for religion #1:  69% 20/29 [00:55<00:23,  2.57s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  76% 22/29 [00:55<00:16,  2.41s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #0:  72% 21/29 [00:57<00:20,  2.51s/ba]\n","Applying counterfactual augmentation for religion #1:  72% 21/29 [00:57<00:20,  2.53s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  76% 22/29 [00:59<00:16,  2.32s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  79% 23/29 [00:59<00:12,  2.16s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  76% 22/29 [00:59<00:17,  2.46s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  83% 24/29 [01:00<00:11,  2.27s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #0:  79% 23/29 [01:02<00:16,  2.76s/ba]\n","Applying counterfactual augmentation for religion #1:  79% 23/29 [01:03<00:17,  2.91s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  86% 25/29 [01:03<00:10,  2.69s/ba]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #0:  83% 24/29 [01:06<00:14,  2.92s/ba]\n","\n","Applying counterfactual augmentation for religion #2:  90% 26/29 [01:06<00:08,  2.76s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #0:  86% 25/29 [01:09<00:12,  3.00s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  90% 26/29 [01:09<00:08,  2.96s/ba]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  86% 25/29 [01:09<00:11,  2.89s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  90% 26/29 [01:11<00:07,  2.66s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  93% 27/29 [01:11<00:05,  2.70s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:  97% 28/29 [01:11<00:02,  2.65s/ba]\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  90% 26/29 [01:12<00:08,  2.75s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #2: 100% 29/29 [01:12<00:00,  2.49s/ba]\n","Applying counterfactual augmentation for religion #0:  93% 27/29 [01:12<00:04,  2.38s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3: 100% 29/29 [01:13<00:00,  2.52s/ba]\n","\n","Applying counterfactual augmentation for religion #0: 100% 29/29 [01:14<00:00,  2.57s/ba]\n","\n","Applying counterfactual augmentation for religion #1: 100% 29/29 [01:14<00:00,  2.57s/ba]\n","Applying counterfactual augmentation for religion #0:   0% 0/2 [00:00<?, ?ba/s]\n","Applying counterfactual augmentation for religion #1:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\n","\n","Applying counterfactual augmentation for religion #2:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Applying counterfactual augmentation for religion #3:   0% 0/2 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n","Applying counterfactual augmentation for religion #1:  50% 1/2 [00:01<00:01,  1.33s/ba]\u001b[A\n","\n","Applying counterfactual augmentation for religion #0:  50% 1/2 [00:02<00:02,  2.14s/ba]\n","\n","\n","Applying counterfactual augmentation for religion #3:  50% 1/2 [00:01<00:01,  1.97s/ba]\u001b[A\u001b[A\u001b[A\n","\n","Applying counterfactual augmentation for religion #2: 100% 2/2 [00:02<00:00,  1.11s/ba]\n","\n","Applying counterfactual augmentation for religion #1: 100% 2/2 [00:02<00:00,  1.17s/ba]\n","\n","\n","\n","Applying counterfactual augmentation for religion #3: 100% 2/2 [00:02<00:00,  1.30s/ba]\n","Applying counterfactual augmentation for religion #0: 100% 2/2 [00:02<00:00,  1.39s/ba]\n","<class 'datasets.dataset_dict.DatasetDict'>\n","<class 'datasets.dataset_dict.DatasetDict'>\n","<class 'datasets.dataset_dict.DatasetDict'>\n","Grouping texts in chunks of 512 #0:   0% 0/4 [00:00<?, ?ba/s]\n","Grouping texts in chunks of 512 #1:   0% 0/4 [00:00<?, ?ba/s]\u001b[A\n","\n","Grouping texts in chunks of 512 #2:   0% 0/4 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0:  25% 1/4 [00:11<00:33, 11.27s/ba]\n","Grouping texts in chunks of 512 #1:  25% 1/4 [00:11<00:34, 11.66s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #2:  25% 1/4 [00:11<00:34, 11.61s/ba]\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0:  50% 2/4 [00:19<00:18,  9.46s/ba]\n","Grouping texts in chunks of 512 #1:  50% 2/4 [00:20<00:20, 10.09s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #2:  50% 2/4 [00:20<00:19, 10.00s/ba]\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0:  75% 3/4 [00:31<00:10, 10.60s/ba]\n","Grouping texts in chunks of 512 #1:  75% 3/4 [00:31<00:10, 10.64s/ba]\u001b[A\n","\n","Grouping texts in chunks of 512 #2:  75% 3/4 [00:31<00:10, 10.64s/ba]\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0: 100% 4/4 [00:36<00:00,  9.11s/ba]\n","\n","Grouping texts in chunks of 512 #1: 100% 4/4 [00:36<00:00,  9.19s/ba]\n","\n","\n","Grouping texts in chunks of 512 #2: 100% 4/4 [00:36<00:00,  9.09s/ba]\n","\n","\n","\n","Grouping texts in chunks of 512 #3: 100% 4/4 [00:35<00:00,  8.88s/ba]\n","Grouping texts in chunks of 512 #0:   0% 0/1 [00:00<?, ?ba/s]\n","Grouping texts in chunks of 512 #1:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\n","\n","Grouping texts in chunks of 512 #2:   0% 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\n","\n","\n","Grouping texts in chunks of 512 #0: 100% 1/1 [00:00<00:00,  2.59ba/s]\n","\n","\n","Grouping texts in chunks of 512 #2: 100% 1/1 [00:00<00:00,  2.39ba/s]\n","\n","Grouping texts in chunks of 512 #1: 100% 1/1 [00:00<00:00,  1.96ba/s]\n","\n","\n","\n","Grouping texts in chunks of 512 #3: 100% 1/1 [00:00<00:00,  2.30ba/s]\n","Traceback (most recent call last):\n","  File \"/content/drive/My Drive/bias-bench-main/experiments/run_mlm.py\", line 929, in <module>\n","    main()\n","  File \"/content/drive/My Drive/bias-bench-main/experiments/run_mlm.py\", line 856, in main\n","    trainer = Trainer(\n","  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 414, in __init__\n","    os.makedirs(self.args.output_dir, exist_ok=True)\n","  File \"/usr/lib/python3.10/os.py\", line 215, in makedirs\n","    makedirs(head, exist_ok=exist_ok)\n","  File \"/usr/lib/python3.10/os.py\", line 225, in makedirs\n","    mkdir(name, mode)\n","OSError: [Errno 107] Transport endpoint is not connected: './checkpoints'\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"2Xi3umZDXiR7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"y1Sj5wYbXibM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"GXYeugFwXimM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VgprKi_OtOde"},"source":["## SENTENCE DEBIAS"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"wbhwaO1StNXs","outputId":"b42fa136-d3e8-4a96-e1b6-f6d358a47786"},"outputs":[{"name":"stdout","output_type":"stream","text":["Computing bias subspace:\n"," - persistent_dir: /content/drive/My Drive/bias-bench-main\n"," - model_name_or_path: bert-base-uncased\n"," - model: BertModel\n"," - bias_type: race\n"," - batch_size: 32\n","Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Encoding race examples:   3% 87/2621 [57:27<27:29:02, 39.05s/it]"]}],"source":["# bert sentence_debias race\n","! python experiments/sentence_debias_subspace.py --model BertModel --model_name_or_path bert-base-uncased --bias_type gender"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UBx5zWWJHq0w"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["## self-debias"],"metadata":{"id":"n9pcDuZMBtQe"}},{"cell_type":"markdown","metadata":{"id":"yelJohDoKKab"},"source":["修改self-debias的bias type代码，使可以一次顺序运行3个bias再出结果。这个论文结论说是最强的。"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":228726,"status":"ok","timestamp":1690548600664,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"LBeRgQRJKK_N","outputId":"db217731-8120-49c1-f940-6147e9e67f82"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: SelfDebiasAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: None\n"," - load_path: None\n"," - bias_type: gender\n","Downloading: 100% 684/684 [00:00<00:00, 4.09MB/s]\n","Downloading: 100% 742k/742k [00:00<00:00, 50.3MB/s]\n","Downloading: 100% 1.25M/1.25M [00:00<00:00, 55.6MB/s]\n","Downloading: 100% 45.2M/45.2M [00:00<00:00, 121MB/s]\n","Evaluating gender examples.\n","100% 262/262 [03:31<00:00,  1.24it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 45.04\n","Stereotype score: 42.77\n","Anti-stereotype score: 48.54\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 45.04\n"]}],"source":["# albert的SelfDebias --bias_type gender\n","\n","! python experiments/crows_debias.py --model SelfDebiasAlbertForMaskedLM --model_name_or_path albert-base-v2 --bias_type gender"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":353806,"status":"ok","timestamp":1690549015797,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"_ZnCbmP-KLBE","outputId":"cf61b939-0d5a-4df7-d9aa-e7d9295964f4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: SelfDebiasAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: None\n"," - load_path: None\n"," - bias_type: race\n","Evaluating race-color examples.\n"," 70% 363/516 [04:02<01:10,  2.18it/s]Skipping example 363.\n","100% 515/516 [05:46<00:00,  1.49it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 57.09\n","Stereotype score: 58.39\n","Anti-stereotype score: 44.19\n","Num. neutral: 0.19\n","====================================================================================================\n","\n","Metric: 57.09\n"]}],"source":["# albert的SelfDebias --bias_type race\n","\n","! python experiments/crows_debias.py --model SelfDebiasAlbertForMaskedLM --model_name_or_path albert-base-v2 --bias_type race"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":310371,"status":"ok","timestamp":1690583440104,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"Kasy4YFGKLDX","outputId":"3cc11795-910a-4981-870f-c4c69b09e4e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: SelfDebiasAlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_direction: None\n"," - projection_matrix: None\n"," - load_path: None\n"," - bias_type: religion\n","> /content/drive/My Drive/bias-bench-main/bias_bench/model/models.py(504)__new__()\n","-> return model\n","(Pdb) n\n","--Return--\n","> /content/drive/My Drive/bias-bench-main/bias_bench/model/models.py(504)__new__()-><bias_bench.d...x7a757411ef50>\n","-> return model\n","(Pdb) n\n","> /content/drive/MyDrive/bias-bench-main/experiments/crows_debias.py(122)<module>()\n","-> if _is_self_debias(args.model):\n","(Pdb) n\n","> /content/drive/MyDrive/bias-bench-main/experiments/crows_debias.py(123)<module>()\n","-> model._model.eval()\n","(Pdb) n\n","> /content/drive/MyDrive/bias-bench-main/experiments/crows_debias.py(127)<module>()\n","-> tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_name_or_path)\n","(Pdb) n\n","> /content/drive/MyDrive/bias-bench-main/experiments/crows_debias.py(129)<module>()\n","-> runner = CrowSPairsRunner(\n","(Pdb) c\n","Evaluating religion examples.\n","  0% 0/105 [00:00<?, ?it/s]> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(360)_average_log_probability()\n","-> probs = []\n","(Pdb) c\n","> /content/drive/My Drive/bias-bench-main/bias_bench/debias/self_debias/modeling.py(94)get_token_logits_self_debiasing()\n","-> logits_processor = SelfDebiasingLogitsProcessor(\n","(Pdb) c\n","> /content/drive/My Drive/bias-bench-main/bias_bench/debias/self_debias/modeling.py(94)get_token_logits_self_debiasing()\n","-> logits_processor = SelfDebiasingLogitsProcessor(\n","(Pdb) c\n","> /content/drive/My Drive/bias-bench-main/bias_bench/debias/self_debias/modeling.py(94)get_token_logits_self_debiasing()\n","-> logits_processor = SelfDebiasingLogitsProcessor(\n","(Pdb) c\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(385)_average_log_probability()\n","-> score = np.mean(probs)\n","(Pdb) p probs\n","[-8.968353271484375, -7.0770745277404785, -7.310459613800049]\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(387)_average_log_probability()\n","-> return score\n","(Pdb) p score\n","-7.785295804341634\n","(Pdb) n\n","--Return--\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(387)_average_log_probability()->-7.785295804341634\n","-> return score\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(129)_likelihood_score()\n","-> score2 = self._average_log_probability(sent2_token_ids, template2)\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(360)_average_log_probability()\n","-> probs = []\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(361)_average_log_probability()\n","-> for position in spans:\n","(Pdb) c\n","> /content/drive/My Drive/bias-bench-main/bias_bench/debias/self_debias/modeling.py(94)get_token_logits_self_debiasing()\n","-> logits_processor = SelfDebiasingLogitsProcessor(\n","(Pdb) c\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(385)_average_log_probability()\n","-> score = np.mean(probs)\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(387)_average_log_probability()\n","-> return score\n","(Pdb) p probs\n","[-9.439262390136719]\n","(Pdb) n\n","--Return--\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(387)_average_log_probability()->-9.439262390136719\n","-> return score\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(131)_likelihood_score()\n","-> score1 = round(score1, 3)\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(132)_likelihood_score()\n","-> score2 = round(score2, 3)\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(134)_likelihood_score()\n","-> N += 1\n","(Pdb) p score1\n","-7.785\n","(Pdb) p score2\n","-9.439\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(135)_likelihood_score()\n","-> pair_score = 0\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(136)_likelihood_score()\n","-> pbar.update(1)\n","(Pdb) n\n","  1% 1/105 [05:19<9:13:05, 319.09s/it]> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(137)_likelihood_score()\n","-> if score1 == score2:\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(140)_likelihood_score()\n","-> if direction == \"stereo\":\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(145)_likelihood_score()\n","-> elif direction == \"antistereo\":\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(146)_likelihood_score()\n","-> total_antistereo += 1\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(147)_likelihood_score()\n","-> if score2 > score1:\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(151)_likelihood_score()\n","-> sent_more, sent_less = \"\", \"\"\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(152)_likelihood_score()\n","-> if direction == \"stereo\":\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(158)_likelihood_score()\n","-> sent_more = data[\"sent2\"]\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(159)_likelihood_score()\n","-> sent_less = data[\"sent1\"]\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(160)_likelihood_score()\n","-> sent_more_score = score2\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(161)_likelihood_score()\n","-> sent_less_score = score1\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(163)_likelihood_score()\n","-> df_score = df_score.append(\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(165)_likelihood_score()\n","-> \"sent_more\": sent_more,\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(166)_likelihood_score()\n","-> \"sent_less\": sent_less,\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(167)_likelihood_score()\n","-> \"sent_more_score\": sent_more_score,\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(168)_likelihood_score()\n","-> \"sent_less_score\": sent_less_score,\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(169)_likelihood_score()\n","-> \"score\": pair_score,\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(170)_likelihood_score()\n","-> \"stereo_antistereo\": direction,\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(171)_likelihood_score()\n","-> \"bias_type\": bias,\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(164)_likelihood_score()\n","-> {\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(173)_likelihood_score()\n","-> ignore_index=True,\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(163)_likelihood_score()\n","-> df_score = df_score.append(\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(104)_likelihood_score()\n","-> for index, data in df_data.iterrows():\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(105)_likelihood_score()\n","-> direction = data[\"direction\"]\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(106)_likelihood_score()\n","-> bias = data[\"bias_type\"]\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(108)_likelihood_score()\n","-> assert bias == self._bias_type\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(110)_likelihood_score()\n","-> sent1, sent2 = data[\"sent1\"], data[\"sent2\"]\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(112)_likelihood_score()\n","-> sent1_token_ids = self._tokenizer.encode(sent1, return_tensors=\"pt\").to(\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(113)_likelihood_score()\n","-> device\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(112)_likelihood_score()\n","-> sent1_token_ids = self._tokenizer.encode(sent1, return_tensors=\"pt\").to(\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(115)_likelihood_score()\n","-> sent2_token_ids = self._tokenizer.encode(sent2, return_tensors=\"pt\").to(\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(116)_likelihood_score()\n","-> device\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(115)_likelihood_score()\n","-> sent2_token_ids = self._tokenizer.encode(sent2, return_tensors=\"pt\").to(\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(120)_likelihood_score()\n","-> template1, template2 = _get_span(\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(121)_likelihood_score()\n","-> sent1_token_ids[0], sent2_token_ids[0], \"diff\"\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(120)_likelihood_score()\n","-> template1, template2 = _get_span(\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(124)_likelihood_score()\n","-> if not template1 or not template2:\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(128)_likelihood_score()\n","-> score1 = self._average_log_probability(sent1_token_ids, template1)\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(360)_average_log_probability()\n","-> probs = []\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(361)_average_log_probability()\n","-> for position in spans:\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(363)_average_log_probability()\n","-> masked_token_ids = token_ids.clone().to(device)\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(364)_average_log_probability()\n","-> masked_token_ids[:, position] = self._tokenizer.mask_token_id\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(366)_average_log_probability()\n","-> with torch.no_grad():\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(367)_average_log_probability()\n","-> if self._is_self_debias:\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(369)_average_log_probability()\n","-> debiasing_prefixes = [DEBIASING_PREFIXES[self._bias_type]]\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(370)_average_log_probability()\n","-> hidden_states = self._model.get_token_logits_self_debiasing(\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(371)_average_log_probability()\n","-> masked_token_ids,\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(372)_average_log_probability()\n","-> debiasing_prefixes=debiasing_prefixes,\n","(Pdb) n\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(373)_average_log_probability()\n","-> decay_constant=50,\n","(Pdb) c\n","> /content/drive/My Drive/bias-bench-main/bias_bench/debias/self_debias/modeling.py(94)get_token_logits_self_debiasing()\n","-> logits_processor = SelfDebiasingLogitsProcessor(\n","(Pdb) c\n","> /content/drive/My Drive/bias-bench-main/bias_bench/debias/self_debias/modeling.py(94)get_token_logits_self_debiasing()\n","-> logits_processor = SelfDebiasingLogitsProcessor(\n","(Pdb) c\n","> /content/drive/My Drive/bias-bench-main/bias_bench/benchmark/crows/crows.py(385)_average_log_probability()\n","-> score = np.mean(probs)\n","(Pdb) p score\n","*** NameError: name 'score' is not defined\n","(Pdb) p probs\n","[-7.212981700897217, -4.925980567932129]\n","(Pdb) --KeyboardInterrupt--\n","(Pdb) --KeyboardInterrupt--\n","(Pdb) ^C\n"]}],"source":["# albert的SelfDebias --bias_type religion\n","\n","! python experiments/crows_debias.py --model SelfDebiasAlbertForMaskedLM --model_name_or_path albert-base-v2 --bias_type religion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VRkfoU8uVixl"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"EVmx_a0gm_ZF"},"source":["未去偏的3个bias type"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":114967,"status":"ok","timestamp":1690550348612,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"6Z6mYu-YKLFk","outputId":"65194dd2-cf51-48ab-d7f6-0300fa736ba0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: AlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_type: gender\n","Evaluating gender examples.\n","100% 262/262 [01:47<00:00,  2.43it/s]\n","====================================================================================================\n","Total examples: 262\n","Metric score: 48.09\n","Stereotype score: 47.8\n","Anti-stereotype score: 48.54\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 48.09\n"]}],"source":["# albert的未去偏 --bias_type gender\n","\n","! python experiments/crows.py --model AlbertForMaskedLM --model_name_or_path albert-base-v2 --bias_type gender"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":175152,"status":"ok","timestamp":1690550532140,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"l6oi6SwPLHOd","outputId":"8c0774ea-76b2-46d5-d9cc-72d949c0fe11"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: AlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_type: race\n","Evaluating race-color examples.\n"," 70% 363/516 [01:57<00:35,  4.28it/s]Skipping example 363.\n","100% 515/516 [02:49<00:00,  3.03it/s]\n","====================================================================================================\n","Total examples: 515\n","Metric score: 62.52\n","Stereotype score: 64.62\n","Anti-stereotype score: 39.53\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 62.52\n"]}],"source":["# albert的未去偏 --bias_type race\n","\n","! python experiments/crows.py --model AlbertForMaskedLM --model_name_or_path albert-base-v2 --bias_type race"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34435,"status":"ok","timestamp":1690550595134,"user":{"displayName":"lii l","userId":"18111132785562331664"},"user_tz":-60},"id":"sI-4huWsLHRT","outputId":"9b5fb825-1a55-4380-e2a1-a10ea0c00ccb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running CrowS-Pairs benchmark:\n"," - persistent_dir: /content/drive/MyDrive/bias-bench-main\n"," - model: AlbertForMaskedLM\n"," - model_name_or_path: albert-base-v2\n"," - bias_type: religion\n","Evaluating religion examples.\n","100% 105/105 [00:29<00:00,  3.58it/s]\n","====================================================================================================\n","Total examples: 105\n","Metric score: 60.0\n","Stereotype score: 60.61\n","Anti-stereotype score: 50.0\n","Num. neutral: 0.0\n","====================================================================================================\n","\n","Metric: 60.0\n"]}],"source":["# albert的未去偏 --bias_type religion\n","\n","! python experiments/crows.py --model AlbertForMaskedLM --model_name_or_path albert-base-v2 --bias_type religion"]},{"cell_type":"markdown","metadata":{"id":"nGPuWPwRo_8_"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CWrQl8GHLHVD"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QVNssRXtLHYL"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","collapsed_sections":["6YB8AtsV7Zjp","aqHAdTYX7PoQ","2G-7fj87XkO-","BcsfXjjGll2y","SAKSYU-Ltem-","MbUtPQOY-Atu","7SRbO0Gr-EJl","yWBCUp-T3adi","l5yGR14q8wLz","zhVjGZN582IV","gJM42WYIU5vG","6ykJpNo5BAN1","2xFNWLJ3UMpj","4HvFC8TPe3m4","7RAs-VhVBRQd","Q72_Rj9CjV2q","MKiDXCwUwF7L","0FNBXQexFuoM","mnozmsjrGY5c","_2MM8-kj_8Xp","JAncqaA6A457","wiXaPTWEY1Tm","cQApSU3OEbCE","3o5iNccOU5Co","DGqukRmWHrFb","vFsGRA4QZnvq","vgCpVnONIm4J","7bLA6keE-etD","ZgAAneyaXhln","VgprKi_OtOde","n9pcDuZMBtQe"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}